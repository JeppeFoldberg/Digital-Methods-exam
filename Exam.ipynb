{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import user specific keys to access twitter\n",
    "from app_cred import CONSUMER_KEY, CONSUMER_SECRET\n",
    "# Import user specific keys to access twitter\n",
    "from app_cred import ACCESS_TOKEN, ACCESS_TOKEN_SECRET \n",
    "\n",
    "auth = tweepy.OAuthHandler(\n",
    "    CONSUMER_KEY, \n",
    "    CONSUMER_SECRET\n",
    ")\n",
    "\n",
    "auth.set_access_token(\n",
    "    ACCESS_TOKEN, \n",
    "    ACCESS_TOKEN_SECRET\n",
    ")\n",
    "\n",
    "api = tweepy.API(\n",
    "    auth, wait_on_rate_limit = True,\n",
    "    wait_on_rate_limit_notify = True,\n",
    "    timeout=900\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split actorlist into four parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles=pd.read_excel(\"actor_list.xlsx\")\n",
    "handles=handles[\"Twitter Handle (uden @)\"]\n",
    "handle1, handle2, handle3, handle4 = \n",
    "i = 1\n",
    "for handle in np.array_split(handles, 4):\n",
    "    handle.to_csv(f\"handle{i}.csv\", index=False)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_of_keys_user=[\n",
    "    \"id\",\"name\",\"screen_name\",\"location\",\n",
    "    \"description\",\"followers_count\",\"friends_count\",\n",
    "    \"statuses_count\",\"created_at\"\n",
    "]\n",
    "\n",
    "list_of_keys_tweet=[\n",
    "    \"created_at\",\"id\",\"lang\",\"full_text\",\n",
    "    \"retweeted\",\"retweeted_status\",\"retweet_count\",\n",
    "    \"is_quote_status\",\"quoted_status\",\"quote_count\",\n",
    "    \"entities\"\n",
    "]\n",
    "\n",
    "def limit_handled(cursor):\n",
    "    \"\"\"Generator to throttle scraping of Twitter-user timeline.\n",
    "    Yields next tweet in timeline\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(cursor)\n",
    "        # If rate limit is reached sleep for 15 minutes\n",
    "        except tweepy.RateLimitError as r: \n",
    "            print(r.reason) \n",
    "            time.sleep(900)\n",
    "        \n",
    "        except tweepy.TweepError as e: \n",
    "            print(e.reason)\n",
    "            time.sleep(5)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "def get_all_tweets(handle):\n",
    "    \"\"\"Function is supposed to return all possible tweets from a user in a df\n",
    "    Handle is the handle of the account\"\"\"\n",
    "    \n",
    "    timeline = tweepy.Cursor(\n",
    "        api.user_timeline, \n",
    "        screen_name=handle,\n",
    "        tweet_mode=\"extended\",\n",
    "        since=start_date\n",
    "    )\n",
    "    \n",
    "    tweet_list = [status._json for status in limit_handled(timeline.items())]\n",
    "    tweets=list()\n",
    "    \n",
    "    for tweet in tweet_list:\n",
    "        for key in tweet:\n",
    "            temp_dict=dict()\n",
    "            \n",
    "            # Access information on user\n",
    "            for user_key in list_of_keys_user:  \n",
    "                try:\n",
    "                    temp_dict[\"user_\"+user_key]=tweet[\"user\"][user_key]\n",
    "                except KeyError:\n",
    "                    temp_dict[\"user_\"+user_key]=None\n",
    "                    \n",
    "            # Access information on tweet\n",
    "            for tweet_key in list_of_keys_tweet: \n",
    "                try: \n",
    "                    temp_dict[\"tweet_\"+tweet_key]=tweet[tweet_key]\n",
    "                except KeyError:\n",
    "                    temp_dict[\"tweet_\"+tweet_key]=None\n",
    "\n",
    "        tweets.append(temp_dict)\n",
    "        \n",
    "    df = pd.DataFrame(tweets) \n",
    "    df = df.fillna(value=np.nan)\n",
    "    return df\n",
    "\n",
    "def get_tweets_from_handles(handlefile,print_handle=False):\n",
    "    \"\"\"Give this function the csv file with the handle \n",
    "    it returns a df including the tweets from all the handles\"\"\"\n",
    "    \n",
    "    handles = pd.read_csv(handlefile)\n",
    "    handles=handles[\"twitter_handle\"].to_list()\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for handle in tqdm(handles):\n",
    "        if print_handle:\n",
    "            print(handle)\n",
    "        temp = get_all_tweets(handle)\n",
    "        df = pd.concat(\n",
    "            [df, temp],\n",
    "            ignore_index = True\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles=get_tweets_from_handles(\"handle1.csv\")\n",
    "\n",
    "compression_options = dict(method=\"zip\", archive_name=\"handles1.csv\")\n",
    "df.to_csv(\"handles1.zip\", compression=compression_options, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import tqdm;tqdm.tqdm.pandas()\n",
    "\n",
    "nlp = spacy.load(\"da_core_news_lg\")\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence):\n",
    "    \"\"\"\n",
    "    Removes Danish stopwords imported from spacy and returns filtered string\n",
    "    \"\"\"  \n",
    "    tokens = sentence.split(\" \")\n",
    "    tokens_filtered= \" \".join([word for word in tokens if not word in all_stopwords])\n",
    "    return tokens_filtered\n",
    "\n",
    "def preproccessor(string, verb_noun_only=False):\n",
    "    \"\"\"\n",
    "    Helper function for lemmatizer().\n",
    "    Preprocesses the string by:\n",
    "    1) lowercasing string\n",
    "    2) removing urls\n",
    "    3) remove mentions, hashtags, and RT\n",
    "    4) remove non-alphanumerical values\n",
    "    5) remove multiple whitespaces\n",
    "    6) remove trailing whitespaces\n",
    "    \"\"\"  \n",
    "    # Lowercase\n",
    "    string=string.lower()\n",
    "    \n",
    "    # Remove url\n",
    "    string=re.sub(\n",
    "        r\"(https|http?):\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b\",\n",
    "        \"\", \n",
    "        string)\n",
    "    \n",
    "    # Remove weird remaining http\n",
    "    string = re.sub(r'https?', '', string)\n",
    "    \n",
    "    # Remove mentions, hashtags, and RT\n",
    "    string=re.sub(\"@\\w+|#\\w+|^rt\",\"\", string)\n",
    "    \n",
    "    # Remove non-alphanumerical values\n",
    "    string=re.sub(r\"\\W\",\" \", string)\n",
    " \n",
    "    # Remove more than one whitespace\n",
    "    string=re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    # Remove trailing whitespaces\n",
    "    string=string.strip()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    string=remove_stopwords(string)\n",
    "    \n",
    "    # Create and return doc object\n",
    "    return nlp(string)  \n",
    "   \n",
    "def lemmatizer(string):\n",
    "    \"\"\"\n",
    "    Lemmatize the preprocessed string using spacy's lemmatizer\n",
    "    \"\"\"\n",
    "    doc=preproccessor(string)\n",
    "    \n",
    "    lemma=\" \".join(\n",
    "        [token.lemma_ for token in doc]\n",
    "    )\n",
    "    return lemma\n",
    "\n",
    "def lemmatizer_reduced(string):\n",
    "    \"\"\"\n",
    "    Reduce the allready lemmatized string by only including proper nouns, nouns, and verbs\n",
    "    \"\"\"\n",
    "    doc=nlp(string)\n",
    "    reduced=\" \".join(\n",
    "    [token.lemma_ for token in doc if len(str(token.lemma_))>3 and token.pos_ in [\"PROPN\",\"NOUN\",\"VERB\"]]\n",
    "    )\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate data into one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files=[\n",
    "    \"handle1.zip\",\n",
    "    \"handle2.zip\",\n",
    "    \"handle3.zip\",\n",
    "    \"handle4.zip\"\n",
    "]\n",
    "\n",
    "all_data=list()\n",
    "\n",
    "# Load in DataFrame and append to all_data\n",
    "for filename in all_files:\n",
    "    frame = pd.read_csv(\n",
    "        filename,\n",
    "        compression=\"zip\"\n",
    "    )\n",
    "    all_data.append(frame)\n",
    "    \n",
    "df = pd.concat(all_data, \n",
    "               axis=0, \n",
    "               ignore_index=True\n",
    "              )\n",
    "\n",
    "# Delete all_data from memory\n",
    "del all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweet_text_lemma\"]=df[\"tweet_full_text\"].progress_apply(lambda tweet: lemmatizer(tweet))\n",
    "df[\"tweet_text_lemma_reduced\"]=df[\"tweet_text_lemma_reduced\"].progress_apply(lambda tweet: lemmatizer_reduced(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_options = dict(method=\"zip\", archive_name=\"data.csv\")\n",
    "df.to_csv(\"data.zip\", compression=compression_options, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "import graph_tool.all as gt\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import ast\n",
    "\n",
    "from spacy.lang.da.stop_words import STOP_WORDS\n",
    "from hSBM_Topicmodel.sbmtm import sbmtm\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_df(topics):\n",
    "    '''takes in model.topic and returns a df that can be used to save the topics \n",
    "    could (and probably should) be rewritten with list comprehensions!'''\n",
    "    topic_nr = []\n",
    "    words = []\n",
    "    weights = []\n",
    "    for topic in topics: \n",
    "        for word in topics[topic]:\n",
    "            topic_nr.append(\n",
    "                'topic ' + str(topic)\n",
    "            )\n",
    "            words.append(word[0])\n",
    "            weights.append(word[1])\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {'topic_nr': topic_nr,\n",
    "         'words': words,\n",
    "         'weights': weights}\n",
    "    )\n",
    "\n",
    "def add_words(topic_df):\n",
    "    '''Function that add related words to the green debate'''\n",
    "    temp_words = []\n",
    "    for i, word in enumerate(topic_df.words):\n",
    "        print('\\n', i, word, '\\n')\n",
    "    inp = input(\n",
    "        '''Are any of these words related to the green debate?\n",
    "        Input index numbers of the word seperated by spaces:\\n''')\n",
    "    indices = [int(i) for i in inp.split(' ') if i.isdigit()]\n",
    "    for i in indices:\n",
    "        temp_words.append(topic_df.words[i])\n",
    "    return temp_words\n",
    "\n",
    "def check_topics(n_topics, df):\n",
    "    '''Function that iterates over each topic\n",
    "    and returns a df with that topic'''\n",
    "    all_words = []\n",
    "    for i in range(n_topics):\n",
    "        topic_df = df.loc[df.topic_nr == f'topic {i+1}'].reset_index()\n",
    "        print(topic_df)\n",
    "        inp = input('\\n\\nDoes this topic include a green word? y or n?\\n')\n",
    "        if inp == 'y':\n",
    "            all_words += add_words(topic_df)\n",
    "        elif inp == 'n':\n",
    "            continue\n",
    "        else:\n",
    "            print('I did not understand that!')\n",
    "    return all_words\n",
    "\n",
    "def expand_words(words, n):\n",
    "    '''takes in a set of words and shows potential words to expand the set'''\n",
    "    extra_words = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            w2v_words=w2v.most_similar(positive=word, topn=n)\n",
    "            similar_words = [word[0] for word in w2v_words if word[0] not in words]\n",
    "        except:\n",
    "            continue\n",
    "        # also removes words that are already in extra words!\n",
    "        similar_words = [word for word in similar_words if word not in extra_words]\n",
    "        for i, word in enumerate(similar_words):\n",
    "            print('\\n', i, word, '\\n')\n",
    "        inp = input(\n",
    "            '''Are any of these words related to the green debate?\n",
    "            Input index numbers of the word seperated by spaces:\\n''')\n",
    "        indices = [int(i) for i in inp.split(' ') if i.isdigit()]\n",
    "        for i in indices:\n",
    "            extra_words.append(similar_words[i])\n",
    "    return extra_words\n",
    "\n",
    "def extract_list(string):\n",
    "    \"\"\"Helper function to extract list from string if string existst\"\"\"\n",
    "    try:\n",
    "        out = ast.literal_eval(string)\n",
    "    except:\n",
    "        out = list()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data and clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessed data\n",
    "df = pd.read_csv(\"data.csv\", parse_dates=['tweet_created_at'], compression='zip')\n",
    "\n",
    "# Removing duplicates\n",
    "df.drop_duplicates(subset='tweet_id')\n",
    "\n",
    "# Only take tweets after 5 june 2019\n",
    "df = df.loc[df.tweet_created_at > '05-06-2020']\n",
    "\n",
    "# Aggregating tweets for actors for each month\n",
    "# Creating new column with month and year to aggregate on\n",
    "df.loc[:, 'year'] = df['tweet_created_at'].dt.year  #.astype(str)\n",
    "df.loc[:, 'month'] = df['tweet_created_at'].dt.month  #.astype(str)\n",
    "\n",
    "# Removing NaNs from tweet_text_lemma  \n",
    "#--> viewing them df.loc[df.tweet_text_lemma.isna()]\n",
    "df = df.dropna(subset=['tweet_text_lemma'])\n",
    "\n",
    "# Aggregating the dataframe\n",
    "tweets_agg = df.groupby(\n",
    "    ['user_screen_name','year','month'],\n",
    "    as_index = False\n",
    ").agg(\n",
    "    {'tweet_text_lemma': ' '.join}\n",
    ")\n",
    "\n",
    "# Now we just need to tokenize the tweets! \n",
    "tokenizer = nltk.tokenize.casual.TweetTokenizer()\n",
    "tweets_agg.loc[:, 'tokens'] = tweets_agg.tweet_text_lemma.apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking the data that we need for the topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tweets_agg['tokens']\n",
    "handles = tweets_agg['user_screen_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data in a format usable for the hSBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove infrequent words; Snorres solution\n",
    "cutoff = 5\n",
    "c = Counter()\n",
    "for doc in tokens:\n",
    "    c.update(Counter(doc))\n",
    "vocab = c.most_common(40000)\n",
    "vocab = set([word for word, count in vocab if count > 1])\n",
    "# remove words\n",
    "docs = [[w for w in doc if w in vocab] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the hSBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "model = sbmtm()\n",
    "\n",
    "# We have to create the document network from the corpus\n",
    "model.make_graph(docs)\n",
    "\n",
    "# Seed for graph-tool's random number generator\n",
    "gt.seed_rng(42) \n",
    "\n",
    "# Fit model\n",
    "model.fit()\n",
    "model.save_graph(filename = 'graph_full_dataset_grouped.xml.gz')\n",
    "\n",
    "topic_df = get_topic_df(model.topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the green words in the topics from hSBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = check_topics(373, topic_df)\n",
    "\n",
    "temp = []\n",
    "for word in all_words:\n",
    "    temp.append(word.lower())\n",
    "all_words_unique = set(temp)\n",
    "all_words_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save green keywords\n",
    "with open(\"green_words.txt\", \"w\") as output:\n",
    "    output.write(str(all_words_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding additional keywords with W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('green_words.txt', 'r') as f:\n",
    "    green_words = f.read()\n",
    "green_words = extract_list(green_words)\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format('w2v/dsl_skipgram_2020_m5_f500_epoch2_w5.model.w2v.bin', binary=True)\n",
    "extra_words = expand_words(green_words, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_combined = set(extra_words + list(green_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding keywords from hSBM and W2V to the keywords from qualitative methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "former_words = pd.read_csv('qualitative_keywords.csv', index_col=0)\n",
    "former_words_set = set(former_words['0'].values)\n",
    "all_words_final = list(all_words_set|former_words_set)\n",
    "all_final_words = pd.DataFrame({'words': all_words_final})\n",
    "all_final_words.to_csv('final_final_keywords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak labeller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_label(df,word_list):\n",
    "    tweet_list=df.tweet_full_text.to_list()\n",
    "    i=0\n",
    "    climate_tweets=[]\n",
    "    for tweet in tweet_list:\n",
    "        i+=1 #iterations\n",
    "        temp=0\n",
    "        try:\n",
    "            tweet2=set(tweet.split()) #split tweet into words\n",
    "        except:\n",
    "            continue\n",
    "        for word in word_list:\n",
    "            try:\n",
    "                if word in tweet2: #see if word is in list of words from tweet\n",
    "                    if temp==0:\n",
    "                        temp+=1\n",
    "                    else: #if 2 or more, append tweet to list of tweets\n",
    "                        climate_tweets.append(tweet)\n",
    "            except:\n",
    "                fails.append(i)\n",
    "    tweet_set=set(climate_tweets) #Only keep unique values, each tweet once            \n",
    "    df['klimarel']=df.tweet_full_text.isin(tweet_set)#Create column with True for tweets in list\n",
    "    klima_df=df.loc[df['klimarel']==True] #Only keep climate tweets\n",
    "    return klima_df #return df of only climate tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data.csv',compression=\"zip\")\n",
    "climate_words=pd.read_csv('final_keywords.csv')\n",
    "climate_words.columns=['0','keywords']\n",
    "label_words=climate_words.keywords.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply weak labeller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeller=weak_label(df,label_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression=dict(method=\"zip\",archive_name=\"only_climate_tweets.csv\")\n",
    "labeller.to_csv(\"only_climate_tweets.zip\",compression=compression,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mention-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import re\n",
    "import ast\n",
    "import tqdm;tqdm.tqdm.pandas()\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data and clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset of tweets since election and list of our actors\n",
    "df=pd.read_csv('only_climate_tweets.csv',compression=\"zip\")\n",
    "df2=pd.read_excel('actor_list.xlsx')\n",
    "\n",
    "#remove duplicates\n",
    "df=df.drop_duplicates(subset='tweet_id')\n",
    "df = df.loc[df.tweet_created_at > '05-06-2020']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define extract functions\n",
    "def extract_list(string):\n",
    "    \"\"\"Helper function to extract list from string if string exists.\n",
    "    Else, return empty dict\"\"\"\n",
    "    try:\n",
    "        out=ast.literal_eval(string)\n",
    "    except:\n",
    "        out=list()\n",
    "    return out\n",
    "\n",
    "\n",
    "def extract_dict(string):\n",
    "    \"\"\"Helper function to extract dict from string if string exists.\n",
    "    Else, return empty dict\"\"\"\n",
    "    try:\n",
    "        out=ast.literal_eval(string)\n",
    "    except:\n",
    "        out=dict()\n",
    "    return out\n",
    "\n",
    "def extract_from_entities(tweet,ent_key,tag_key):\n",
    "    \"\"\"Helper function to extract information from tweet_entities.\n",
    "    tweet_entities is a dict-of-dicts containing all information on \n",
    "    twitter entities from a given tweet.\n",
    "    ent_key: key used to access the dictionary of interest e.g. \"hastags\"\n",
    "    tag_key: key used to access value of interest e.g. \"text\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        out=[tag[tag_key] for tag in tweet[ent_key] if tweet[ent_key]!=ent_key]\n",
    "    except:\n",
    "        out=list()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract tweet entities dict into seperate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweet_retweeted_status\"]=df[\"tweet_retweeted_status\"].progress_apply(\n",
    "    lambda x:extract_dict(x)\n",
    ")\n",
    "\n",
    "df[\"tweet_quoted_status\"]=df[\"tweet_quoted_status\"].progress_apply(\n",
    "    lambda x:extract_dict(x)\n",
    ")\n",
    "\n",
    "df[\"tweet_entities\"]=df[\"tweet_entities\"].progress_apply(lambda x:extract_dict(x)\n",
    ")\n",
    "\n",
    "df[\"tweet_hashtags\"]=df.tweet_entities.progress_apply(\n",
    "    lambda tweet: extract_from_entities(\n",
    "        tweet,\n",
    "        \"hashtags\",\n",
    "        \"text\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df[\"tweet_mentions\"]=df.tweet_entities.progress_apply(\n",
    "    lambda tweet: extract_from_entities(\n",
    "        tweet,\n",
    "        \"user_mentions\",\n",
    "        \"screen_name\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dict of accounts and who they mention\n",
    "mention_dict = defaultdict(list)\n",
    "for idx,row in df.iterrows():\n",
    "    mention_dict[row['user_screen_name']].append(\n",
    "        row['tweet_mentions']\n",
    "    )\n",
    "    \n",
    "#keep only alphanumerical characters\n",
    "mention_dict2 = {key: value for key, value in mention_dict.items() if key.isalpha()}\n",
    "\n",
    "#flatten the lists in dict\n",
    "for key in mention_dict2.keys():\n",
    "    mention_dict2[key]=[item for sublist in mention_dict2[key] for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create network from dict\n",
    "mention_network=nx.from_dict_of_lists(mention_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of nodes that are not in our actor list\n",
    "not_rel=[]\n",
    "for i in list(mention_network.nodes()):\n",
    "    if i not in df2['actors'].to_list():\n",
    "        if i not in list(klima_df.user_screen_name.unique()):\n",
    "            if i not in df2['actors'].str.lower():\n",
    "                not_rel.append(i)\n",
    "\n",
    "#remove \n",
    "for i in not_rel:\n",
    "    mention_network.remove_node(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtag-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import re\n",
    "import ast\n",
    "import tqdm;tqdm.tqdm.pandas()\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "import factor_analyzer\n",
    "import tqdm;tqdm.tqdm.pandas()\n",
    "import exam_utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from adjustText import adjust_text\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams[\"figure.figsize\"] = (16,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cronbach_alpha(df):\n",
    "    \"\"\"\n",
    "    Function that returns Cronbach's Alpha from a pandas.DataFrame().\n",
    "    \"\"\"\n",
    "    # Transform the df into a correlation matrix\n",
    "    df_corr = df.corr()\n",
    "    \n",
    "    # Calculate N\n",
    "    # The number of variables equals the number of columns in the df\n",
    "    N = df.shape[1]\n",
    "    \n",
    "    # Calculate R\n",
    "    # For this, the function loops through the columns and append every\n",
    "    # relevant correlation to an array calles \"r_s\". Then, the function \n",
    "    # calculates the mean of \"r_s\"\n",
    "    rs = np.array([])\n",
    "    for i, col in enumerate(df_corr.columns):\n",
    "        sum_ = df_corr[col][i+1:].values\n",
    "        rs = np.append(sum_, rs)\n",
    "    mean_r = np.mean(rs)\n",
    "    \n",
    "   # Use the formula to calculate Cronbach's Alpha \n",
    "    cronbach_alpha = (N * mean_r) / (1 + (N - 1) * mean_r)\n",
    "    return cronbach_alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate=pd.read_csv(\"only_climate_tweets.zip\",compression=\"zip\")\n",
    "index=climate.tweet_id.to_list()\n",
    "actor_types=pd.read_excel(\"initial_network.xlsx\")\n",
    "actor_types=actor_types[[\"user_screen_name\",\"actor_type\"]]\n",
    "\n",
    "#Create df with data of interest\n",
    "df=climate[[\"user_screen_name\",\"tweet_created_at\",\"tweet_full_text\",\"tweet_text_lemma_reduced\"]]\n",
    "df=df[df[\"tweet_created_at\"]>=\"26-02-2021\"].reset_index(drop=True)\n",
    "\n",
    "# Remove climate dataframe from memory\n",
    "del climate\n",
    "\n",
    "# Aggregate tweets on user\n",
    "tweets_agg = df.groupby(\n",
    "    \"user_screen_name\", \n",
    "    as_index = False\n",
    ").agg(\n",
    "    {\"tweet_text_lemma_reduced\": \" \".join}\n",
    ")\n",
    "\n",
    "# Merge tweets_agg with actor_types\n",
    "tweets_agg=tweets_agg.merge(\n",
    "    actor_types,\n",
    "    on=\"user_screen_name\"\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store attributes in variables\n",
    "min_df=.05\n",
    "max_df=0.95\n",
    "ngram_range=(1,1)\n",
    "\n",
    "# Initialise CountVectorizer object\n",
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=ngram_range,\n",
    "    min_df=min_df,\n",
    "    max_df=max_df\n",
    ")\n",
    "\n",
    "# Create sparse count matrix\n",
    "sparse_count = vectorizer.fit_transform(\n",
    "    tweets_agg.tweet_text_lemma_reduced.to_numpy(),\n",
    "    tweets_agg.user_screen_name.to_numpy()\n",
    ")\n",
    "\n",
    "# Cast data in a pandas.DataFrame()\n",
    "count_matrix = pd.DataFrame(\n",
    "    data=sparse_count.toarray(), \n",
    "    columns=vectorizer.get_feature_names(),\n",
    "    index=tweets_agg.user_screen_name\n",
    ")\n",
    "\n",
    "# Show shape\n",
    "# count_matrix.drop(\n",
    "#     [\"000\"],axis=1,inplace=True)\n",
    "\n",
    "feature_names=count_matrix.columns\n",
    "print(count_matrix.shape)\n",
    "count_matrix.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise StandardScaler()\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_std = sc.fit_transform(\n",
    "    count_matrix.to_numpy()\n",
    ")\n",
    "\n",
    "X_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components= X_std.shape[0]\n",
    "columns=[f\"PC{d}\" for d in range(1,n_components+1)]\n",
    "\n",
    "# Initialise PCA()\n",
    "pca = PCA(\n",
    "    n_components=n_components,\n",
    "    svd_solver=\"auto\"\n",
    ")\n",
    "\n",
    "pca.fit(X_std)\n",
    "\n",
    "X=pca.transform(X_std)\n",
    "\n",
    "pc_df=pd.DataFrame(\n",
    "    X,\n",
    "    columns=columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loadings Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components=pca.components_\n",
    "\n",
    "loadings = pd.DataFrame(\n",
    "    pca_components, \n",
    "    index=count_matrix.index,\n",
    "    columns=feature_names\n",
    ")\n",
    "\n",
    "# Compute loadings scores\n",
    "L = pca_components.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "# Scale PC scores\n",
    "scaled_PC = L / np.max(abs(L), axis=0) \n",
    "\n",
    "loading_matrix = pd.DataFrame(\n",
    "    L, \n",
    "    index=feature_names,\n",
    "    columns=columns\n",
    ")\n",
    "\n",
    "print(L.max())\n",
    "print(L.min())\n",
    "print(L.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify \n",
    "eigen_below_1=np.where(np.sqrt(pca.explained_variance_)<1)[0][0]\n",
    "fig, ax = plt.subplots(1,1)                                                              \n",
    "                                                              \n",
    "ax.set_title(\"Eigenvalues\",fontsize=20)\n",
    "ax.set_xlabel(\"Principal component\")\n",
    "ax.set_ylabel(\"Eigenvalue\")\n",
    "ax.axvline(\n",
    "    x=eigen_below_1,\n",
    "    c=\"grey\", \n",
    "    linestyle=\"--\"\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    np.sqrt(pca.explained_variance_)\n",
    "       )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singular_values=np.diag(pca.singular_values_)\n",
    "\n",
    "covariance_principal_compenents=pd.DataFrame(\n",
    "    singular_values,\n",
    "    index=columns,\n",
    "    columns=columns\n",
    ")\n",
    "\n",
    "explained_variance=pca.explained_variance_ratio_\n",
    "cumulative_variance=np.cumsum(explained_variance)\n",
    "\n",
    "# How much does the first two PC's explain?\n",
    "round(cumulative_variance[1]*100,2)\n",
    "\n",
    "# How many PC's are needed to explain 90% of the total variation?\n",
    "variance_above_90=np.where(cumulative_variance>=.9)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise subplots\n",
    "fig,ax=plt.subplots(1,1)\n",
    "\n",
    "ymin=0\n",
    "ax.set_title(\n",
    "    \"Variance captured by principal components\\n\",\n",
    "    fontsize = 20\n",
    ")\n",
    "# Plot the amount of variance captured by each principal components\n",
    "plot1 = ax.scatter(\n",
    "    range(n_components), \n",
    "    explained_variance, \n",
    "    color=\"royalblue\",\n",
    "    label=\"Variance captured by each principal components\"\n",
    ")\n",
    "ax.set_xlabel(\"Principal Component\")\n",
    "ax.set_ylabel(\"Variance captured\")\n",
    "ax.set_ylim(ymin=ymin)\n",
    "\n",
    "# Plot the cumulative amount of variance captured on second y-axis\n",
    "ax2 = ax.twinx()  # Create second y-axis\n",
    "plot2=ax2.scatter(\n",
    "    range(n_components),\n",
    "    cumulative_variance,\n",
    "    label=\"Cumulative variance captured\"\n",
    ")\n",
    "ax2.set_ylabel(\"Cumulative variance captured\")\n",
    "ax2.set_ylim(ymin=ymin)\n",
    "\n",
    "ax2.axvline(\n",
    "    x=variance_above_90,\n",
    "    c=\"grey\", \n",
    "    linestyle=\"--\"\n",
    ")\n",
    "\n",
    "plt.legend(\n",
    "    handles = [plot1, plot2],\n",
    "    loc=4\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Socio-symbolic constellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance of features for PC's \n",
    "\n",
    "# 10 most important words for the variance explained in PC1:\n",
    "loadings_scores_PC1=pd.Series(\n",
    "    pca.components_[0],\n",
    "    index=feature_names\n",
    ")\n",
    "sorted_loadings_scores_PC1=loadings_scores_PC1.abs().sort_values(ascending=False)\n",
    "sorted_loadings_scores_PC1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 most important words for the variance explained in PC2:\n",
    "loadings_scores_PC2=pd.Series(\n",
    "    pca.components_[1],\n",
    "    index=feature_names\n",
    ")\n",
    "sorted_loadings_scores_PC2=loadings_scores_PC2.abs().sort_values(ascending=False)\n",
    "sorted_loadings_scores_PC2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many words to plot in each direction\n",
    "n = 10\n",
    "\n",
    "# Identify the indices that sorts the two first components\n",
    "PC1_loadings = pca_components.T[:,0].argsort()\n",
    "PC2_loadings = pca_components.T[:,1].argsort()\n",
    "\n",
    "# Find the terms (indicies) that load most on the first principal component\n",
    "PC1_plot_indicies = np.concatenate(\n",
    "    (\n",
    "        PC1_loadings[:n], \n",
    "        PC1_loadings[-n:]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Identify remaining indices\n",
    "remain_indicies = np.array(\n",
    "    [index for index in PC2_loadings if index not in PC1_plot_indicies]\n",
    ")\n",
    "\n",
    "# Find the remaining terms that load most on the second principal component\n",
    "PC2_plot_indicies = np.concatenate(\n",
    "    (\n",
    "        remain_indicies[:n], \n",
    "        remain_indicies[-n:]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combine the indices\n",
    "PC_plot_indicies = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            PC1_plot_indicies, \n",
    "            PC2_plot_indicies\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Get the term names\n",
    "PC_plot_names = feature_names[PC_plot_indicies]\n",
    "\n",
    "# Get the term loadings\n",
    "PC_plot_load = L[PC_plot_indicies]\n",
    "# PC_plot_load=PC_plot_load[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define names to plot in PCA-plot\n",
    "actor_names=[\n",
    "    \"klimaraadet\",\"KlimaMin\",\"Spolitik\",\"venstredk\",\n",
    "    \"DanskEnergi\",\"DanskIndustri\",\"winddenmark\",\"biogasdanmark\",\"EuropeanEnergy_\",\n",
    "    \"DenGroenneStud\",\"spisekammeret\",\"FrederikSandby\",\"NOAH_dk\",\"greenpeacedk\",\"Klimabev\",\n",
    "    \"SorenHave\",\"larskohler\",\"concitoinfo\",\"DanJoergensen\",\"ExtinctionRDK\"\n",
    "]\n",
    "\n",
    "# Find actor's index in count_matrix\n",
    "index=list()\n",
    "for a in actor_names:\n",
    "    temp=count_matrix.index.to_list().index(a)\n",
    "    index.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the socio-symbolic constellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "# Set title \n",
    "ax.set_title(\n",
    "    f\"PCA of word matrix ({count_matrix.shape[0]} actors, {count_matrix.shape[1]} words)\",\n",
    "    fontsize = 20\n",
    ")\n",
    "\n",
    "# Set x label\n",
    "ax.set_xlabel(\n",
    "    f\"Principal Component 1 ({round(100*pca.explained_variance_ratio_[0],2)} % of variation)\",\n",
    "    fontsize = 15\n",
    ")\n",
    "\n",
    "# Set y label\n",
    "ax.set_ylabel(\n",
    "    f\"Principal Component 2 ({round(100*pca.explained_variance_ratio_[1],2)} % of variation)\",\n",
    "    fontsize = 15\n",
    ")\n",
    "\n",
    "# Mark 0,0 on the coordinate system\n",
    "ax.axvline(\n",
    "    x=0,\n",
    "    c=\"grey\", \n",
    "    linestyle=\"--\"\n",
    ")\n",
    "ax.axhline(\n",
    "    y=0, \n",
    "    c=\"grey\", \n",
    "    linestyle=\"--\"\n",
    ")\n",
    "\n",
    "# Plot word loadings\n",
    "ax.scatter(\n",
    "    PC_plot_load[:,0], \n",
    "    PC_plot_load[:,1], \n",
    "    marker = \"o\", \n",
    "    label=\"Word\",\n",
    "    alpha=.75\n",
    ")\n",
    "\n",
    "# Plot standardized principal component scores\n",
    "ax.scatter(\n",
    "    scaled_PC[index,0], \n",
    "    scaled_PC[index,1], \n",
    "    marker = \"x\", \n",
    "    label=\"Actor\", \n",
    "    alpha=0.9\n",
    ")\n",
    "\n",
    "# Annotate the plot\n",
    "texts=[]\n",
    "\n",
    "# Words\n",
    "for x, y, txt in zip(\n",
    "    PC_plot_load[:,0], \n",
    "    PC_plot_load[:,1], \n",
    "    PC_plot_names):\n",
    "    texts.append(\n",
    "        plt.text(\n",
    "            x,\n",
    "            y,\n",
    "            txt,\n",
    "            size=10,\n",
    "            weight=\"bold\"\n",
    "        )\n",
    ")\n",
    "    \n",
    "# Actors\n",
    "for x, y, txt in zip(\n",
    "    scaled_PC.T[index,0],\n",
    "    scaled_PC.T[index,1],\n",
    "    actor_names):\n",
    "    \n",
    "    texts.append(\n",
    "        plt.text(\n",
    "            x,\n",
    "            y,\n",
    "            txt,\n",
    "            size=12\n",
    "        )\n",
    ")\n",
    "    \n",
    "adjust_text(\n",
    "    texts, \n",
    "    arrowprops=dict(\n",
    "        arrowstyle=\"->\", \n",
    "        color=\"grey\"\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate test-statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmo=factor_analyzer.factor_analyzer.calculate_kmo(np.corrcoef(X))[1]\n",
    "cronbach_a=cronbach_alpha(pd.DataFrame(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying k-cluster algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_comp=pd.DataFrame(pca_components)\n",
    "ks = range(1, 49)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(pc_comp.iloc[:,:1])\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array([PC_plot_load[:,0]\n",
    "               ,PC_plot_load[:,1]]).T\n",
    "\n",
    "model = KMeans(n_clusters = 4, init = \"k-means++\")\n",
    "\n",
    "label = model.fit_predict(\n",
    "    data\n",
    ")\n",
    "\n",
    "uniq = np.unique(label)\n",
    "for i in uniq:\n",
    "    plt.scatter(\n",
    "        data[label == i , 0],\n",
    "        data[label == i , 1],\n",
    "        label = i\n",
    "    )\n",
    "\n",
    "texts=[]    \n",
    "for x, y, txt in zip(\n",
    "    PC_plot_load[:,0], \n",
    "    PC_plot_load[:,1], \n",
    "    PC_plot_names):\n",
    "    texts.append(\n",
    "        plt.text(\n",
    "            x,\n",
    "            y,\n",
    "            txt,\n",
    "            size=10,\n",
    "            weight=\"bold\"\n",
    "        )\n",
    "    )    \n",
    "adjust_text(\n",
    "    texts, \n",
    "    arrowprops=dict(\n",
    "        arrowstyle=\"->\", \n",
    "        color=\"grey\"\n",
    "    )\n",
    ")    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "# utils and general stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from dateutil.parser import parse\n",
    "\n",
    "#Packages to create DFM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "#Models to train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Packages for cross-validation and parameter tuning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_parser(string):\n",
    "    \"\"\"\n",
    "    Helper function to extract time in format date month year. \n",
    "    \"\"\"\n",
    "    dt=parse(string)\n",
    "    dt=dt.strftime(\"%d-%m-%Y\")\n",
    "    return datetime.datetime.strptime(dt,\"%d-%m-%Y\")\n",
    "\n",
    "def sample_dataset(df, n=100, random_state=42):\n",
    "    '''Takes in a df and returns 100 random tweets to be labelled'''\n",
    "    temp = df.sample(n, random_state=random_state)\n",
    "    temp.loc[:, 'label'] = np.nan\n",
    "    return temp\n",
    "\n",
    "def split_data(path, test_data=False):\n",
    "    '''takes in the path to the latest labelled data set and returns X_train, y_train, and a df\n",
    "    could have used train_test_split'''\n",
    "    new_df = pd.read_csv(path)\n",
    "    X = new_df.tweet_text_lemma\n",
    "    y = new_df.label\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    if test_data:\n",
    "        return X_train, X_test, y_train, y_test, new_df\n",
    "    else:\n",
    "        return X, y, new_df\n",
    "\n",
    "def get_unlabelled(new_df, old_df):\n",
    "    '''takes in the new df and removes the ones in the new one from the old one'''\n",
    "    unlabelled_df = old_df.loc[~old_df.index.isin(new_df.index)]\n",
    "    return unlabelled_df\n",
    "\n",
    "def predict_unlabelled(pipeline, unlabelled_df):\n",
    "    '''takes in a pipeline, the unlabelled df and adds the maximum probability column\n",
    "    Then it sorts the dataframe by max proba and returns it'''\n",
    "    # predicts for the three classes for all entries in the dataset\n",
    "    predictions = pipeline.predict_proba(unlabelled_df.tweet_text_lemma)\n",
    "    # creates a column with the max probability\n",
    "    temp = unlabelled_df.copy()\n",
    "    temp.loc[:, 'max_proba'] = [max(pred) for pred in predictions]\n",
    "    return temp\n",
    "\n",
    "def label_new_set(unlabelled_df, labelled_df, new_name):\n",
    "    '''takes in the df produced above, sorts it and saves a new df to be labelled'''\n",
    "    unlabelled_df.sort_values(by='max_proba', inplace=True)\n",
    "    new_df = unlabelled_df[:100].copy()\n",
    "    new_df.loc[:, 'label'] = np.nan\n",
    "    new_df = pd.concat([new_df, labelled_df])\n",
    "    new_df.to_excel(f'{new_name}.xlsx')\n",
    "    return None\n",
    "\n",
    "def cohens_kappa(path_to_labelled, full_df):\n",
    "    '''takes in a labelled dict and an unlabelled dict. Fits a model for each hundred labelled entries\n",
    "    predicts on the unlabelled set and then calculates cohens kappa for each models prediction\n",
    "    and the former iterations and returns a list of cohens kappa scores for each iteration\n",
    "    Could also be augmented to print the score each iteration'''\n",
    "    df = pd.read_excel(path_to_labelled, index_col=0)\n",
    "    unlabelled_df = get_unlabelled(df, full_df)\n",
    "    X_test = unlabelled_df.tweet_text_lemma\n",
    "    predictions = []\n",
    "    \n",
    "    n = df.shape[0]\n",
    "    for i in range(100, n + 100, 100):\n",
    "        # creates a temp df with only the n lowest labelled examples\n",
    "        temp_df = df.tail(i).copy()\n",
    "        X = temp_df.tweet_text_lemma\n",
    "        y = temp_df.label\n",
    "        pipeline.fit(X, y)\n",
    "        predictions.append(pipeline.predict(X_test))\n",
    "    kappas = []\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        if (i + 1) == len(predictions):\n",
    "            break\n",
    "        else:\n",
    "            kappas.append(cohen_kappa_score(prediction, predictions[i + 1]))\n",
    "    return kappas\n",
    "\n",
    "def heatmap(confusion_df, title):\n",
    "    cmap = sns.dark_palette('seagreen', as_cmap=True)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(confusion_df, annot=True, cmap=cmap)\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('Coded label')\n",
    "    plt.yticks(rotation=90)\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.savefig(title, format='png')\n",
    "    \n",
    "heatmap(logreg_eval, 'Logistic regression confusion-matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data and clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('only_climate_tweets.csv', compression='zip')\n",
    "df.loc[:, 'tweet_created_at'] = df.tweet_created_at.apply(time_parser)\n",
    "\n",
    "# subsetting only after the electiong and making a copy to get rid of the setting with copy warning\n",
    "df_ = df.loc[df.tweet_created_at > '2019-06-05'].copy()  \n",
    "df_.loc[:, 'tweet_id'] = df_ae.loc[:, 'tweet_id'].astype(int)\n",
    "\n",
    "# removing all retweets\n",
    "df_ = df_.loc[~df_.tweet_full_text.str.contains('^RT')]\n",
    "\n",
    "# dropping nans in tweet lemma\n",
    "df_ = df_.dropna(subset=['tweet_text_lemma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create label-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the first dataset to label!\n",
    "label = sample_dataset(df_)\n",
    "label.to_excel('label_this.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active learning loop\n",
    "\n",
    "#### importing data and splitting into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, labelled_df = split_data('label1.csv', test_data=True)\n",
    "unlabelled_df = get_unlabelled(labelled_df, df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline to train on\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "pipeline = Pipeline([ \n",
    "    ('cv', CountVectorizer(\n",
    "        tokenizer=tokenizer.tokenize,\n",
    "        ngram_range = (1, 2),\n",
    "        max_df=0.999,\n",
    "        min_df=0.01)\n",
    "    ),\n",
    "    ('tfidf', TfidfTransformer(use_idf = False)),\n",
    "    ('logreg', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_df = predict_unlabelled(pipeline, unlabelled_df)\n",
    "label_new_set(unlabelled_df, labelled_df, 'label1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the current score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappas = cohens_kappa('label1.xlsx', df_)\n",
    "kappa_df = pd.DataFrame(kappas, columns=[\"Cohen's kappa score\"], index=index)\n",
    "kappa_df.to_csv('cohens\\ kappas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in the parameter values in the grid \n",
    "parameter_grid = {\n",
    "    'tfidf__use_idf': [False, True],\n",
    "    'logreg__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'logreg__C': [0.1, 0.5, 1],\n",
    "}\n",
    "\n",
    "#Initializing a kfold with 5 folds\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "#Initializing the GridSearchCV\n",
    "search = GridSearchCV(\n",
    "    pipeline, \n",
    "    parameter_grid,\n",
    "    cv=cv, \n",
    "    n_jobs = -1,\n",
    "    verbose=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(X_train, y_train)\n",
    "pipeline.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_train)\n",
    "conf = confusion_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "y_test = np.array(y_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Neutral', 'Negative', 'Positive', 'Not climate']\n",
    "logreg_eval = pd.DataFrame(\n",
    "    confusion_matrix(\n",
    "        y_test,\n",
    "        y_pred\n",
    "    ), \n",
    "    columns=categories, \n",
    "    index=categories\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(\n",
    "    logreg_eval, \n",
    "    'Logistic regression confusion-matrix'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report_logreg = pd.DataFrame(\n",
    "    classification_report(\n",
    "        y_test, \n",
    "        y_pred, \n",
    "        target_names=categories, \n",
    "        output_dict=True\n",
    "    )\n",
    ")\n",
    "\n",
    "logreg_scores = class_report_logreg.loc[\n",
    "    ['precision', 'recall', 'f1-score'],\n",
    "    ['Neutral', 'Negative', 'Positive', 'Not climate']\n",
    "]\n",
    "print(logreg_scores.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohens K: intercoder-reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_excel('label7_1.xlsx', index_col=0)\n",
    "df_2 = pd.read_excel('label7_2.xlsx', index_col=0)\n",
    "\n",
    "labels_1 = df_1.head(200).label\n",
    "labels_2 = df_2.head(200).label\n",
    "\n",
    "cohen_kappa_score(labels_1, labels_2)\n",
    "\n",
    "df_1.loc[:, '2_labels'] = df_2.loc[:, 'label'].copy()\n",
    "df_1.loc[df_1.label != df_1.2_labels].to_csv('disagree_1_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and intial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "# for loading in data and splitting into test and train\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import datasets\n",
    "import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from attack.model_def import ElectraClassifier\n",
    "\n",
    "# for fine tuning in pytorch with transformers trainer api\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "#from transformers import ElectraModel\n",
    "import torch\n",
    "#import torch.nn.functional as F\n",
    "#import torch.nn as nn\n",
    "#from torch.utils.data import Dataset, RandomSampler, DataLoader\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_parser(string):\n",
    "    \"\"\"\n",
    "    Helper function to extract time in format date month year. \n",
    "    \"\"\"\n",
    "    dt=parse(string)\n",
    "    dt=dt.strftime(\"%d-%m-%Y\")\n",
    "    return datetime.datetime.strptime(dt,\"%d-%m-%Y\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    '''\n",
    "    tokenizing the datasets\n",
    "    '''\n",
    "    # pads or truncates the text so it fits with the maximum length the nn can take\n",
    "    return tokenizer(examples['tweet_full_text'], max_length = 512, padding='max_length', truncation=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def get_labels(trainer, dataset):\n",
    "    predict = trainer.predict(dataset)\n",
    "    print('Done with the first part')\n",
    "    labels = [np.argmax(predict.predictions[i]) for i in range(len(predict.predictions))]\n",
    "    return labels\n",
    "\n",
    "def load_model():\n",
    "    model_checkpoint = 'Maltehb/-l-ctra-danish-electra-small-cased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "    \n",
    "    model = torch.load('saving_models_attempt/nearly_done_full_model.pt')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return(model, tokenizer)\n",
    "\n",
    "def make_prediction(dataset):\n",
    "    input_ids = dataset['input_ids']\n",
    "    attention_masks = dataset['attention_mask']\n",
    "    logits = model(input_ids, attention_masks)\n",
    "    \n",
    "    logit, preds = torch.max(logits, dim=1)\n",
    "    return(int(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data and clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('only_climate_tweets.csv', compression='zip')\n",
    "df_all.loc[:, 'tweet_created_at'] = df_all.tweet_created_at.apply(lambda t: time_parser(t))\n",
    "df_all = df_all.loc[df_all.tweet_created_at > '2019-06-05']\n",
    "df_sample = df_all.sample(59000)\n",
    "df_sample.to_csv('sample_for_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset, using PyTorch primitive function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files = ['sample_for_prediction.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise -l-ctra model and tokenize data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the model for finetuning\n",
    "model = AutoModelForSequenceClassification.from_pretrained('Maltehb/-l-ctra-danish-electra-small-cased', num_labels=4)\n",
    "\n",
    "# Initialise tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Maltehb/-l-ctra-danish-electra-small-cased\")\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "#tokenized_datasets\n",
    "sample_predict_dataset = tokenized_datasets['train']\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(8))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(8))\n",
    "full_train_dataset = tokenized_datasets['train']\n",
    "full_eval_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Trainer object and train on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='final_results',\n",
    "    num_train_epochs=30,\n",
    "    evaluation_strategy='epoch',     # computes metrics every epoch!\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.02,               # strength of weight decay higher means less overfitting\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10,\n",
    "    metric_for_best_model='accuracy'\n",
    ")\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "cb = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=full_train_dataset,\n",
    "    eval_dataset=full_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = cb\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(trainer, sample_predict_dataset)\n",
    "sample_predicted = sample_predict_dataset.add_column('label_pred', labels)\n",
    "sample_predicted.to_csv('full_59000_predicted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'saving_models_attempt/full_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model()\n",
    "make_prediction(small_eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
