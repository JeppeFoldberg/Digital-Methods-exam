{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dbe310dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading in data and splitting into test and train\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from attack.model_def import ElectraClassifier\n",
    "\n",
    "# for fine tuning in pytorch with transformers trainer api\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "from transformers import ElectraModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, RandomSampler, DataLoader\n",
    "\n",
    "# python engineer\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "37500702",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('label6.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "94f8a62e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-435188e10a352d0c\n",
      "Reusing dataset csv (/Users/jeppefoldberg/.cache/huggingface/datasets/csv/default-435188e10a352d0c/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files = ['label6.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f16fc484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'test'])\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset['train'].train_test_split()\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d87326cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'user_screen_name', 'tweet_id', 'tweet_created_at', 'tweet_full_text', 'tweet_text_lemma', 'tweet_text_lemma_reduced', 'max_proba', 'label'],\n",
       "        num_rows: 450\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'user_screen_name', 'tweet_id', 'tweet_created_at', 'tweet_full_text', 'tweet_text_lemma', 'tweet_text_lemma_reduced', 'max_proba', 'label'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "794e3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Maltehb/-l-ctra-danish-electra-small-cased\")\n",
    "\n",
    "# tokenizing the datasets\n",
    "def tokenize_function(examples):\n",
    "    # pads or truncates the text so it fits with the maximum length the nn can take\n",
    "    return tokenizer(examples['tweet_full_text'], max_length = 280, padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f35b4e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef37157806f043f7a2caa76cda6400a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36e2753e3f5459ba447769366095fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2a750c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Maltehb/-l-ctra-danish-electra-small-cased were not used when initializing ElectraForSequenceClassification: ['generator.encoder.layer.1.intermediate.dense.weight', 'generator.encoder.layer.5.attention.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.key.weight', 'generator.encoder.layer.10.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.intermediate.dense.weight', 'generator.encoder.layer.2.attention.self.key.weight', 'generator.encoder.layer.3.output.dense.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.bias', 'discriminator_predictions.dense.weight', 'generator.encoder.layer.7.output.dense.bias', 'generator.encoder.layer.4.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.value.weight', 'generator.encoder.layer.8.output.dense.weight', 'generator.encoder.layer.5.attention.self.value.bias', 'generator.encoder.layer.6.attention.self.value.weight', 'generator_predictions.decoder.bias', 'generator.encoder.layer.1.output.dense.bias', 'generator.encoder.layer.3.attention.output.dense.weight', 'generator.encoder.layer.2.intermediate.dense.bias', 'generator.embeddings.LayerNorm.bias', 'generator.encoder.layer.6.output.LayerNorm.weight', 'generator.encoder.layer.1.attention.self.query.bias', 'generator.encoder.layer.8.output.LayerNorm.bias', 'generator.encoder.layer.7.attention.self.query.bias', 'generator.encoder.layer.2.attention.output.dense.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.weight', 'generator.encoder.layer.3.intermediate.dense.bias', 'generator.encoder.layer.10.output.LayerNorm.bias', 'generator.encoder.layer.6.intermediate.dense.bias', 'generator.encoder.layer.3.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.query.bias', 'generator.encoder.layer.2.attention.self.key.bias', 'generator.encoder.layer.11.output.LayerNorm.weight', 'generator.encoder.layer.8.attention.self.key.weight', 'generator.encoder.layer.4.output.LayerNorm.weight', 'generator.encoder.layer.4.attention.self.query.bias', 'generator.encoder.layer.5.attention.self.query.bias', 'generator.encoder.layer.11.attention.output.dense.weight', 'generator.encoder.layer.10.attention.self.query.weight', 'generator.encoder.layer.8.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.dense.weight', 'generator.encoder.layer.4.output.dense.weight', 'generator.encoder.layer.9.attention.output.dense.weight', 'generator.encoder.layer.9.attention.self.key.bias', 'generator.encoder.layer.6.intermediate.dense.weight', 'generator.encoder.layer.9.output.LayerNorm.bias', 'generator.encoder.layer.5.output.dense.weight', 'generator.encoder.layer.1.attention.output.dense.bias', 'generator.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.query.bias', 'generator.encoder.layer.10.attention.self.value.weight', 'generator.encoder.layer.4.attention.self.value.weight', 'generator.encoder.layer.2.attention.self.value.bias', 'generator.encoder.layer.9.attention.self.value.bias', 'generator_predictions.LayerNorm.bias', 'generator.encoder.layer.6.output.dense.bias', 'generator.embeddings_project.weight', 'generator.encoder.layer.5.attention.self.key.bias', 'generator.encoder.layer.0.attention.self.key.bias', 'generator.encoder.layer.1.attention.self.query.weight', 'generator.encoder.layer.8.attention.self.key.bias', 'generator.encoder.layer.11.intermediate.dense.bias', 'generator.encoder.layer.9.output.LayerNorm.weight', 'generator.encoder.layer.7.attention.output.dense.bias', 'generator.encoder.layer.1.output.dense.weight', 'discriminator_predictions.dense.bias', 'generator.encoder.layer.3.attention.self.key.weight', 'generator.encoder.layer.9.attention.self.value.weight', 'generator.encoder.layer.0.attention.output.dense.bias', 'generator.encoder.layer.5.output.LayerNorm.bias', 'generator.encoder.layer.11.intermediate.dense.weight', 'generator.encoder.layer.3.output.dense.weight', 'generator.encoder.layer.9.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.weight', 'generator.encoder.layer.8.attention.self.value.bias', 'generator.encoder.layer.3.output.LayerNorm.bias', 'generator.encoder.layer.9.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.value.bias', 'discriminator_predictions.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.key.bias', 'generator.encoder.layer.2.output.dense.bias', 'generator.encoder.layer.9.output.dense.bias', 'generator.encoder.layer.5.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.intermediate.dense.bias', 'generator.encoder.layer.8.attention.self.value.weight', 'generator.encoder.layer.10.attention.self.key.weight', 'generator.encoder.layer.5.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.value.weight', 'generator.encoder.layer.10.attention.output.dense.bias', 'generator.encoder.layer.8.attention.self.query.bias', 'generator.encoder.layer.0.output.dense.bias', 'generator.encoder.layer.10.output.dense.bias', 'generator.encoder.layer.1.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.value.weight', 'generator.encoder.layer.2.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.self.value.weight', 'generator.encoder.layer.11.attention.self.value.bias', 'generator.encoder.layer.9.intermediate.dense.bias', 'generator.encoder.layer.7.attention.self.query.weight', 'generator.encoder.layer.10.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.weight', 'generator.encoder.layer.5.attention.output.dense.bias', 'generator.encoder.layer.9.attention.self.query.weight', 'generator.encoder.layer.9.attention.output.dense.bias', 'generator.encoder.layer.9.output.dense.weight', 'generator.encoder.layer.1.output.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator.encoder.layer.2.attention.output.dense.bias', 'generator.encoder.layer.10.attention.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.key.weight', 'discriminator_predictions.classifier.weight', 'generator.encoder.layer.7.attention.output.LayerNorm.bias', 'generator.encoder.layer.10.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.key.weight', 'generator.encoder.layer.6.output.dense.weight', 'generator.encoder.layer.4.attention.self.key.weight', 'generator.encoder.layer.2.intermediate.dense.weight', 'generator.embeddings.word_embeddings.weight', 'discriminator_predictions.classifier.bias', 'generator.encoder.layer.1.attention.self.value.bias', 'generator.encoder.layer.3.attention.self.key.bias', 'generator.encoder.layer.0.output.dense.weight', 'generator.encoder.layer.8.attention.self.query.weight', 'generator.encoder.layer.7.attention.self.key.weight', 'generator_predictions.bias', 'generator.encoder.layer.3.intermediate.dense.weight', 'generator.encoder.layer.10.output.dense.weight', 'generator.encoder.layer.8.output.LayerNorm.weight', 'generator.encoder.layer.10.intermediate.dense.bias', 'generator.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.self.query.bias', 'generator.encoder.layer.7.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.output.dense.weight', 'generator.encoder.layer.3.attention.self.query.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.query.bias', 'generator.encoder.layer.4.attention.self.value.bias', 'generator.encoder.layer.4.intermediate.dense.weight', 'generator.encoder.layer.7.attention.self.value.weight', 'generator.encoder.layer.1.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.bias', 'generator.encoder.layer.4.attention.self.query.weight', 'discriminator_predictions.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.query.weight', 'generator.encoder.layer.10.attention.self.value.bias', 'generator.encoder.layer.4.intermediate.dense.bias', 'generator.encoder.layer.8.attention.output.dense.bias', 'generator.encoder.layer.1.intermediate.dense.bias', 'generator.encoder.layer.8.intermediate.dense.weight', 'generator.encoder.layer.9.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.dense.bias', 'generator.encoder.layer.11.attention.output.dense.bias', 'generator.encoder.layer.8.attention.output.dense.weight', 'generator.encoder.layer.8.output.dense.bias', 'generator.encoder.layer.7.attention.self.key.bias', 'generator.encoder.layer.5.attention.self.key.weight', 'generator.encoder.layer.7.attention.self.value.bias', 'generator.encoder.layer.0.attention.self.value.bias', 'generator.encoder.layer.5.output.dense.bias', 'generator.embeddings_project.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.weight', 'generator.embeddings.position_embeddings.weight', 'generator.embeddings.LayerNorm.weight', 'generator.encoder.layer.9.attention.self.key.weight', 'generator.embeddings.token_type_embeddings.weight', 'generator.encoder.layer.4.attention.output.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator.encoder.layer.2.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.bias', 'generator_predictions.dense.weight', 'generator_predictions.decoder.weight', 'generator.encoder.layer.7.attention.output.dense.weight', 'generator.encoder.layer.4.attention.self.key.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.bias', 'generator.encoder.layer.0.intermediate.dense.weight', 'generator.encoder.layer.3.attention.self.query.weight', 'generator.encoder.layer.3.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.self.value.bias', 'generator.encoder.layer.2.output.dense.weight', 'generator.encoder.layer.3.attention.self.value.weight', 'generator.encoder.layer.7.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.query.weight', 'generator.encoder.layer.4.output.dense.bias', 'generator.encoder.layer.5.attention.self.query.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.value.weight', 'generator.encoder.layer.11.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.output.dense.weight', 'generator.encoder.layer.6.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.query.weight', 'generator.encoder.layer.6.attention.self.key.bias', 'generator.encoder.layer.9.attention.self.query.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.dense.weight', 'generator.encoder.layer.6.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.weight', 'generator.encoder.layer.0.attention.self.key.weight', 'generator.encoder.layer.1.attention.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.bias', 'generator.encoder.layer.1.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at Maltehb/-l-ctra-danish-electra-small-cased and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# creating the model for finetuning\n",
    "model = AutoModelForSequenceClassification.from_pretrained('Maltehb/-l-ctra-danish-electra-small-cased', num_labels=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5f4e7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(50))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(50))\n",
    "full_train_dataset = tokenized_datasets['train']\n",
    "full_eval_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "daadafe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=full_train_dataset,\n",
    "    eval_dataset=full_eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4fb12f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='870' max='870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [870/870 2:17:51, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.375400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.368800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.366800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.362200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.366900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.357400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.352600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.344400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.348300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.343300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.306600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.357600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.307500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.301600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.305700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.298500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.304400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.229600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.230300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.144500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.108500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.134900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.882600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.861100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.860800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.871800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.789400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.757900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.692600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.670400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.687300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.564700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.518400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.466100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.424300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.389600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.406400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.310100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.270300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.181800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.073200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.065500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.049700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.031600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.029500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=870, training_loss=0.6893669307917014, metrics={'train_runtime': 8278.522, 'train_samples_per_second': 0.105, 'total_flos': 132441877438080.0, 'epoch': 30.0})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "51697062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.7828035354614258,\n",
       " 'eval_accuracy': 0.56,\n",
       " 'eval_runtime': 19.4462,\n",
       " 'eval_samples_per_second': 7.714,\n",
       " 'epoch': 30.0}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee25359",
   "metadata": {},
   "source": [
    "## Trying to use &tals algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a683910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model_checkpoint = 'Maltehb/-l-ctra-danish-electra-small-cased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "    model = ElectraClassifier(model_checkpoint,2)\n",
    "    model_path = 'attack/pytorch_model.bin'\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return(model, tokenizer)\n",
    "\n",
    "def make_prediction(text):\n",
    "    tokenized_text = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids = tokenized_text['input_ids']\n",
    "    attention_masks = tokenized_text['attention_mask']\n",
    "    logits = model(input_ids, attention_masks)\n",
    "    \n",
    "    logit,preds = torch.max(logits, dim=1)\n",
    "    return(int(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca4791d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Maltehb/-l-ctra-danish-electra-small-cased were not used when initializing ElectraModel: ['generator.encoder.layer.1.intermediate.dense.weight', 'generator.encoder.layer.5.attention.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.key.weight', 'generator.encoder.layer.10.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.intermediate.dense.weight', 'generator.encoder.layer.2.attention.self.key.weight', 'generator.encoder.layer.3.output.dense.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.bias', 'discriminator_predictions.dense.weight', 'generator.encoder.layer.7.output.dense.bias', 'generator.encoder.layer.4.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.value.weight', 'generator.encoder.layer.8.output.dense.weight', 'generator.encoder.layer.5.attention.self.value.bias', 'generator.encoder.layer.6.attention.self.value.weight', 'generator_predictions.decoder.bias', 'generator.encoder.layer.1.output.dense.bias', 'generator.encoder.layer.3.attention.output.dense.weight', 'generator.encoder.layer.2.intermediate.dense.bias', 'generator.embeddings.LayerNorm.bias', 'generator.encoder.layer.6.output.LayerNorm.weight', 'generator.encoder.layer.1.attention.self.query.bias', 'generator.encoder.layer.8.output.LayerNorm.bias', 'generator.encoder.layer.7.attention.self.query.bias', 'generator.encoder.layer.2.attention.output.dense.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.weight', 'generator.encoder.layer.3.intermediate.dense.bias', 'generator.encoder.layer.10.output.LayerNorm.bias', 'generator.encoder.layer.6.intermediate.dense.bias', 'generator.encoder.layer.3.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.query.bias', 'generator.encoder.layer.2.attention.self.key.bias', 'generator.encoder.layer.11.output.LayerNorm.weight', 'generator.encoder.layer.8.attention.self.key.weight', 'generator.encoder.layer.4.output.LayerNorm.weight', 'generator.encoder.layer.4.attention.self.query.bias', 'generator.encoder.layer.5.attention.self.query.bias', 'generator.encoder.layer.11.attention.output.dense.weight', 'generator.encoder.layer.10.attention.self.query.weight', 'generator.encoder.layer.8.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.dense.weight', 'generator.encoder.layer.4.output.dense.weight', 'generator.encoder.layer.9.attention.output.dense.weight', 'generator.encoder.layer.9.attention.self.key.bias', 'generator.encoder.layer.6.intermediate.dense.weight', 'generator.encoder.layer.9.output.LayerNorm.bias', 'generator.encoder.layer.5.output.dense.weight', 'generator.encoder.layer.1.attention.output.dense.bias', 'generator.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.query.bias', 'generator.encoder.layer.10.attention.self.value.weight', 'generator.encoder.layer.4.attention.self.value.weight', 'generator.encoder.layer.2.attention.self.value.bias', 'generator.encoder.layer.9.attention.self.value.bias', 'generator_predictions.LayerNorm.bias', 'generator.encoder.layer.6.output.dense.bias', 'generator.embeddings_project.weight', 'generator.encoder.layer.5.attention.self.key.bias', 'generator.encoder.layer.0.attention.self.key.bias', 'generator.encoder.layer.1.attention.self.query.weight', 'generator.encoder.layer.8.attention.self.key.bias', 'generator.encoder.layer.11.intermediate.dense.bias', 'generator.encoder.layer.9.output.LayerNorm.weight', 'generator.encoder.layer.7.attention.output.dense.bias', 'generator.encoder.layer.1.output.dense.weight', 'discriminator_predictions.dense.bias', 'generator.encoder.layer.3.attention.self.key.weight', 'generator.encoder.layer.9.attention.self.value.weight', 'generator.encoder.layer.0.attention.output.dense.bias', 'generator.encoder.layer.5.output.LayerNorm.bias', 'generator.encoder.layer.11.intermediate.dense.weight', 'generator.encoder.layer.3.output.dense.weight', 'generator.encoder.layer.9.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.weight', 'generator.encoder.layer.8.attention.self.value.bias', 'generator.encoder.layer.3.output.LayerNorm.bias', 'generator.encoder.layer.9.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.value.bias', 'discriminator_predictions.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.key.bias', 'generator.encoder.layer.2.output.dense.bias', 'generator.encoder.layer.9.output.dense.bias', 'generator.encoder.layer.5.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.intermediate.dense.bias', 'generator.encoder.layer.8.attention.self.value.weight', 'generator.encoder.layer.10.attention.self.key.weight', 'generator.encoder.layer.5.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.value.weight', 'generator.encoder.layer.10.attention.output.dense.bias', 'generator.encoder.layer.8.attention.self.query.bias', 'generator.encoder.layer.0.output.dense.bias', 'generator.encoder.layer.10.output.dense.bias', 'generator.encoder.layer.1.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.value.weight', 'generator.encoder.layer.2.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.self.value.weight', 'generator.encoder.layer.11.attention.self.value.bias', 'generator.encoder.layer.9.intermediate.dense.bias', 'generator.encoder.layer.7.attention.self.query.weight', 'generator.encoder.layer.10.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.weight', 'generator.encoder.layer.5.attention.output.dense.bias', 'generator.encoder.layer.9.attention.self.query.weight', 'generator.encoder.layer.9.attention.output.dense.bias', 'generator.encoder.layer.9.output.dense.weight', 'generator.encoder.layer.1.output.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator.encoder.layer.2.attention.output.dense.bias', 'generator.encoder.layer.10.attention.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.key.weight', 'discriminator_predictions.classifier.weight', 'generator.encoder.layer.7.attention.output.LayerNorm.bias', 'generator.encoder.layer.10.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.key.weight', 'generator.encoder.layer.6.output.dense.weight', 'generator.encoder.layer.4.attention.self.key.weight', 'generator.encoder.layer.2.intermediate.dense.weight', 'generator.embeddings.word_embeddings.weight', 'discriminator_predictions.classifier.bias', 'generator.encoder.layer.1.attention.self.value.bias', 'generator.encoder.layer.3.attention.self.key.bias', 'generator.encoder.layer.0.output.dense.weight', 'generator.encoder.layer.8.attention.self.query.weight', 'generator.encoder.layer.7.attention.self.key.weight', 'generator_predictions.bias', 'generator.encoder.layer.3.intermediate.dense.weight', 'generator.encoder.layer.10.output.dense.weight', 'generator.encoder.layer.8.output.LayerNorm.weight', 'generator.encoder.layer.10.intermediate.dense.bias', 'generator.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.self.query.bias', 'generator.encoder.layer.7.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.output.dense.weight', 'generator.encoder.layer.3.attention.self.query.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.query.bias', 'generator.encoder.layer.4.attention.self.value.bias', 'generator.encoder.layer.4.intermediate.dense.weight', 'generator.encoder.layer.7.attention.self.value.weight', 'generator.encoder.layer.1.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.bias', 'generator.encoder.layer.4.attention.self.query.weight', 'discriminator_predictions.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.query.weight', 'generator.encoder.layer.10.attention.self.value.bias', 'generator.encoder.layer.4.intermediate.dense.bias', 'generator.encoder.layer.8.attention.output.dense.bias', 'generator.encoder.layer.1.intermediate.dense.bias', 'generator.encoder.layer.8.intermediate.dense.weight', 'generator.encoder.layer.9.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.dense.bias', 'generator.encoder.layer.11.attention.output.dense.bias', 'generator.encoder.layer.8.attention.output.dense.weight', 'generator.encoder.layer.8.output.dense.bias', 'generator.encoder.layer.7.attention.self.key.bias', 'generator.encoder.layer.5.attention.self.key.weight', 'generator.encoder.layer.7.attention.self.value.bias', 'generator.encoder.layer.0.attention.self.value.bias', 'generator.encoder.layer.5.output.dense.bias', 'generator.embeddings_project.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.weight', 'generator.embeddings.position_embeddings.weight', 'generator.embeddings.LayerNorm.weight', 'generator.encoder.layer.9.attention.self.key.weight', 'generator.embeddings.token_type_embeddings.weight', 'generator.encoder.layer.4.attention.output.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator.encoder.layer.2.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.bias', 'generator_predictions.dense.weight', 'generator_predictions.decoder.weight', 'generator.encoder.layer.7.attention.output.dense.weight', 'generator.encoder.layer.4.attention.self.key.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.bias', 'generator.encoder.layer.0.intermediate.dense.weight', 'generator.encoder.layer.3.attention.self.query.weight', 'generator.encoder.layer.3.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.self.value.bias', 'generator.encoder.layer.2.output.dense.weight', 'generator.encoder.layer.3.attention.self.value.weight', 'generator.encoder.layer.7.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.query.weight', 'generator.encoder.layer.4.output.dense.bias', 'generator.encoder.layer.5.attention.self.query.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.value.weight', 'generator.encoder.layer.11.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.output.dense.weight', 'generator.encoder.layer.6.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.query.weight', 'generator.encoder.layer.6.attention.self.key.bias', 'generator.encoder.layer.9.attention.self.query.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.dense.weight', 'generator.encoder.layer.6.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.weight', 'generator.encoder.layer.0.attention.self.key.weight', 'generator.encoder.layer.1.attention.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.bias', 'generator.encoder.layer.1.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "370b7f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction('Helt sikkert din torsk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc75ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = \"models/Ælæctra_uncased_32k/pytorch_model.bin\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/-l-ctra-danish-electra-small-uncased-ner-dane\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"models/Ælæctra_uncased_32k/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab49dd4",
   "metadata": {},
   "source": [
    "## Trying to finetune Ælæctra in the same way that &TAL did it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78acfdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_model_name, num_labels=4):\n",
    "        super(ElectraClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.electra = ElectraModel.from_pretrained(pretrained_model_name)\n",
    "        self.dense = nn.Linear(self.electra.config.hidden_size, self.electra.config.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.electra.config.hidden_dropout_prob)\n",
    "        self.out_proj = nn.Linear(self.electra.config.hidden_size, self.num_labels)\n",
    "\n",
    "    def classifier(self, sequence_output):\n",
    "        x = sequence_output[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.dense(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.dense(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.dense(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.out_proj(x)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        discriminator_hidden_states = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = discriminator_hidden_states[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7af06527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, text, targets, tokenizer, max_len):\n",
    "        self.text = text\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.text[item]\n",
    "        target = self.targets[item]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "          'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'targets': torch.tensor(target, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def get_data_loader(path, tokenizer, max_len, batch_size):\n",
    "    # data is stored with its context, in case we want to train a model using the context as well\n",
    "    dataset = pd.read_csv(path, index_col = 0)\n",
    "    dataset = remove_invalid_inputs(dataset, 'tweet_full_text')\n",
    "\n",
    "    data = custom_dataset(\n",
    "                    text= dataset.tweet_full_text.to_numpy(),  # used to be text\n",
    "                    targets= dataset.label.to_numpy(),  # used to be target\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_len=max_len\n",
    "                    )\n",
    "\n",
    "    sampler = RandomSampler(data)\n",
    "    dataloader = DataLoader(data,batch_size=batch_size,sampler=sampler,pin_memory=True)\n",
    "    return dataloader, data\n",
    "\n",
    "def remove_invalid_inputs(dataset,text_column):\n",
    "    'Simpel metode til at fjerne alle rækker fra en dataframe, baseret på om værdierne i en kolonne er af typen str'\n",
    "    dataset['valid'] = dataset[text_column].apply(lambda x: isinstance(x, str))\n",
    "    return dataset.loc[dataset.valid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e56fabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Maltehb/-l-ctra-danish-electra-small-cased were not used when initializing ElectraModel: ['generator.encoder.layer.1.intermediate.dense.weight', 'generator.encoder.layer.5.attention.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.key.weight', 'generator.encoder.layer.10.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.intermediate.dense.weight', 'generator.encoder.layer.2.attention.self.key.weight', 'generator.encoder.layer.3.output.dense.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.bias', 'discriminator_predictions.dense.weight', 'generator.encoder.layer.7.output.dense.bias', 'generator.encoder.layer.4.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.value.weight', 'generator.encoder.layer.8.output.dense.weight', 'generator.encoder.layer.5.attention.self.value.bias', 'generator.encoder.layer.6.attention.self.value.weight', 'generator_predictions.decoder.bias', 'generator.encoder.layer.1.output.dense.bias', 'generator.encoder.layer.3.attention.output.dense.weight', 'generator.encoder.layer.2.intermediate.dense.bias', 'generator.embeddings.LayerNorm.bias', 'generator.encoder.layer.6.output.LayerNorm.weight', 'generator.encoder.layer.1.attention.self.query.bias', 'generator.encoder.layer.8.output.LayerNorm.bias', 'generator.encoder.layer.7.attention.self.query.bias', 'generator.encoder.layer.2.attention.output.dense.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.weight', 'generator.encoder.layer.3.intermediate.dense.bias', 'generator.encoder.layer.10.output.LayerNorm.bias', 'generator.encoder.layer.6.intermediate.dense.bias', 'generator.encoder.layer.3.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.query.bias', 'generator.encoder.layer.2.attention.self.key.bias', 'generator.encoder.layer.11.output.LayerNorm.weight', 'generator.encoder.layer.8.attention.self.key.weight', 'generator.encoder.layer.4.output.LayerNorm.weight', 'generator.encoder.layer.4.attention.self.query.bias', 'generator.encoder.layer.5.attention.self.query.bias', 'generator.encoder.layer.11.attention.output.dense.weight', 'generator.encoder.layer.10.attention.self.query.weight', 'generator.encoder.layer.8.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.dense.weight', 'generator.encoder.layer.4.output.dense.weight', 'generator.encoder.layer.9.attention.output.dense.weight', 'generator.encoder.layer.9.attention.self.key.bias', 'generator.encoder.layer.6.intermediate.dense.weight', 'generator.encoder.layer.9.output.LayerNorm.bias', 'generator.encoder.layer.5.output.dense.weight', 'generator.encoder.layer.1.attention.output.dense.bias', 'generator.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.query.bias', 'generator.encoder.layer.10.attention.self.value.weight', 'generator.encoder.layer.4.attention.self.value.weight', 'generator.encoder.layer.2.attention.self.value.bias', 'generator.encoder.layer.9.attention.self.value.bias', 'generator_predictions.LayerNorm.bias', 'generator.encoder.layer.6.output.dense.bias', 'generator.embeddings_project.weight', 'generator.encoder.layer.5.attention.self.key.bias', 'generator.encoder.layer.0.attention.self.key.bias', 'generator.encoder.layer.1.attention.self.query.weight', 'generator.encoder.layer.8.attention.self.key.bias', 'generator.encoder.layer.11.intermediate.dense.bias', 'generator.encoder.layer.9.output.LayerNorm.weight', 'generator.encoder.layer.7.attention.output.dense.bias', 'generator.encoder.layer.1.output.dense.weight', 'discriminator_predictions.dense.bias', 'generator.encoder.layer.3.attention.self.key.weight', 'generator.encoder.layer.9.attention.self.value.weight', 'generator.encoder.layer.0.attention.output.dense.bias', 'generator.encoder.layer.5.output.LayerNorm.bias', 'generator.encoder.layer.11.intermediate.dense.weight', 'generator.encoder.layer.3.output.dense.weight', 'generator.encoder.layer.9.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.weight', 'generator.encoder.layer.8.attention.self.value.bias', 'generator.encoder.layer.3.output.LayerNorm.bias', 'generator.encoder.layer.9.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.value.bias', 'discriminator_predictions.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.key.bias', 'generator.encoder.layer.2.output.dense.bias', 'generator.encoder.layer.9.output.dense.bias', 'generator.encoder.layer.5.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.intermediate.dense.bias', 'generator.encoder.layer.8.attention.self.value.weight', 'generator.encoder.layer.10.attention.self.key.weight', 'generator.encoder.layer.5.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.value.weight', 'generator.encoder.layer.10.attention.output.dense.bias', 'generator.encoder.layer.8.attention.self.query.bias', 'generator.encoder.layer.0.output.dense.bias', 'generator.encoder.layer.10.output.dense.bias', 'generator.encoder.layer.1.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.value.weight', 'generator.encoder.layer.2.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.self.value.weight', 'generator.encoder.layer.11.attention.self.value.bias', 'generator.encoder.layer.9.intermediate.dense.bias', 'generator.encoder.layer.7.attention.self.query.weight', 'generator.encoder.layer.10.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.weight', 'generator.encoder.layer.5.attention.output.dense.bias', 'generator.encoder.layer.9.attention.self.query.weight', 'generator.encoder.layer.9.attention.output.dense.bias', 'generator.encoder.layer.9.output.dense.weight', 'generator.encoder.layer.1.output.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator.encoder.layer.2.attention.output.dense.bias', 'generator.encoder.layer.10.attention.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.key.weight', 'discriminator_predictions.classifier.weight', 'generator.encoder.layer.7.attention.output.LayerNorm.bias', 'generator.encoder.layer.10.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.key.weight', 'generator.encoder.layer.6.output.dense.weight', 'generator.encoder.layer.4.attention.self.key.weight', 'generator.encoder.layer.2.intermediate.dense.weight', 'generator.embeddings.word_embeddings.weight', 'discriminator_predictions.classifier.bias', 'generator.encoder.layer.1.attention.self.value.bias', 'generator.encoder.layer.3.attention.self.key.bias', 'generator.encoder.layer.0.output.dense.weight', 'generator.encoder.layer.8.attention.self.query.weight', 'generator.encoder.layer.7.attention.self.key.weight', 'generator_predictions.bias', 'generator.encoder.layer.3.intermediate.dense.weight', 'generator.encoder.layer.10.output.dense.weight', 'generator.encoder.layer.8.output.LayerNorm.weight', 'generator.encoder.layer.10.intermediate.dense.bias', 'generator.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.self.query.bias', 'generator.encoder.layer.7.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.output.dense.weight', 'generator.encoder.layer.3.attention.self.query.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.query.bias', 'generator.encoder.layer.4.attention.self.value.bias', 'generator.encoder.layer.4.intermediate.dense.weight', 'generator.encoder.layer.7.attention.self.value.weight', 'generator.encoder.layer.1.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.bias', 'generator.encoder.layer.4.attention.self.query.weight', 'discriminator_predictions.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.query.weight', 'generator.encoder.layer.10.attention.self.value.bias', 'generator.encoder.layer.4.intermediate.dense.bias', 'generator.encoder.layer.8.attention.output.dense.bias', 'generator.encoder.layer.1.intermediate.dense.bias', 'generator.encoder.layer.8.intermediate.dense.weight', 'generator.encoder.layer.9.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.dense.bias', 'generator.encoder.layer.11.attention.output.dense.bias', 'generator.encoder.layer.8.attention.output.dense.weight', 'generator.encoder.layer.8.output.dense.bias', 'generator.encoder.layer.7.attention.self.key.bias', 'generator.encoder.layer.5.attention.self.key.weight', 'generator.encoder.layer.7.attention.self.value.bias', 'generator.encoder.layer.0.attention.self.value.bias', 'generator.encoder.layer.5.output.dense.bias', 'generator.embeddings_project.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.weight', 'generator.embeddings.position_embeddings.weight', 'generator.embeddings.LayerNorm.weight', 'generator.encoder.layer.9.attention.self.key.weight', 'generator.embeddings.token_type_embeddings.weight', 'generator.encoder.layer.4.attention.output.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator.encoder.layer.2.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.bias', 'generator_predictions.dense.weight', 'generator_predictions.decoder.weight', 'generator.encoder.layer.7.attention.output.dense.weight', 'generator.encoder.layer.4.attention.self.key.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.bias', 'generator.encoder.layer.0.intermediate.dense.weight', 'generator.encoder.layer.3.attention.self.query.weight', 'generator.encoder.layer.3.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.self.value.bias', 'generator.encoder.layer.2.output.dense.weight', 'generator.encoder.layer.3.attention.self.value.weight', 'generator.encoder.layer.7.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.query.weight', 'generator.encoder.layer.4.output.dense.bias', 'generator.encoder.layer.5.attention.self.query.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.value.weight', 'generator.encoder.layer.11.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.output.dense.weight', 'generator.encoder.layer.6.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.query.weight', 'generator.encoder.layer.6.attention.self.key.bias', 'generator.encoder.layer.9.attention.self.query.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.dense.weight', 'generator.encoder.layer.6.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.weight', 'generator.encoder.layer.0.attention.self.key.weight', 'generator.encoder.layer.1.attention.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.bias', 'generator.encoder.layer.1.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ElectraClassifier('Maltehb/-l-ctra-danish-electra-small-cased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7609bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'Maltehb/-l-ctra-danish-electra-small-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72e187b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, data = get_data_loader('label6.csv', tokenizer=tokenizer, max_len=280, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e05c21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d6609d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1856ba6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Fremskrivning fra @Energistyr viser, at øget produktion af biogas gør, at vi er tættere på at nå klimamålet end forventet. Klimavenlig biogas leverer konkrete CO2-reduktioner nu og her og rummer et stort eksportpotentiale for DK. Det har jeg talt med @tv2fyn om #dkpol #dkgreen https://t.co/jqWOX3WaLM',\n",
       "  'Små skridt i den rigtige retning med ny bilaftale, men slet ikke ambitiøst nok. Godt at flere får mulighed for elbil, og at de største og mest forurenende biler bliver dyrere. Men vi er ikke færdige med at finde CO2-reduktioner på transport #dkpol #dkgreen',\n",
       "  '@tselsmark @okologidk Det danske klima er for koldt til soja, i stedet kan vi fodre dyr med protein fra græs, ærter, lupiner, hestebønner mm.',\n",
       "  '»Man skal lede med luppen«: Dan Jørgensen har fremlagt sine grønne bedrifter et år efter klimavalget 🔐 \\nhttps://t.co/ARxQno3ajg',\n",
       "  'Havde håbet at 40% af landbrugsstøtten skulle være grøn. Nu bliver det kun 20 % - hvordan kan det være godt @MogensJensenS ? . @enhedslisten ser frem til at prioritere klima, natur og økologi i den danske landbrugspolitik. #dkpol #dkgreen @okologidk https://t.co/06y3w5fNY9'],\n",
       " 'input_ids': tensor([[    2, 22143,  9046,  ...,     0,     0,     0],\n",
       "         [    2, 24495,  4953,  ...,     0,     0,     0],\n",
       "         [    2,    36,  9849,  ...,     0,     0,     0],\n",
       "         [    2,    98,  2027,  ...,     0,     0,     0],\n",
       "         [    2,  2196, 12211,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'targets': tensor([2, 1, 0, 1, 1])}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "325788ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from python engineer\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0511f616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-9fa2a6cc26c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mstep_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-24e8d2d50762>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloaders' is not defined"
     ]
    }
   ],
   "source": [
    "# freezing layers\n",
    "model = model.to(device)\n",
    "\n",
    "# can be given weights might be useful since we have uneven distribution of classes\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 7, gamma=0.1)\n",
    "\n",
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38fae49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
