{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "dbe310dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading in data and splitting into test and train\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import datasets\n",
    "from exam_utils import timeParser\n",
    "import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from attack.model_def import ElectraClassifier\n",
    "\n",
    "# for fine tuning in pytorch with transformers trainer api\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "#from transformers import ElectraModel\n",
    "import torch\n",
    "#import torch.nn.functional as F\n",
    "#import torch.nn as nn\n",
    "#from torch.utils.data import Dataset, RandomSampler, DataLoader\n",
    "\n",
    "# python engineer\n",
    "#from torch.optim import lr_scheduler\n",
    "#import time\n",
    "import os\n",
    "#import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5f3db326",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('lemma_all.csv', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "755456bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.loc[:, 'tweet_created_at'] = df_all.tweet_created_at.apply(lambda t: timeParser(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4d5d2bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.loc[df_all.tweet_created_at > '2019-06-05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9aeed12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_all.sample(59000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f9fb691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.to_csv('sample_for_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b0e6d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('lemma_all_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7373104b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3d0ec83b214b51ae\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/jeppefoldberg/.cache/huggingface/datasets/csv/default-3d0ec83b214b51ae/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/jeppefoldberg/.cache/huggingface/datasets/csv/default-3d0ec83b214b51ae/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files = ['sample_for_prediction.csv'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f95b4590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'user_screen_name', 'tweet_id', 'tweet_created_at', 'tweet_full_text', 'tweet_text_lemma', 'tweet_text_lemma_reduced'],\n",
       "        num_rows: 59000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb1d953c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 100193,\n",
       " 'user_screen_name': 'JonBurgwald',\n",
       " 'tweet_id': 1341060279845720064,\n",
       " 'tweet_created_at': '2020-12-21',\n",
       " 'tweet_full_text': '.@MaiVilladsen på Folketingets talerstol: \"Vi har indgået mange grønne aftaler det sidste halve år. De fleste har været fremskridt, men fremskridtene har ikke været store nok\". Nemlig rigtigt. Vi kommer ikke udenom at genåbne flere af dem. #dkgreen #dkpol',\n",
       " 'tweet_text_lemma': 'folketing talerstol indgå grøn aftale sidste halv år fremskridt fremskridt stor rigtig udenom genåbne',\n",
       " 'tweet_text_lemma_reduced': 'folketing talerstol indgå aftale år fremskridt fremskridt genåbne',\n",
       " 'max_proba': 0.2537778234568019,\n",
       " 'label': 1,\n",
       " 'Unnamed: 0.1': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f16fc484",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train'].train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "794e3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Maltehb/-l-ctra-danish-electra-small-cased\")\n",
    "\n",
    "# tokenizing the datasets\n",
    "def tokenize_function(examples):\n",
    "    # pads or truncates the text so it fits with the maximum length the nn can take\n",
    "    return tokenizer(examples['tweet_full_text'], max_length = 512, padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f35b4e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8a7da400904bebb3fb70f911c0c9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=59.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "207bfb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets\n",
    "sample_predict_dataset = tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a750c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Maltehb/-l-ctra-danish-electra-small-cased were not used when initializing ElectraForSequenceClassification: ['generator.encoder.layer.4.intermediate.dense.weight', 'generator.encoder.layer.9.output.dense.bias', 'discriminator_predictions.dense.bias', 'generator.embeddings.token_type_embeddings.weight', 'generator.encoder.layer.0.attention.self.value.weight', 'generator.encoder.layer.1.attention.self.value.weight', 'generator.encoder.layer.6.output.dense.weight', 'generator.encoder.layer.2.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.bias', 'generator.encoder.layer.8.output.dense.bias', 'generator.encoder.layer.6.output.LayerNorm.bias', 'generator.encoder.layer.7.attention.self.key.weight', 'generator.encoder.layer.3.attention.output.dense.bias', 'generator.encoder.layer.4.attention.output.dense.weight', 'generator.encoder.layer.10.attention.self.key.bias', 'generator.encoder.layer.5.attention.self.key.bias', 'generator.encoder.layer.6.attention.self.value.bias', 'generator.encoder.layer.8.output.LayerNorm.weight', 'generator.encoder.layer.2.output.dense.weight', 'generator.encoder.layer.3.intermediate.dense.weight', 'generator.encoder.layer.10.attention.output.LayerNorm.bias', 'generator.encoder.layer.9.attention.self.query.bias', 'generator.encoder.layer.4.attention.self.value.weight', 'generator.encoder.layer.6.attention.output.LayerNorm.bias', 'generator.encoder.layer.10.intermediate.dense.weight', 'generator.encoder.layer.1.attention.output.dense.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.value.bias', 'generator.encoder.layer.9.attention.self.query.weight', 'generator.encoder.layer.0.intermediate.dense.weight', 'generator.embeddings_project.weight', 'generator.encoder.layer.6.intermediate.dense.bias', 'generator.encoder.layer.3.attention.self.key.bias', 'generator.encoder.layer.11.attention.output.dense.weight', 'generator.encoder.layer.8.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.weight', 'generator.encoder.layer.2.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.value.bias', 'generator.encoder.layer.6.attention.self.key.bias', 'generator.encoder.layer.11.output.LayerNorm.bias', 'generator.encoder.layer.11.intermediate.dense.bias', 'generator.encoder.layer.11.attention.self.query.bias', 'generator.encoder.layer.3.attention.self.value.bias', 'generator.encoder.layer.4.attention.output.dense.bias', 'generator.encoder.layer.3.output.dense.bias', 'generator_predictions.LayerNorm.bias', 'generator.encoder.layer.11.output.dense.bias', 'generator.encoder.layer.9.attention.self.value.bias', 'generator.encoder.layer.3.output.LayerNorm.weight', 'generator.encoder.layer.2.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.attention.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.self.key.weight', 'generator.encoder.layer.0.output.LayerNorm.weight', 'generator.encoder.layer.2.attention.output.dense.bias', 'generator.encoder.layer.5.attention.output.dense.bias', 'generator.encoder.layer.5.attention.self.query.weight', 'generator.encoder.layer.10.attention.self.query.weight', 'generator.encoder.layer.2.attention.self.key.bias', 'generator.encoder.layer.8.attention.self.key.weight', 'generator.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.self.key.bias', 'generator.encoder.layer.9.attention.output.dense.weight', 'generator.encoder.layer.6.attention.self.value.weight', 'generator.encoder.layer.4.output.dense.weight', 'generator.encoder.layer.5.output.LayerNorm.bias', 'generator.encoder.layer.9.attention.self.key.bias', 'generator.embeddings.word_embeddings.weight', 'generator.encoder.layer.9.attention.output.dense.bias', 'generator.encoder.layer.10.attention.output.dense.weight', 'generator.encoder.layer.5.intermediate.dense.weight', 'generator.embeddings_project.bias', 'generator.encoder.layer.0.output.dense.bias', 'generator.encoder.layer.7.attention.output.dense.weight', 'generator.encoder.layer.2.attention.self.key.weight', 'generator.encoder.layer.10.output.dense.bias', 'generator.encoder.layer.0.attention.self.query.weight', 'generator.encoder.layer.6.output.dense.bias', 'generator.encoder.layer.8.output.dense.weight', 'generator.encoder.layer.8.attention.output.dense.bias', 'generator.encoder.layer.0.intermediate.dense.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.value.bias', 'generator.encoder.layer.7.intermediate.dense.weight', 'generator.encoder.layer.4.attention.self.query.weight', 'generator.encoder.layer.4.attention.output.LayerNorm.weight', 'generator.encoder.layer.3.attention.self.query.bias', 'generator.encoder.layer.1.attention.self.key.bias', 'generator.encoder.layer.6.attention.self.key.weight', 'generator.encoder.layer.10.output.LayerNorm.weight', 'generator.encoder.layer.1.intermediate.dense.bias', 'discriminator_predictions.classifier.bias', 'generator.encoder.layer.5.attention.self.value.weight', 'generator.encoder.layer.8.attention.self.value.weight', 'generator.encoder.layer.5.attention.self.query.bias', 'generator.encoder.layer.5.attention.output.dense.weight', 'generator.encoder.layer.4.attention.self.value.bias', 'generator.encoder.layer.5.output.dense.bias', 'generator.encoder.layer.4.attention.self.key.weight', 'generator.encoder.layer.11.output.LayerNorm.weight', 'generator.encoder.layer.8.output.LayerNorm.bias', 'generator.encoder.layer.8.attention.self.key.bias', 'generator.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.output.dense.bias', 'generator.embeddings.LayerNorm.weight', 'generator_predictions.decoder.bias', 'generator.encoder.layer.3.attention.self.value.weight', 'generator.encoder.layer.5.attention.self.value.bias', 'generator.encoder.layer.10.attention.self.key.weight', 'generator.encoder.layer.1.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.self.key.weight', 'generator.encoder.layer.1.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.value.bias', 'generator.encoder.layer.2.attention.self.value.weight', 'generator.encoder.layer.8.attention.self.query.weight', 'generator.encoder.layer.9.output.dense.weight', 'generator_predictions.bias', 'generator.encoder.layer.11.attention.self.value.weight', 'generator.encoder.layer.10.attention.self.value.weight', 'generator.encoder.layer.6.attention.self.query.bias', 'generator.encoder.layer.3.output.LayerNorm.bias', 'generator.encoder.layer.11.intermediate.dense.weight', 'generator.encoder.layer.1.output.dense.bias', 'generator.encoder.layer.0.attention.output.dense.weight', 'generator.encoder.layer.7.attention.output.dense.bias', 'discriminator_predictions.dense.weight', 'generator.encoder.layer.0.attention.self.key.weight', 'generator.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.embeddings.position_embeddings.weight', 'generator.encoder.layer.6.output.LayerNorm.weight', 'generator.encoder.layer.2.output.dense.bias', 'generator.encoder.layer.2.intermediate.dense.bias', 'generator.encoder.layer.7.attention.self.value.weight', 'generator.encoder.layer.11.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.dense.bias', 'generator.encoder.layer.2.intermediate.dense.weight', 'generator.encoder.layer.6.intermediate.dense.weight', 'generator.encoder.layer.1.attention.self.query.bias', 'generator.encoder.layer.7.attention.output.LayerNorm.weight', 'generator.encoder.layer.4.attention.self.query.bias', 'generator.encoder.layer.10.output.LayerNorm.bias', 'generator.encoder.layer.1.output.LayerNorm.weight', 'generator.encoder.layer.3.attention.self.query.weight', 'generator.encoder.layer.9.output.LayerNorm.bias', 'generator.encoder.layer.8.attention.output.LayerNorm.weight', 'generator.encoder.layer.1.intermediate.dense.weight', 'generator.encoder.layer.5.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.output.dense.bias', 'generator.encoder.layer.7.attention.self.query.bias', 'generator.encoder.layer.3.intermediate.dense.bias', 'generator.encoder.layer.11.attention.self.query.weight', 'generator.encoder.layer.5.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.output.dense.bias', 'generator.encoder.layer.8.attention.output.LayerNorm.bias', 'generator.encoder.layer.6.attention.output.dense.weight', 'generator.encoder.layer.10.output.dense.weight', 'generator.encoder.layer.1.attention.self.query.weight', 'discriminator_predictions.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.dense.weight', 'generator.encoder.layer.7.attention.self.query.weight', 'generator.encoder.layer.7.output.dense.weight', 'generator.encoder.layer.6.attention.self.query.weight', 'generator.encoder.layer.10.intermediate.dense.bias', 'generator.encoder.layer.9.attention.output.LayerNorm.bias', 'generator.encoder.layer.10.attention.output.dense.bias', 'generator.encoder.layer.8.attention.output.dense.weight', 'generator.encoder.layer.7.output.LayerNorm.weight', 'generator.encoder.layer.4.intermediate.dense.bias', 'generator.encoder.layer.2.attention.self.value.bias', 'generator.encoder.layer.9.attention.self.value.weight', 'generator.encoder.layer.8.intermediate.dense.bias', 'generator.encoder.layer.9.intermediate.dense.bias', 'generator.encoder.layer.9.attention.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.output.LayerNorm.bias', 'generator.encoder.layer.4.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.self.key.weight', 'generator.encoder.layer.7.attention.self.key.bias', 'generator.encoder.layer.0.output.dense.weight', 'generator.encoder.layer.5.intermediate.dense.bias', 'generator.encoder.layer.1.attention.self.key.weight', 'generator.encoder.layer.4.attention.self.key.bias', 'generator_predictions.dense.weight', 'generator.encoder.layer.0.attention.self.query.bias', 'generator.encoder.layer.4.output.dense.bias', 'generator.encoder.layer.0.attention.self.key.bias', 'generator.encoder.layer.4.output.LayerNorm.weight', 'discriminator_predictions.LayerNorm.weight', 'generator.encoder.layer.1.output.dense.weight', 'generator.encoder.layer.8.attention.self.value.bias', 'generator_predictions.LayerNorm.weight', 'generator_predictions.decoder.weight', 'generator.embeddings.LayerNorm.bias', 'generator.encoder.layer.2.attention.output.dense.weight', 'generator.encoder.layer.3.output.dense.weight', 'generator.encoder.layer.5.output.dense.weight', 'generator.encoder.layer.10.attention.self.query.bias', 'generator.encoder.layer.7.attention.self.value.bias', 'generator.encoder.layer.10.attention.output.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator.encoder.layer.2.attention.self.query.bias', 'generator.encoder.layer.7.output.LayerNorm.bias', 'generator.encoder.layer.5.attention.output.LayerNorm.bias', 'generator.encoder.layer.9.output.LayerNorm.weight', 'generator.encoder.layer.7.intermediate.dense.bias', 'generator.encoder.layer.9.intermediate.dense.weight', 'generator.encoder.layer.2.attention.self.query.weight', 'discriminator_predictions.classifier.weight', 'generator.encoder.layer.9.attention.self.key.weight', 'generator.encoder.layer.8.attention.self.query.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at Maltehb/-l-ctra-danish-electra-small-cased and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# creating the model for finetuning\n",
    "model = AutoModelForSequenceClassification.from_pretrained('Maltehb/-l-ctra-danish-electra-small-cased', num_labels=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f4e7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(8))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(8))\n",
    "full_train_dataset = tokenized_datasets['train']\n",
    "full_eval_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "daadafe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='final_results',\n",
    "    num_train_epochs=30,\n",
    "    evaluation_strategy='epoch',      # computes metrics every epoch!\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.02,               # strength of weight decay higher means less overfitting\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10,\n",
    "    metric_for_best_model='accuracy'\n",
    ")\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "cb = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=full_train_dataset,\n",
    "    eval_dataset=full_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = cb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4fb12f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='832' max='1560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 832/1560 4:11:55 < 3:40:58, 0.05 it/s, Epoch 16/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.377300</td>\n",
       "      <td>1.368871</td>\n",
       "      <td>0.349091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.366500</td>\n",
       "      <td>1.362964</td>\n",
       "      <td>0.349091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.375400</td>\n",
       "      <td>1.354841</td>\n",
       "      <td>0.349091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.348400</td>\n",
       "      <td>1.338100</td>\n",
       "      <td>0.349091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.321200</td>\n",
       "      <td>1.297163</td>\n",
       "      <td>0.429091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.188200</td>\n",
       "      <td>1.219107</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.106800</td>\n",
       "      <td>1.180200</td>\n",
       "      <td>0.538182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.956300</td>\n",
       "      <td>1.108454</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.794200</td>\n",
       "      <td>1.095311</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.581600</td>\n",
       "      <td>1.098373</td>\n",
       "      <td>0.549091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.477400</td>\n",
       "      <td>0.970945</td>\n",
       "      <td>0.603636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.308300</td>\n",
       "      <td>1.269027</td>\n",
       "      <td>0.530909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>1.246725</td>\n",
       "      <td>0.556364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.221600</td>\n",
       "      <td>1.297854</td>\n",
       "      <td>0.581818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>1.650669</td>\n",
       "      <td>0.534545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>1.678593</td>\n",
       "      <td>0.556364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=832, training_loss=0.8217360883360155, metrics={'train_runtime': 15136.4916, 'train_samples_per_second': 0.103, 'total_flos': 0, 'epoch': 16.0})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a4d4c900",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "log() missing 1 required positional argument: 'logs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-d87fff6d15c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: log() missing 1 required positional argument: 'logs'"
     ]
    }
   ],
   "source": [
    "trainer.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c7abe274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraForSequenceClassification(\n",
      "  (electra): ElectraModel(\n",
      "    (embeddings): ElectraEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (encoder): ElectraEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): ElectraClassificationHead(\n",
      "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=256, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14e75c48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 247,\n",
       " 'Unnamed: 0.1': 104238.0,\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'input_ids': [2,\n",
       "  36,\n",
       "  13925,\n",
       "  1037,\n",
       "  4823,\n",
       "  3360,\n",
       "  36,\n",
       "  11056,\n",
       "  13483,\n",
       "  2645,\n",
       "  24915,\n",
       "  14541,\n",
       "  1975,\n",
       "  1940,\n",
       "  1959,\n",
       "  27641,\n",
       "  1927,\n",
       "  29719,\n",
       "  1045,\n",
       "  12305,\n",
       "  18,\n",
       "  25440,\n",
       "  1914,\n",
       "  1931,\n",
       "  4943,\n",
       "  2281,\n",
       "  18,\n",
       "  2229,\n",
       "  2001,\n",
       "  4429,\n",
       "  2170,\n",
       "  2220,\n",
       "  1934,\n",
       "  28810,\n",
       "  1959,\n",
       "  19118,\n",
       "  13507,\n",
       "  18,\n",
       "  2802,\n",
       "  1968,\n",
       "  2001,\n",
       "  3851,\n",
       "  1944,\n",
       "  3461,\n",
       "  16,\n",
       "  1952,\n",
       "  2814,\n",
       "  3090,\n",
       "  1969,\n",
       "  1963,\n",
       "  4391,\n",
       "  18,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'label': 2,\n",
       " 'max_proba': 0.2915293857255226,\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'tweet_created_at': '2020-12-04',\n",
       " 'tweet_full_text': '@BrianEquator @MortenReventlow Hedder det ikke logik for perlehøns. \\n\\nFremtiden er foran os. Og vi vidste jo godt at olien ikke varede evigt. Nu har vi lavet en aftale, der tager hånd om den situation.',\n",
       " 'tweet_id': 1334760867389452032,\n",
       " 'tweet_text_lemma': 'hedde logik perlehøne fremtid vide godte olie vare evig aftale tage hånd situation',\n",
       " 'tweet_text_lemma_reduced': 'hedde logik perlehøne fremtid vide godte olie vare aftale tag hånd situation',\n",
       " 'user_screen_name': 'JuulMona'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_eval_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "13e9d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(trainer, dataset):\n",
    "    predict = trainer.predict(dataset)\n",
    "    print('Done with the first part')\n",
    "    labels = [np.argmax(predict.predictions[i]) for i in range(len(predict.predictions))]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "016525a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the first part\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = get_labels(trainer, sample_predict_dataset)\n",
    "os.system('say \"jeg er færdig motherfucker\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f4b47f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59000"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8b9b23e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('say \"jeg er færdig måtter fåkker\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0fc8fbdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_predicted = sample_predict_dataset.add_column('label_pred', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4bd023f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328448359"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_predicted.to_csv('full_59000_predicted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "87567497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'attention_mask', 'input_ids', 'token_type_ids', 'tweet_created_at', 'tweet_full_text', 'tweet_id', 'tweet_text_lemma', 'tweet_text_lemma_reduced', 'user_screen_name', 'label_pred'],\n",
       "    num_rows: 59000\n",
       "})"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0b22c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'saving_models_attempt/full_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1754185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model_checkpoint = 'Maltehb/-l-ctra-danish-electra-small-cased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "    \n",
    "    model = torch.load('saving_models_attempt/nearly_done_full_model.pt')\n",
    "\n",
    "    #model = ElectraClassifier(model_checkpoint, 4)\n",
    "    #model_path = 'nearly_done_full_model.pt'\n",
    "    #model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return(model, tokenizer)\n",
    "\n",
    "def make_prediction(dataset):\n",
    "    input_ids = dataset['input_ids']\n",
    "    attention_masks = dataset['attention_mask']\n",
    "    logits = model(input_ids, attention_masks)\n",
    "    \n",
    "    logit, preds = torch.max(logits, dim=1)\n",
    "    return(int(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3067344",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc56d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenizer.tokenize('spolitik har ikke gjort noget som helst godt for klimaet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae243fb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c7fb27b06aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_eval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-f4eb4028d180>\u001b[0m in \u001b[0;36mmake_prediction\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mattention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         discriminator_hidden_states = self.electra(\n\u001b[0m\u001b[1;32m    941\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    839\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "make_prediction(small_eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee25359",
   "metadata": {},
   "source": [
    "## Trying to use &tals algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a683910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model_checkpoint = 'Maltehb/-l-ctra-danish-electra-small-cased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "    model = ElectraClassifier(model_checkpoint, 2)\n",
    "    model_path = 'attack/pytorch_model.bin'\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return(model, tokenizer)\n",
    "\n",
    "def make_prediction(text):\n",
    "    tokenized_text = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids = tokenized_text['input_ids']\n",
    "    attention_masks = tokenized_text['attention_mask']\n",
    "    logits = model(input_ids, attention_masks)\n",
    "    \n",
    "    logit, preds = torch.max(logits, dim=1)\n",
    "    return(int(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca4791d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Maltehb/-l-ctra-danish-electra-small-cased were not used when initializing ElectraModel: ['generator.encoder.layer.1.intermediate.dense.weight', 'generator.encoder.layer.5.attention.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.key.weight', 'generator.encoder.layer.10.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.intermediate.dense.weight', 'generator.encoder.layer.2.attention.self.key.weight', 'generator.encoder.layer.3.output.dense.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.bias', 'discriminator_predictions.dense.weight', 'generator.encoder.layer.7.output.dense.bias', 'generator.encoder.layer.4.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.value.weight', 'generator.encoder.layer.8.output.dense.weight', 'generator.encoder.layer.5.attention.self.value.bias', 'generator.encoder.layer.6.attention.self.value.weight', 'generator_predictions.decoder.bias', 'generator.encoder.layer.1.output.dense.bias', 'generator.encoder.layer.3.attention.output.dense.weight', 'generator.encoder.layer.2.intermediate.dense.bias', 'generator.embeddings.LayerNorm.bias', 'generator.encoder.layer.6.output.LayerNorm.weight', 'generator.encoder.layer.1.attention.self.query.bias', 'generator.encoder.layer.8.output.LayerNorm.bias', 'generator.encoder.layer.7.attention.self.query.bias', 'generator.encoder.layer.2.attention.output.dense.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.weight', 'generator.encoder.layer.3.intermediate.dense.bias', 'generator.encoder.layer.10.output.LayerNorm.bias', 'generator.encoder.layer.6.intermediate.dense.bias', 'generator.encoder.layer.3.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.query.bias', 'generator.encoder.layer.2.attention.self.key.bias', 'generator.encoder.layer.11.output.LayerNorm.weight', 'generator.encoder.layer.8.attention.self.key.weight', 'generator.encoder.layer.4.output.LayerNorm.weight', 'generator.encoder.layer.4.attention.self.query.bias', 'generator.encoder.layer.5.attention.self.query.bias', 'generator.encoder.layer.11.attention.output.dense.weight', 'generator.encoder.layer.10.attention.self.query.weight', 'generator.encoder.layer.8.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.dense.weight', 'generator.encoder.layer.4.output.dense.weight', 'generator.encoder.layer.9.attention.output.dense.weight', 'generator.encoder.layer.9.attention.self.key.bias', 'generator.encoder.layer.6.intermediate.dense.weight', 'generator.encoder.layer.9.output.LayerNorm.bias', 'generator.encoder.layer.5.output.dense.weight', 'generator.encoder.layer.1.attention.output.dense.bias', 'generator.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.query.bias', 'generator.encoder.layer.10.attention.self.value.weight', 'generator.encoder.layer.4.attention.self.value.weight', 'generator.encoder.layer.2.attention.self.value.bias', 'generator.encoder.layer.9.attention.self.value.bias', 'generator_predictions.LayerNorm.bias', 'generator.encoder.layer.6.output.dense.bias', 'generator.embeddings_project.weight', 'generator.encoder.layer.5.attention.self.key.bias', 'generator.encoder.layer.0.attention.self.key.bias', 'generator.encoder.layer.1.attention.self.query.weight', 'generator.encoder.layer.8.attention.self.key.bias', 'generator.encoder.layer.11.intermediate.dense.bias', 'generator.encoder.layer.9.output.LayerNorm.weight', 'generator.encoder.layer.7.attention.output.dense.bias', 'generator.encoder.layer.1.output.dense.weight', 'discriminator_predictions.dense.bias', 'generator.encoder.layer.3.attention.self.key.weight', 'generator.encoder.layer.9.attention.self.value.weight', 'generator.encoder.layer.0.attention.output.dense.bias', 'generator.encoder.layer.5.output.LayerNorm.bias', 'generator.encoder.layer.11.intermediate.dense.weight', 'generator.encoder.layer.3.output.dense.weight', 'generator.encoder.layer.9.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.weight', 'generator.encoder.layer.8.attention.self.value.bias', 'generator.encoder.layer.3.output.LayerNorm.bias', 'generator.encoder.layer.9.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.value.bias', 'discriminator_predictions.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.key.bias', 'generator.encoder.layer.2.output.dense.bias', 'generator.encoder.layer.9.output.dense.bias', 'generator.encoder.layer.5.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.intermediate.dense.bias', 'generator.encoder.layer.8.attention.self.value.weight', 'generator.encoder.layer.10.attention.self.key.weight', 'generator.encoder.layer.5.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.value.weight', 'generator.encoder.layer.10.attention.output.dense.bias', 'generator.encoder.layer.8.attention.self.query.bias', 'generator.encoder.layer.0.output.dense.bias', 'generator.encoder.layer.10.output.dense.bias', 'generator.encoder.layer.1.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.value.weight', 'generator.encoder.layer.2.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.self.value.weight', 'generator.encoder.layer.11.attention.self.value.bias', 'generator.encoder.layer.9.intermediate.dense.bias', 'generator.encoder.layer.7.attention.self.query.weight', 'generator.encoder.layer.10.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.weight', 'generator.encoder.layer.5.attention.output.dense.bias', 'generator.encoder.layer.9.attention.self.query.weight', 'generator.encoder.layer.9.attention.output.dense.bias', 'generator.encoder.layer.9.output.dense.weight', 'generator.encoder.layer.1.output.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator.encoder.layer.2.attention.output.dense.bias', 'generator.encoder.layer.10.attention.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.key.weight', 'discriminator_predictions.classifier.weight', 'generator.encoder.layer.7.attention.output.LayerNorm.bias', 'generator.encoder.layer.10.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.key.weight', 'generator.encoder.layer.6.output.dense.weight', 'generator.encoder.layer.4.attention.self.key.weight', 'generator.encoder.layer.2.intermediate.dense.weight', 'generator.embeddings.word_embeddings.weight', 'discriminator_predictions.classifier.bias', 'generator.encoder.layer.1.attention.self.value.bias', 'generator.encoder.layer.3.attention.self.key.bias', 'generator.encoder.layer.0.output.dense.weight', 'generator.encoder.layer.8.attention.self.query.weight', 'generator.encoder.layer.7.attention.self.key.weight', 'generator_predictions.bias', 'generator.encoder.layer.3.intermediate.dense.weight', 'generator.encoder.layer.10.output.dense.weight', 'generator.encoder.layer.8.output.LayerNorm.weight', 'generator.encoder.layer.10.intermediate.dense.bias', 'generator.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.self.query.bias', 'generator.encoder.layer.7.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.output.dense.weight', 'generator.encoder.layer.3.attention.self.query.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.query.bias', 'generator.encoder.layer.4.attention.self.value.bias', 'generator.encoder.layer.4.intermediate.dense.weight', 'generator.encoder.layer.7.attention.self.value.weight', 'generator.encoder.layer.1.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.bias', 'generator.encoder.layer.4.attention.self.query.weight', 'discriminator_predictions.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.query.weight', 'generator.encoder.layer.10.attention.self.value.bias', 'generator.encoder.layer.4.intermediate.dense.bias', 'generator.encoder.layer.8.attention.output.dense.bias', 'generator.encoder.layer.1.intermediate.dense.bias', 'generator.encoder.layer.8.intermediate.dense.weight', 'generator.encoder.layer.9.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.dense.bias', 'generator.encoder.layer.11.attention.output.dense.bias', 'generator.encoder.layer.8.attention.output.dense.weight', 'generator.encoder.layer.8.output.dense.bias', 'generator.encoder.layer.7.attention.self.key.bias', 'generator.encoder.layer.5.attention.self.key.weight', 'generator.encoder.layer.7.attention.self.value.bias', 'generator.encoder.layer.0.attention.self.value.bias', 'generator.encoder.layer.5.output.dense.bias', 'generator.embeddings_project.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.weight', 'generator.embeddings.position_embeddings.weight', 'generator.embeddings.LayerNorm.weight', 'generator.encoder.layer.9.attention.self.key.weight', 'generator.embeddings.token_type_embeddings.weight', 'generator.encoder.layer.4.attention.output.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator.encoder.layer.2.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.bias', 'generator_predictions.dense.weight', 'generator_predictions.decoder.weight', 'generator.encoder.layer.7.attention.output.dense.weight', 'generator.encoder.layer.4.attention.self.key.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.bias', 'generator.encoder.layer.0.intermediate.dense.weight', 'generator.encoder.layer.3.attention.self.query.weight', 'generator.encoder.layer.3.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.self.value.bias', 'generator.encoder.layer.2.output.dense.weight', 'generator.encoder.layer.3.attention.self.value.weight', 'generator.encoder.layer.7.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.query.weight', 'generator.encoder.layer.4.output.dense.bias', 'generator.encoder.layer.5.attention.self.query.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.value.weight', 'generator.encoder.layer.11.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.output.dense.weight', 'generator.encoder.layer.6.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.query.weight', 'generator.encoder.layer.6.attention.self.key.bias', 'generator.encoder.layer.9.attention.self.query.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.dense.weight', 'generator.encoder.layer.6.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.weight', 'generator.encoder.layer.0.attention.self.key.weight', 'generator.encoder.layer.1.attention.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.bias', 'generator.encoder.layer.1.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "370b7f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction('Helt sikkert din torsk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc75ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = \"models/Ælæctra_uncased_32k/pytorch_model.bin\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/-l-ctra-danish-electra-small-uncased-ner-dane\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"models/Ælæctra_uncased_32k/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab49dd4",
   "metadata": {},
   "source": [
    "## Trying to finetune Ælæctra in the same way that &TAL did it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78acfdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_model_name, num_labels=4):\n",
    "        super(ElectraClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.electra = ElectraModel.from_pretrained(pretrained_model_name)\n",
    "        self.dense = nn.Linear(self.electra.config.hidden_size, self.electra.config.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.electra.config.hidden_dropout_prob)\n",
    "        self.out_proj = nn.Linear(self.electra.config.hidden_size, self.num_labels)\n",
    "\n",
    "    def classifier(self, sequence_output):\n",
    "        x = sequence_output[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.dense(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.dense(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.dense(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.out_proj(x)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        discriminator_hidden_states = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = discriminator_hidden_states[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7af06527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, text, targets, tokenizer, max_len):\n",
    "        self.text = text\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.text[item]\n",
    "        target = self.targets[item]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "          'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'targets': torch.tensor(target, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def get_data_loader(path, tokenizer, max_len, batch_size):\n",
    "    # data is stored with its context, in case we want to train a model using the context as well\n",
    "    dataset = pd.read_csv(path, index_col = 0)\n",
    "    dataset = remove_invalid_inputs(dataset, 'tweet_full_text')\n",
    "\n",
    "    data = custom_dataset(\n",
    "                    text= dataset.tweet_full_text.to_numpy(),  # used to be text\n",
    "                    targets= dataset.label.to_numpy(),  # used to be target\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_len=max_len\n",
    "                    )\n",
    "\n",
    "    sampler = RandomSampler(data)\n",
    "    dataloader = DataLoader(data,batch_size=batch_size,sampler=sampler,pin_memory=True)\n",
    "    return dataloader, data\n",
    "\n",
    "def remove_invalid_inputs(dataset,text_column):\n",
    "    'Simpel metode til at fjerne alle rækker fra en dataframe, baseret på om værdierne i en kolonne er af typen str'\n",
    "    dataset['valid'] = dataset[text_column].apply(lambda x: isinstance(x, str))\n",
    "    return dataset.loc[dataset.valid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e56fabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Maltehb/-l-ctra-danish-electra-small-cased were not used when initializing ElectraModel: ['generator.encoder.layer.1.intermediate.dense.weight', 'generator.encoder.layer.5.attention.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.key.weight', 'generator.encoder.layer.10.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.intermediate.dense.weight', 'generator.encoder.layer.2.attention.self.key.weight', 'generator.encoder.layer.3.output.dense.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.bias', 'discriminator_predictions.dense.weight', 'generator.encoder.layer.7.output.dense.bias', 'generator.encoder.layer.4.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.value.weight', 'generator.encoder.layer.8.output.dense.weight', 'generator.encoder.layer.5.attention.self.value.bias', 'generator.encoder.layer.6.attention.self.value.weight', 'generator_predictions.decoder.bias', 'generator.encoder.layer.1.output.dense.bias', 'generator.encoder.layer.3.attention.output.dense.weight', 'generator.encoder.layer.2.intermediate.dense.bias', 'generator.embeddings.LayerNorm.bias', 'generator.encoder.layer.6.output.LayerNorm.weight', 'generator.encoder.layer.1.attention.self.query.bias', 'generator.encoder.layer.8.output.LayerNorm.bias', 'generator.encoder.layer.7.attention.self.query.bias', 'generator.encoder.layer.2.attention.output.dense.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.weight', 'generator.encoder.layer.3.intermediate.dense.bias', 'generator.encoder.layer.10.output.LayerNorm.bias', 'generator.encoder.layer.6.intermediate.dense.bias', 'generator.encoder.layer.3.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.query.bias', 'generator.encoder.layer.2.attention.self.key.bias', 'generator.encoder.layer.11.output.LayerNorm.weight', 'generator.encoder.layer.8.attention.self.key.weight', 'generator.encoder.layer.4.output.LayerNorm.weight', 'generator.encoder.layer.4.attention.self.query.bias', 'generator.encoder.layer.5.attention.self.query.bias', 'generator.encoder.layer.11.attention.output.dense.weight', 'generator.encoder.layer.10.attention.self.query.weight', 'generator.encoder.layer.8.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.dense.weight', 'generator.encoder.layer.4.output.dense.weight', 'generator.encoder.layer.9.attention.output.dense.weight', 'generator.encoder.layer.9.attention.self.key.bias', 'generator.encoder.layer.6.intermediate.dense.weight', 'generator.encoder.layer.9.output.LayerNorm.bias', 'generator.encoder.layer.5.output.dense.weight', 'generator.encoder.layer.1.attention.output.dense.bias', 'generator.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.query.bias', 'generator.encoder.layer.10.attention.self.value.weight', 'generator.encoder.layer.4.attention.self.value.weight', 'generator.encoder.layer.2.attention.self.value.bias', 'generator.encoder.layer.9.attention.self.value.bias', 'generator_predictions.LayerNorm.bias', 'generator.encoder.layer.6.output.dense.bias', 'generator.embeddings_project.weight', 'generator.encoder.layer.5.attention.self.key.bias', 'generator.encoder.layer.0.attention.self.key.bias', 'generator.encoder.layer.1.attention.self.query.weight', 'generator.encoder.layer.8.attention.self.key.bias', 'generator.encoder.layer.11.intermediate.dense.bias', 'generator.encoder.layer.9.output.LayerNorm.weight', 'generator.encoder.layer.7.attention.output.dense.bias', 'generator.encoder.layer.1.output.dense.weight', 'discriminator_predictions.dense.bias', 'generator.encoder.layer.3.attention.self.key.weight', 'generator.encoder.layer.9.attention.self.value.weight', 'generator.encoder.layer.0.attention.output.dense.bias', 'generator.encoder.layer.5.output.LayerNorm.bias', 'generator.encoder.layer.11.intermediate.dense.weight', 'generator.encoder.layer.3.output.dense.weight', 'generator.encoder.layer.9.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.weight', 'generator.encoder.layer.8.attention.self.value.bias', 'generator.encoder.layer.3.output.LayerNorm.bias', 'generator.encoder.layer.9.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.value.bias', 'discriminator_predictions.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.key.bias', 'generator.encoder.layer.2.output.dense.bias', 'generator.encoder.layer.9.output.dense.bias', 'generator.encoder.layer.5.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.intermediate.dense.bias', 'generator.encoder.layer.8.attention.self.value.weight', 'generator.encoder.layer.10.attention.self.key.weight', 'generator.encoder.layer.5.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.value.weight', 'generator.encoder.layer.10.attention.output.dense.bias', 'generator.encoder.layer.8.attention.self.query.bias', 'generator.encoder.layer.0.output.dense.bias', 'generator.encoder.layer.10.output.dense.bias', 'generator.encoder.layer.1.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.value.weight', 'generator.encoder.layer.2.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.self.value.weight', 'generator.encoder.layer.11.attention.self.value.bias', 'generator.encoder.layer.9.intermediate.dense.bias', 'generator.encoder.layer.7.attention.self.query.weight', 'generator.encoder.layer.10.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.weight', 'generator.encoder.layer.5.attention.output.dense.bias', 'generator.encoder.layer.9.attention.self.query.weight', 'generator.encoder.layer.9.attention.output.dense.bias', 'generator.encoder.layer.9.output.dense.weight', 'generator.encoder.layer.1.output.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator.encoder.layer.2.attention.output.dense.bias', 'generator.encoder.layer.10.attention.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.key.weight', 'discriminator_predictions.classifier.weight', 'generator.encoder.layer.7.attention.output.LayerNorm.bias', 'generator.encoder.layer.10.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.key.weight', 'generator.encoder.layer.6.output.dense.weight', 'generator.encoder.layer.4.attention.self.key.weight', 'generator.encoder.layer.2.intermediate.dense.weight', 'generator.embeddings.word_embeddings.weight', 'discriminator_predictions.classifier.bias', 'generator.encoder.layer.1.attention.self.value.bias', 'generator.encoder.layer.3.attention.self.key.bias', 'generator.encoder.layer.0.output.dense.weight', 'generator.encoder.layer.8.attention.self.query.weight', 'generator.encoder.layer.7.attention.self.key.weight', 'generator_predictions.bias', 'generator.encoder.layer.3.intermediate.dense.weight', 'generator.encoder.layer.10.output.dense.weight', 'generator.encoder.layer.8.output.LayerNorm.weight', 'generator.encoder.layer.10.intermediate.dense.bias', 'generator.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.self.query.bias', 'generator.encoder.layer.7.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.output.dense.weight', 'generator.encoder.layer.3.attention.self.query.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.query.bias', 'generator.encoder.layer.4.attention.self.value.bias', 'generator.encoder.layer.4.intermediate.dense.weight', 'generator.encoder.layer.7.attention.self.value.weight', 'generator.encoder.layer.1.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.bias', 'generator.encoder.layer.4.attention.self.query.weight', 'discriminator_predictions.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.query.weight', 'generator.encoder.layer.10.attention.self.value.bias', 'generator.encoder.layer.4.intermediate.dense.bias', 'generator.encoder.layer.8.attention.output.dense.bias', 'generator.encoder.layer.1.intermediate.dense.bias', 'generator.encoder.layer.8.intermediate.dense.weight', 'generator.encoder.layer.9.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.dense.bias', 'generator.encoder.layer.11.attention.output.dense.bias', 'generator.encoder.layer.8.attention.output.dense.weight', 'generator.encoder.layer.8.output.dense.bias', 'generator.encoder.layer.7.attention.self.key.bias', 'generator.encoder.layer.5.attention.self.key.weight', 'generator.encoder.layer.7.attention.self.value.bias', 'generator.encoder.layer.0.attention.self.value.bias', 'generator.encoder.layer.5.output.dense.bias', 'generator.embeddings_project.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.weight', 'generator.embeddings.position_embeddings.weight', 'generator.embeddings.LayerNorm.weight', 'generator.encoder.layer.9.attention.self.key.weight', 'generator.embeddings.token_type_embeddings.weight', 'generator.encoder.layer.4.attention.output.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator.encoder.layer.2.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.bias', 'generator_predictions.dense.weight', 'generator_predictions.decoder.weight', 'generator.encoder.layer.7.attention.output.dense.weight', 'generator.encoder.layer.4.attention.self.key.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.bias', 'generator.encoder.layer.0.intermediate.dense.weight', 'generator.encoder.layer.3.attention.self.query.weight', 'generator.encoder.layer.3.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.self.value.bias', 'generator.encoder.layer.2.output.dense.weight', 'generator.encoder.layer.3.attention.self.value.weight', 'generator.encoder.layer.7.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.query.weight', 'generator.encoder.layer.4.output.dense.bias', 'generator.encoder.layer.5.attention.self.query.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.value.weight', 'generator.encoder.layer.11.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.output.dense.weight', 'generator.encoder.layer.6.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.query.weight', 'generator.encoder.layer.6.attention.self.key.bias', 'generator.encoder.layer.9.attention.self.query.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.dense.weight', 'generator.encoder.layer.6.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.weight', 'generator.encoder.layer.0.attention.self.key.weight', 'generator.encoder.layer.1.attention.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.bias', 'generator.encoder.layer.1.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ElectraClassifier('Maltehb/-l-ctra-danish-electra-small-cased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7609bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'Maltehb/-l-ctra-danish-electra-small-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72e187b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, data = get_data_loader('label6.csv', tokenizer=tokenizer, max_len=280, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e05c21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d6609d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1856ba6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Fremskrivning fra @Energistyr viser, at øget produktion af biogas gør, at vi er tættere på at nå klimamålet end forventet. Klimavenlig biogas leverer konkrete CO2-reduktioner nu og her og rummer et stort eksportpotentiale for DK. Det har jeg talt med @tv2fyn om #dkpol #dkgreen https://t.co/jqWOX3WaLM',\n",
       "  'Små skridt i den rigtige retning med ny bilaftale, men slet ikke ambitiøst nok. Godt at flere får mulighed for elbil, og at de største og mest forurenende biler bliver dyrere. Men vi er ikke færdige med at finde CO2-reduktioner på transport #dkpol #dkgreen',\n",
       "  '@tselsmark @okologidk Det danske klima er for koldt til soja, i stedet kan vi fodre dyr med protein fra græs, ærter, lupiner, hestebønner mm.',\n",
       "  '»Man skal lede med luppen«: Dan Jørgensen har fremlagt sine grønne bedrifter et år efter klimavalget 🔐 \\nhttps://t.co/ARxQno3ajg',\n",
       "  'Havde håbet at 40% af landbrugsstøtten skulle være grøn. Nu bliver det kun 20 % - hvordan kan det være godt @MogensJensenS ? . @enhedslisten ser frem til at prioritere klima, natur og økologi i den danske landbrugspolitik. #dkpol #dkgreen @okologidk https://t.co/06y3w5fNY9'],\n",
       " 'input_ids': tensor([[    2, 22143,  9046,  ...,     0,     0,     0],\n",
       "         [    2, 24495,  4953,  ...,     0,     0,     0],\n",
       "         [    2,    36,  9849,  ...,     0,     0,     0],\n",
       "         [    2,    98,  2027,  ...,     0,     0,     0],\n",
       "         [    2,  2196, 12211,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'targets': tensor([2, 1, 0, 1, 1])}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "325788ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from python engineer\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0511f616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-9fa2a6cc26c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mstep_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-24e8d2d50762>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloaders' is not defined"
     ]
    }
   ],
   "source": [
    "# freezing layers\n",
    "model = model.to(device)\n",
    "\n",
    "# can be given weights might be useful since we have uneven distribution of classes\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 7, gamma=0.1)\n",
    "\n",
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38fae49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
