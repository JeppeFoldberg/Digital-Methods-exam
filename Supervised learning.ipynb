{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "dbe310dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading in data and splitting into test and train\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import datasets\n",
    "from exam_utils import timeParser\n",
    "import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from attack.model_def import ElectraClassifier\n",
    "\n",
    "# for fine tuning in pytorch with transformers trainer api\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "import torch\n",
    "#import torch.nn.functional as F\n",
    "#import torch.nn as nn\n",
    "#from torch.utils.data import Dataset, RandomSampler, DataLoader\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d609717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('lemma_all.csv', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b5da5d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.loc[:, 'tweet_created_at'] = df_all.tweet_created_at.apply(lambda t: timeParser(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a8813ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.loc[df_all.tweet_created_at > '2019-06-05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "52872f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_all.sample(59000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2425fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.to_csv('sample_for_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e48daed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('lemma_all_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b80b69c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3d0ec83b214b51ae\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/jeppefoldberg/.cache/huggingface/datasets/csv/default-3d0ec83b214b51ae/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/jeppefoldberg/.cache/huggingface/datasets/csv/default-3d0ec83b214b51ae/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files = ['sample_for_prediction.csv'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f95b4590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'user_screen_name', 'tweet_id', 'tweet_created_at', 'tweet_full_text', 'tweet_text_lemma', 'tweet_text_lemma_reduced'],\n",
       "        num_rows: 59000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb1d953c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 100193,\n",
       " 'user_screen_name': 'JonBurgwald',\n",
       " 'tweet_id': 1341060279845720064,\n",
       " 'tweet_created_at': '2020-12-21',\n",
       " 'tweet_full_text': '.@MaiVilladsen p√• Folketingets talerstol: \"Vi har indg√•et mange gr√∏nne aftaler det sidste halve √•r. De fleste har v√¶ret fremskridt, men fremskridtene har ikke v√¶ret store nok\". Nemlig rigtigt. Vi kommer ikke udenom at gen√•bne flere af dem. #dkgreen #dkpol',\n",
       " 'tweet_text_lemma': 'folketing talerstol indg√• gr√∏n aftale sidste halv √•r fremskridt fremskridt stor rigtig udenom gen√•bne',\n",
       " 'tweet_text_lemma_reduced': 'folketing talerstol indg√• aftale √•r fremskridt fremskridt gen√•bne',\n",
       " 'max_proba': 0.2537778234568019,\n",
       " 'label': 1,\n",
       " 'Unnamed: 0.1': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f16fc484",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train'].train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "794e3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Maltehb/-l-ctra-danish-electra-small-cased\")\n",
    "\n",
    "# tokenizing the datasets\n",
    "def tokenize_function(examples):\n",
    "    # pads or truncates the text so it fits with the maximum length the nn can take\n",
    "    return tokenizer(examples['tweet_full_text'], max_length = 512, padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f35b4e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8a7da400904bebb3fb70f911c0c9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=59.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "207bfb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets\n",
    "sample_predict_dataset = tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a750c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Maltehb/-l-ctra-danish-electra-small-cased were not used when initializing ElectraForSequenceClassification: ['generator.encoder.layer.4.intermediate.dense.weight', 'generator.encoder.layer.9.output.dense.bias', 'discriminator_predictions.dense.bias', 'generator.embeddings.token_type_embeddings.weight', 'generator.encoder.layer.0.attention.self.value.weight', 'generator.encoder.layer.1.attention.self.value.weight', 'generator.encoder.layer.6.output.dense.weight', 'generator.encoder.layer.2.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.bias', 'generator.encoder.layer.8.output.dense.bias', 'generator.encoder.layer.6.output.LayerNorm.bias', 'generator.encoder.layer.7.attention.self.key.weight', 'generator.encoder.layer.3.attention.output.dense.bias', 'generator.encoder.layer.4.attention.output.dense.weight', 'generator.encoder.layer.10.attention.self.key.bias', 'generator.encoder.layer.5.attention.self.key.bias', 'generator.encoder.layer.6.attention.self.value.bias', 'generator.encoder.layer.8.output.LayerNorm.weight', 'generator.encoder.layer.2.output.dense.weight', 'generator.encoder.layer.3.intermediate.dense.weight', 'generator.encoder.layer.10.attention.output.LayerNorm.bias', 'generator.encoder.layer.9.attention.self.query.bias', 'generator.encoder.layer.4.attention.self.value.weight', 'generator.encoder.layer.6.attention.output.LayerNorm.bias', 'generator.encoder.layer.10.intermediate.dense.weight', 'generator.encoder.layer.1.attention.output.dense.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.value.bias', 'generator.encoder.layer.9.attention.self.query.weight', 'generator.encoder.layer.0.intermediate.dense.weight', 'generator.embeddings_project.weight', 'generator.encoder.layer.6.intermediate.dense.bias', 'generator.encoder.layer.3.attention.self.key.bias', 'generator.encoder.layer.11.attention.output.dense.weight', 'generator.encoder.layer.8.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.weight', 'generator.encoder.layer.2.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.value.bias', 'generator.encoder.layer.6.attention.self.key.bias', 'generator.encoder.layer.11.output.LayerNorm.bias', 'generator.encoder.layer.11.intermediate.dense.bias', 'generator.encoder.layer.11.attention.self.query.bias', 'generator.encoder.layer.3.attention.self.value.bias', 'generator.encoder.layer.4.attention.output.dense.bias', 'generator.encoder.layer.3.output.dense.bias', 'generator_predictions.LayerNorm.bias', 'generator.encoder.layer.11.output.dense.bias', 'generator.encoder.layer.9.attention.self.value.bias', 'generator.encoder.layer.3.output.LayerNorm.weight', 'generator.encoder.layer.2.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.attention.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.self.key.weight', 'generator.encoder.layer.0.output.LayerNorm.weight', 'generator.encoder.layer.2.attention.output.dense.bias', 'generator.encoder.layer.5.attention.output.dense.bias', 'generator.encoder.layer.5.attention.self.query.weight', 'generator.encoder.layer.10.attention.self.query.weight', 'generator.encoder.layer.2.attention.self.key.bias', 'generator.encoder.layer.8.attention.self.key.weight', 'generator.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.self.key.bias', 'generator.encoder.layer.9.attention.output.dense.weight', 'generator.encoder.layer.6.attention.self.value.weight', 'generator.encoder.layer.4.output.dense.weight', 'generator.encoder.layer.5.output.LayerNorm.bias', 'generator.encoder.layer.9.attention.self.key.bias', 'generator.embeddings.word_embeddings.weight', 'generator.encoder.layer.9.attention.output.dense.bias', 'generator.encoder.layer.10.attention.output.dense.weight', 'generator.encoder.layer.5.intermediate.dense.weight', 'generator.embeddings_project.bias', 'generator.encoder.layer.0.output.dense.bias', 'generator.encoder.layer.7.attention.output.dense.weight', 'generator.encoder.layer.2.attention.self.key.weight', 'generator.encoder.layer.10.output.dense.bias', 'generator.encoder.layer.0.attention.self.query.weight', 'generator.encoder.layer.6.output.dense.bias', 'generator.encoder.layer.8.output.dense.weight', 'generator.encoder.layer.8.attention.output.dense.bias', 'generator.encoder.layer.0.intermediate.dense.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.value.bias', 'generator.encoder.layer.7.intermediate.dense.weight', 'generator.encoder.layer.4.attention.self.query.weight', 'generator.encoder.layer.4.attention.output.LayerNorm.weight', 'generator.encoder.layer.3.attention.self.query.bias', 'generator.encoder.layer.1.attention.self.key.bias', 'generator.encoder.layer.6.attention.self.key.weight', 'generator.encoder.layer.10.output.LayerNorm.weight', 'generator.encoder.layer.1.intermediate.dense.bias', 'discriminator_predictions.classifier.bias', 'generator.encoder.layer.5.attention.self.value.weight', 'generator.encoder.layer.8.attention.self.value.weight', 'generator.encoder.layer.5.attention.self.query.bias', 'generator.encoder.layer.5.attention.output.dense.weight', 'generator.encoder.layer.4.attention.self.value.bias', 'generator.encoder.layer.5.output.dense.bias', 'generator.encoder.layer.4.attention.self.key.weight', 'generator.encoder.layer.11.output.LayerNorm.weight', 'generator.encoder.layer.8.output.LayerNorm.bias', 'generator.encoder.layer.8.attention.self.key.bias', 'generator.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.output.dense.bias', 'generator.embeddings.LayerNorm.weight', 'generator_predictions.decoder.bias', 'generator.encoder.layer.3.attention.self.value.weight', 'generator.encoder.layer.5.attention.self.value.bias', 'generator.encoder.layer.10.attention.self.key.weight', 'generator.encoder.layer.1.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.self.key.weight', 'generator.encoder.layer.1.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.value.bias', 'generator.encoder.layer.2.attention.self.value.weight', 'generator.encoder.layer.8.attention.self.query.weight', 'generator.encoder.layer.9.output.dense.weight', 'generator_predictions.bias', 'generator.encoder.layer.11.attention.self.value.weight', 'generator.encoder.layer.10.attention.self.value.weight', 'generator.encoder.layer.6.attention.self.query.bias', 'generator.encoder.layer.3.output.LayerNorm.bias', 'generator.encoder.layer.11.intermediate.dense.weight', 'generator.encoder.layer.1.output.dense.bias', 'generator.encoder.layer.0.attention.output.dense.weight', 'generator.encoder.layer.7.attention.output.dense.bias', 'discriminator_predictions.dense.weight', 'generator.encoder.layer.0.attention.self.key.weight', 'generator.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.embeddings.position_embeddings.weight', 'generator.encoder.layer.6.output.LayerNorm.weight', 'generator.encoder.layer.2.output.dense.bias', 'generator.encoder.layer.2.intermediate.dense.bias', 'generator.encoder.layer.7.attention.self.value.weight', 'generator.encoder.layer.11.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.dense.bias', 'generator.encoder.layer.2.intermediate.dense.weight', 'generator.encoder.layer.6.intermediate.dense.weight', 'generator.encoder.layer.1.attention.self.query.bias', 'generator.encoder.layer.7.attention.output.LayerNorm.weight', 'generator.encoder.layer.4.attention.self.query.bias', 'generator.encoder.layer.10.output.LayerNorm.bias', 'generator.encoder.layer.1.output.LayerNorm.weight', 'generator.encoder.layer.3.attention.self.query.weight', 'generator.encoder.layer.9.output.LayerNorm.bias', 'generator.encoder.layer.8.attention.output.LayerNorm.weight', 'generator.encoder.layer.1.intermediate.dense.weight', 'generator.encoder.layer.5.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.output.dense.bias', 'generator.encoder.layer.7.attention.self.query.bias', 'generator.encoder.layer.3.intermediate.dense.bias', 'generator.encoder.layer.11.attention.self.query.weight', 'generator.encoder.layer.5.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.output.dense.bias', 'generator.encoder.layer.8.attention.output.LayerNorm.bias', 'generator.encoder.layer.6.attention.output.dense.weight', 'generator.encoder.layer.10.output.dense.weight', 'generator.encoder.layer.1.attention.self.query.weight', 'discriminator_predictions.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.dense.weight', 'generator.encoder.layer.7.attention.self.query.weight', 'generator.encoder.layer.7.output.dense.weight', 'generator.encoder.layer.6.attention.self.query.weight', 'generator.encoder.layer.10.intermediate.dense.bias', 'generator.encoder.layer.9.attention.output.LayerNorm.bias', 'generator.encoder.layer.10.attention.output.dense.bias', 'generator.encoder.layer.8.attention.output.dense.weight', 'generator.encoder.layer.7.output.LayerNorm.weight', 'generator.encoder.layer.4.intermediate.dense.bias', 'generator.encoder.layer.2.attention.self.value.bias', 'generator.encoder.layer.9.attention.self.value.weight', 'generator.encoder.layer.8.intermediate.dense.bias', 'generator.encoder.layer.9.intermediate.dense.bias', 'generator.encoder.layer.9.attention.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.output.LayerNorm.bias', 'generator.encoder.layer.4.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.self.key.weight', 'generator.encoder.layer.7.attention.self.key.bias', 'generator.encoder.layer.0.output.dense.weight', 'generator.encoder.layer.5.intermediate.dense.bias', 'generator.encoder.layer.1.attention.self.key.weight', 'generator.encoder.layer.4.attention.self.key.bias', 'generator_predictions.dense.weight', 'generator.encoder.layer.0.attention.self.query.bias', 'generator.encoder.layer.4.output.dense.bias', 'generator.encoder.layer.0.attention.self.key.bias', 'generator.encoder.layer.4.output.LayerNorm.weight', 'discriminator_predictions.LayerNorm.weight', 'generator.encoder.layer.1.output.dense.weight', 'generator.encoder.layer.8.attention.self.value.bias', 'generator_predictions.LayerNorm.weight', 'generator_predictions.decoder.weight', 'generator.embeddings.LayerNorm.bias', 'generator.encoder.layer.2.attention.output.dense.weight', 'generator.encoder.layer.3.output.dense.weight', 'generator.encoder.layer.5.output.dense.weight', 'generator.encoder.layer.10.attention.self.query.bias', 'generator.encoder.layer.7.attention.self.value.bias', 'generator.encoder.layer.10.attention.output.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator.encoder.layer.2.attention.self.query.bias', 'generator.encoder.layer.7.output.LayerNorm.bias', 'generator.encoder.layer.5.attention.output.LayerNorm.bias', 'generator.encoder.layer.9.output.LayerNorm.weight', 'generator.encoder.layer.7.intermediate.dense.bias', 'generator.encoder.layer.9.intermediate.dense.weight', 'generator.encoder.layer.2.attention.self.query.weight', 'discriminator_predictions.classifier.weight', 'generator.encoder.layer.9.attention.self.key.weight', 'generator.encoder.layer.8.attention.self.query.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at Maltehb/-l-ctra-danish-electra-small-cased and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# creating the model for finetuning\n",
    "model = AutoModelForSequenceClassification.from_pretrained('Maltehb/-l-ctra-danish-electra-small-cased', num_labels=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f4e7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(8))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(8))\n",
    "full_train_dataset = tokenized_datasets['train']\n",
    "full_eval_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb4e3ca",
   "metadata": {},
   "source": [
    "### Training the classifier with our data\n",
    "We do not freeze since this is not good practice with ü§ó-transformers! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='final_results',\n",
    "    num_train_epochs=30,\n",
    "    evaluation_strategy='epoch',      # computes metrics every epoch!\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.02,               # strength of weight decay higher means less overfitting\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10,\n",
    "    metric_for_best_model='accuracy'\n",
    ")\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "cb = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=full_train_dataset,\n",
    "    eval_dataset=full_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = cb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4fb12f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='832' max='1560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 832/1560 4:11:55 < 3:40:58, 0.05 it/s, Epoch 16/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.377300</td>\n",
       "      <td>1.368871</td>\n",
       "      <td>0.349091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.366500</td>\n",
       "      <td>1.362964</td>\n",
       "      <td>0.349091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.375400</td>\n",
       "      <td>1.354841</td>\n",
       "      <td>0.349091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.348400</td>\n",
       "      <td>1.338100</td>\n",
       "      <td>0.349091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.321200</td>\n",
       "      <td>1.297163</td>\n",
       "      <td>0.429091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.188200</td>\n",
       "      <td>1.219107</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.106800</td>\n",
       "      <td>1.180200</td>\n",
       "      <td>0.538182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.956300</td>\n",
       "      <td>1.108454</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.794200</td>\n",
       "      <td>1.095311</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.581600</td>\n",
       "      <td>1.098373</td>\n",
       "      <td>0.549091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.477400</td>\n",
       "      <td>0.970945</td>\n",
       "      <td>0.603636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.308300</td>\n",
       "      <td>1.269027</td>\n",
       "      <td>0.530909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>1.246725</td>\n",
       "      <td>0.556364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.221600</td>\n",
       "      <td>1.297854</td>\n",
       "      <td>0.581818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>1.650669</td>\n",
       "      <td>0.534545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>1.678593</td>\n",
       "      <td>0.556364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=832, training_loss=0.8217360883360155, metrics={'train_runtime': 15136.4916, 'train_samples_per_second': 0.103, 'total_flos': 0, 'epoch': 16.0})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2b55fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraForSequenceClassification(\n",
      "  (electra): ElectraModel(\n",
      "    (embeddings): ElectraEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (encoder): ElectraEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): ElectraClassificationHead(\n",
      "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=256, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd1c7d54",
   "metadata": {},
   "source": [
    "## predicting labels for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "734d4f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(trainer, dataset):\n",
    "    predict = trainer.predict(dataset)\n",
    "    print('Done with the first part')\n",
    "    labels = [np.argmax(predict.predictions[i]) for i in range(len(predict.predictions))]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "06c172c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the first part\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = get_labels(trainer, sample_predict_dataset)\n",
    "os.system('say \"jeg er f√¶rdig\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b5f28367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59000"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "30c6e362",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_predicted = sample_predict_dataset.add_column('label_pred', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2cc9d215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328448359"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_predicted.to_csv('full_59000_predicted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c5af9916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'attention_mask', 'input_ids', 'token_type_ids', 'tweet_created_at', 'tweet_full_text', 'tweet_id', 'tweet_text_lemma', 'tweet_text_lemma_reduced', 'user_screen_name', 'label_pred'],\n",
       "    num_rows: 59000\n",
       "})"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0b22c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'saving_models_attempt/full_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83ac7f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model_checkpoint = 'Maltehb/-l-ctra-danish-electra-small-cased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "    \n",
    "    model = torch.load('saving_models_attempt/nearly_done_full_model.pt')\n",
    "\n",
    "    #model = ElectraClassifier(model_checkpoint, 4)\n",
    "    #model_path = 'nearly_done_full_model.pt'\n",
    "    #model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return(model, tokenizer)\n",
    "\n",
    "def make_prediction(dataset):\n",
    "    input_ids = dataset['input_ids']\n",
    "    attention_masks = dataset['attention_mask']\n",
    "    logits = model(input_ids, attention_masks)\n",
    "    \n",
    "    logit, preds = torch.max(logits, dim=1)\n",
    "    return(int(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05093cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c4f7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenizer.tokenize('spolitik har ikke gjort noget som helst godt for klimaet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee25359",
   "metadata": {},
   "source": [
    "## Trying to use &tals algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a683910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model_checkpoint = 'Maltehb/-l-ctra-danish-electra-small-cased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "    model = ElectraClassifier(model_checkpoint, 2)\n",
    "    model_path = 'attack/pytorch_model.bin'\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return(model, tokenizer)\n",
    "\n",
    "def make_prediction(text):\n",
    "    tokenized_text = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids = tokenized_text['input_ids']\n",
    "    attention_masks = tokenized_text['attention_mask']\n",
    "    logits = model(input_ids, attention_masks)\n",
    "    \n",
    "    logit, preds = torch.max(logits, dim=1)\n",
    "    return(int(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca4791d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Maltehb/-l-ctra-danish-electra-small-cased were not used when initializing ElectraModel: ['generator.encoder.layer.1.intermediate.dense.weight', 'generator.encoder.layer.5.attention.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.key.weight', 'generator.encoder.layer.10.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.intermediate.dense.weight', 'generator.encoder.layer.2.attention.self.key.weight', 'generator.encoder.layer.3.output.dense.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.bias', 'discriminator_predictions.dense.weight', 'generator.encoder.layer.7.output.dense.bias', 'generator.encoder.layer.4.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.value.weight', 'generator.encoder.layer.8.output.dense.weight', 'generator.encoder.layer.5.attention.self.value.bias', 'generator.encoder.layer.6.attention.self.value.weight', 'generator_predictions.decoder.bias', 'generator.encoder.layer.1.output.dense.bias', 'generator.encoder.layer.3.attention.output.dense.weight', 'generator.encoder.layer.2.intermediate.dense.bias', 'generator.embeddings.LayerNorm.bias', 'generator.encoder.layer.6.output.LayerNorm.weight', 'generator.encoder.layer.1.attention.self.query.bias', 'generator.encoder.layer.8.output.LayerNorm.bias', 'generator.encoder.layer.7.attention.self.query.bias', 'generator.encoder.layer.2.attention.output.dense.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.weight', 'generator.encoder.layer.3.intermediate.dense.bias', 'generator.encoder.layer.10.output.LayerNorm.bias', 'generator.encoder.layer.6.intermediate.dense.bias', 'generator.encoder.layer.3.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.query.bias', 'generator.encoder.layer.2.attention.self.key.bias', 'generator.encoder.layer.11.output.LayerNorm.weight', 'generator.encoder.layer.8.attention.self.key.weight', 'generator.encoder.layer.4.output.LayerNorm.weight', 'generator.encoder.layer.4.attention.self.query.bias', 'generator.encoder.layer.5.attention.self.query.bias', 'generator.encoder.layer.11.attention.output.dense.weight', 'generator.encoder.layer.10.attention.self.query.weight', 'generator.encoder.layer.8.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.dense.weight', 'generator.encoder.layer.4.output.dense.weight', 'generator.encoder.layer.9.attention.output.dense.weight', 'generator.encoder.layer.9.attention.self.key.bias', 'generator.encoder.layer.6.intermediate.dense.weight', 'generator.encoder.layer.9.output.LayerNorm.bias', 'generator.encoder.layer.5.output.dense.weight', 'generator.encoder.layer.1.attention.output.dense.bias', 'generator.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.query.bias', 'generator.encoder.layer.10.attention.self.value.weight', 'generator.encoder.layer.4.attention.self.value.weight', 'generator.encoder.layer.2.attention.self.value.bias', 'generator.encoder.layer.9.attention.self.value.bias', 'generator_predictions.LayerNorm.bias', 'generator.encoder.layer.6.output.dense.bias', 'generator.embeddings_project.weight', 'generator.encoder.layer.5.attention.self.key.bias', 'generator.encoder.layer.0.attention.self.key.bias', 'generator.encoder.layer.1.attention.self.query.weight', 'generator.encoder.layer.8.attention.self.key.bias', 'generator.encoder.layer.11.intermediate.dense.bias', 'generator.encoder.layer.9.output.LayerNorm.weight', 'generator.encoder.layer.7.attention.output.dense.bias', 'generator.encoder.layer.1.output.dense.weight', 'discriminator_predictions.dense.bias', 'generator.encoder.layer.3.attention.self.key.weight', 'generator.encoder.layer.9.attention.self.value.weight', 'generator.encoder.layer.0.attention.output.dense.bias', 'generator.encoder.layer.5.output.LayerNorm.bias', 'generator.encoder.layer.11.intermediate.dense.weight', 'generator.encoder.layer.3.output.dense.weight', 'generator.encoder.layer.9.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.weight', 'generator.encoder.layer.8.attention.self.value.bias', 'generator.encoder.layer.3.output.LayerNorm.bias', 'generator.encoder.layer.9.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.value.bias', 'discriminator_predictions.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.key.bias', 'generator.encoder.layer.2.output.dense.bias', 'generator.encoder.layer.9.output.dense.bias', 'generator.encoder.layer.5.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.intermediate.dense.bias', 'generator.encoder.layer.8.attention.self.value.weight', 'generator.encoder.layer.10.attention.self.key.weight', 'generator.encoder.layer.5.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.value.weight', 'generator.encoder.layer.10.attention.output.dense.bias', 'generator.encoder.layer.8.attention.self.query.bias', 'generator.encoder.layer.0.output.dense.bias', 'generator.encoder.layer.10.output.dense.bias', 'generator.encoder.layer.1.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.value.weight', 'generator.encoder.layer.2.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.self.value.weight', 'generator.encoder.layer.11.attention.self.value.bias', 'generator.encoder.layer.9.intermediate.dense.bias', 'generator.encoder.layer.7.attention.self.query.weight', 'generator.encoder.layer.10.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.weight', 'generator.encoder.layer.5.attention.output.dense.bias', 'generator.encoder.layer.9.attention.self.query.weight', 'generator.encoder.layer.9.attention.output.dense.bias', 'generator.encoder.layer.9.output.dense.weight', 'generator.encoder.layer.1.output.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator.encoder.layer.2.attention.output.dense.bias', 'generator.encoder.layer.10.attention.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.key.weight', 'discriminator_predictions.classifier.weight', 'generator.encoder.layer.7.attention.output.LayerNorm.bias', 'generator.encoder.layer.10.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.key.weight', 'generator.encoder.layer.6.output.dense.weight', 'generator.encoder.layer.4.attention.self.key.weight', 'generator.encoder.layer.2.intermediate.dense.weight', 'generator.embeddings.word_embeddings.weight', 'discriminator_predictions.classifier.bias', 'generator.encoder.layer.1.attention.self.value.bias', 'generator.encoder.layer.3.attention.self.key.bias', 'generator.encoder.layer.0.output.dense.weight', 'generator.encoder.layer.8.attention.self.query.weight', 'generator.encoder.layer.7.attention.self.key.weight', 'generator_predictions.bias', 'generator.encoder.layer.3.intermediate.dense.weight', 'generator.encoder.layer.10.output.dense.weight', 'generator.encoder.layer.8.output.LayerNorm.weight', 'generator.encoder.layer.10.intermediate.dense.bias', 'generator.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.self.query.bias', 'generator.encoder.layer.7.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.output.dense.weight', 'generator.encoder.layer.3.attention.self.query.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.query.bias', 'generator.encoder.layer.4.attention.self.value.bias', 'generator.encoder.layer.4.intermediate.dense.weight', 'generator.encoder.layer.7.attention.self.value.weight', 'generator.encoder.layer.1.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.bias', 'generator.encoder.layer.4.attention.self.query.weight', 'discriminator_predictions.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.query.weight', 'generator.encoder.layer.10.attention.self.value.bias', 'generator.encoder.layer.4.intermediate.dense.bias', 'generator.encoder.layer.8.attention.output.dense.bias', 'generator.encoder.layer.1.intermediate.dense.bias', 'generator.encoder.layer.8.intermediate.dense.weight', 'generator.encoder.layer.9.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.dense.bias', 'generator.encoder.layer.11.attention.output.dense.bias', 'generator.encoder.layer.8.attention.output.dense.weight', 'generator.encoder.layer.8.output.dense.bias', 'generator.encoder.layer.7.attention.self.key.bias', 'generator.encoder.layer.5.attention.self.key.weight', 'generator.encoder.layer.7.attention.self.value.bias', 'generator.encoder.layer.0.attention.self.value.bias', 'generator.encoder.layer.5.output.dense.bias', 'generator.embeddings_project.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.weight', 'generator.embeddings.position_embeddings.weight', 'generator.embeddings.LayerNorm.weight', 'generator.encoder.layer.9.attention.self.key.weight', 'generator.embeddings.token_type_embeddings.weight', 'generator.encoder.layer.4.attention.output.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator.encoder.layer.2.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.bias', 'generator_predictions.dense.weight', 'generator_predictions.decoder.weight', 'generator.encoder.layer.7.attention.output.dense.weight', 'generator.encoder.layer.4.attention.self.key.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.bias', 'generator.encoder.layer.0.intermediate.dense.weight', 'generator.encoder.layer.3.attention.self.query.weight', 'generator.encoder.layer.3.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.self.value.bias', 'generator.encoder.layer.2.output.dense.weight', 'generator.encoder.layer.3.attention.self.value.weight', 'generator.encoder.layer.7.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.query.weight', 'generator.encoder.layer.4.output.dense.bias', 'generator.encoder.layer.5.attention.self.query.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.value.weight', 'generator.encoder.layer.11.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.output.dense.weight', 'generator.encoder.layer.6.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.query.weight', 'generator.encoder.layer.6.attention.self.key.bias', 'generator.encoder.layer.9.attention.self.query.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.dense.weight', 'generator.encoder.layer.6.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.weight', 'generator.encoder.layer.0.attention.self.key.weight', 'generator.encoder.layer.1.attention.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.bias', 'generator.encoder.layer.1.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "370b7f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction('Helt sikkert din torsk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc75ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = \"models/√Ül√¶ctra_uncased_32k/pytorch_model.bin\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/-l-ctra-danish-electra-small-uncased-ner-dane\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"models/√Ül√¶ctra_uncased_32k/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab49dd4",
   "metadata": {},
   "source": [
    "## Trying to finetune √Ül√¶ctra in the same way that &TAL did it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78acfdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_model_name, num_labels=4):\n",
    "        super(ElectraClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.electra = ElectraModel.from_pretrained(pretrained_model_name)\n",
    "        self.dense = nn.Linear(self.electra.config.hidden_size, self.electra.config.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.electra.config.hidden_dropout_prob)\n",
    "        self.out_proj = nn.Linear(self.electra.config.hidden_size, self.num_labels)\n",
    "\n",
    "    def classifier(self, sequence_output):\n",
    "        x = sequence_output[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.dense(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.dense(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.dense(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.out_proj(x)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        discriminator_hidden_states = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = discriminator_hidden_states[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7af06527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, text, targets, tokenizer, max_len):\n",
    "        self.text = text\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.text[item]\n",
    "        target = self.targets[item]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "          'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'targets': torch.tensor(target, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def get_data_loader(path, tokenizer, max_len, batch_size):\n",
    "    # data is stored with its context, in case we want to train a model using the context as well\n",
    "    dataset = pd.read_csv(path, index_col = 0)\n",
    "    dataset = remove_invalid_inputs(dataset, 'tweet_full_text')\n",
    "\n",
    "    data = custom_dataset(\n",
    "                    text= dataset.tweet_full_text.to_numpy(),  # used to be text\n",
    "                    targets= dataset.label.to_numpy(),  # used to be target\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_len=max_len\n",
    "                    )\n",
    "\n",
    "    sampler = RandomSampler(data)\n",
    "    dataloader = DataLoader(data,batch_size=batch_size,sampler=sampler,pin_memory=True)\n",
    "    return dataloader, data\n",
    "\n",
    "def remove_invalid_inputs(dataset,text_column):\n",
    "    'Simpel metode til at fjerne alle r√¶kker fra en dataframe, baseret p√• om v√¶rdierne i en kolonne er af typen str'\n",
    "    dataset['valid'] = dataset[text_column].apply(lambda x: isinstance(x, str))\n",
    "    return dataset.loc[dataset.valid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7609bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'Maltehb/-l-ctra-danish-electra-small-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72e187b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, data = get_data_loader('label6.csv', tokenizer=tokenizer, max_len=280, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e05c21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d6609d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataiter.next()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
