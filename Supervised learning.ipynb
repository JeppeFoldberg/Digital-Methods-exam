{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dbe310dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading in data and splitting into test and train\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import datasets\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from attack.model_def import ElectraClassifier\n",
    "\n",
    "# for fine tuning in pytorch with transformers trainer api\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "#from transformers import ElectraModel\n",
    "import torch\n",
    "#import torch.nn.functional as F\n",
    "#import torch.nn as nn\n",
    "#from torch.utils.data import Dataset, RandomSampler, DataLoader\n",
    "\n",
    "# python engineer\n",
    "#from torch.optim import lr_scheduler\n",
    "#import time\n",
    "#import os\n",
    "#import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97a99386",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('label8.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8eb4c503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 8)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "88d31a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ['tweet_full_text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c5c0a8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b90d9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('traindata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "94f8a62e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b1f8101fa99dd650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/jeppefoldberg/.cache/huggingface/datasets/csv/default-b1f8101fa99dd650/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/jeppefoldberg/.cache/huggingface/datasets/csv/default-b1f8101fa99dd650/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files = ['traindata.csv'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f95b4590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tweet_full_text', 'label'],\n",
       "        num_rows: 900\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eb1d953c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweet_full_text': 'Godt at @regeringDK (efter moderat pres) også gør sig grønne tanker - men nu må vi se på de konkrete tiltag. Det bliver en spændende dag i morgen. #dkpol #dkgreen https://t.co/vqwvEo0htA',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f16fc484",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train'].train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "794e3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Maltehb/-l-ctra-danish-electra-small-cased\")\n",
    "\n",
    "# tokenizing the datasets\n",
    "def tokenize_function(examples):\n",
    "    # pads or truncates the text so it fits with the maximum length the nn can take\n",
    "    return tokenizer(examples['tweet_full_text'], max_length = 280, padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f35b4e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3249cdf2f944ed2a49544704acebebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0408f15f9b4dd9b8f3d92aeac639a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "207bfb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label', 'token_type_ids', 'tweet_full_text'],\n",
       "        num_rows: 675\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label', 'token_type_ids', 'tweet_full_text'],\n",
       "        num_rows: 225\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a750c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Maltehb/-l-ctra-danish-electra-small-cased were not used when initializing ElectraForSequenceClassification: ['generator.encoder.layer.3.attention.output.dense.weight', 'generator.encoder.layer.6.output.LayerNorm.bias', 'generator.embeddings.LayerNorm.bias', 'generator.encoder.layer.5.attention.output.LayerNorm.bias', 'generator.encoder.layer.7.attention.self.query.weight', 'generator.encoder.layer.0.attention.self.key.bias', 'generator.encoder.layer.5.output.LayerNorm.bias', 'generator.encoder.layer.7.attention.output.LayerNorm.bias', 'generator.encoder.layer.11.intermediate.dense.bias', 'generator.encoder.layer.4.attention.output.dense.bias', 'generator.encoder.layer.1.intermediate.dense.weight', 'generator.embeddings.token_type_embeddings.weight', 'generator.encoder.layer.4.output.LayerNorm.bias', 'generator.encoder.layer.5.attention.self.query.weight', 'generator.encoder.layer.5.attention.self.query.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.bias', 'generator_predictions.decoder.weight', 'generator.encoder.layer.1.attention.output.dense.bias', 'generator.embeddings.position_embeddings.weight', 'generator.encoder.layer.4.output.dense.bias', 'generator.encoder.layer.6.output.LayerNorm.weight', 'generator.encoder.layer.0.output.LayerNorm.weight', 'generator.encoder.layer.4.attention.output.dense.weight', 'generator.encoder.layer.9.intermediate.dense.weight', 'generator.encoder.layer.1.attention.self.value.weight', 'generator.encoder.layer.10.output.LayerNorm.weight', 'generator.encoder.layer.10.output.dense.bias', 'generator.encoder.layer.6.output.dense.weight', 'generator.encoder.layer.0.attention.self.query.weight', 'generator.encoder.layer.3.intermediate.dense.bias', 'generator.encoder.layer.11.output.LayerNorm.weight', 'generator.encoder.layer.4.attention.self.value.weight', 'generator.encoder.layer.11.attention.self.key.weight', 'generator.encoder.layer.1.output.dense.weight', 'generator.encoder.layer.5.attention.self.key.bias', 'generator.encoder.layer.8.output.dense.weight', 'generator.encoder.layer.0.attention.self.key.weight', 'generator.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.encoder.layer.6.intermediate.dense.weight', 'generator.encoder.layer.10.intermediate.dense.bias', 'generator_predictions.dense.bias', 'generator.encoder.layer.0.attention.self.value.weight', 'generator.encoder.layer.4.output.LayerNorm.weight', 'generator.encoder.layer.4.attention.self.key.bias', 'generator.encoder.layer.10.attention.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.self.key.weight', 'generator_predictions.bias', 'generator.encoder.layer.3.output.LayerNorm.bias', 'generator.encoder.layer.9.output.dense.bias', 'generator.encoder.layer.7.attention.self.query.bias', 'generator.encoder.layer.9.attention.self.value.bias', 'generator.encoder.layer.7.attention.output.dense.bias', 'generator.encoder.layer.11.attention.self.query.bias', 'generator_predictions.dense.weight', 'generator.encoder.layer.11.output.LayerNorm.bias', 'generator.encoder.layer.6.output.dense.bias', 'generator.encoder.layer.1.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.weight', 'discriminator_predictions.dense.weight', 'generator.encoder.layer.0.intermediate.dense.bias', 'generator.encoder.layer.6.attention.self.query.bias', 'generator.encoder.layer.9.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.output.dense.bias', 'generator.encoder.layer.5.attention.self.value.weight', 'generator.encoder.layer.10.intermediate.dense.weight', 'generator.encoder.layer.9.output.dense.weight', 'generator.encoder.layer.8.output.LayerNorm.weight', 'generator.encoder.layer.5.intermediate.dense.weight', 'generator.encoder.layer.3.output.dense.bias', 'generator.encoder.layer.2.attention.self.query.weight', 'generator.encoder.layer.7.attention.self.value.bias', 'generator.encoder.layer.8.attention.self.value.weight', 'generator.encoder.layer.7.attention.self.key.weight', 'generator.encoder.layer.9.attention.self.key.weight', 'generator.encoder.layer.11.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.bias', 'generator.encoder.layer.2.attention.output.dense.bias', 'discriminator_predictions.dense.bias', 'generator.encoder.layer.0.attention.self.query.bias', 'generator.encoder.layer.8.output.LayerNorm.bias', 'generator.encoder.layer.7.output.LayerNorm.bias', 'generator.encoder.layer.9.attention.output.dense.weight', 'generator.encoder.layer.4.attention.self.value.bias', 'generator.encoder.layer.0.output.LayerNorm.bias', 'generator.encoder.layer.11.intermediate.dense.weight', 'generator.encoder.layer.1.attention.self.query.bias', 'generator.encoder.layer.6.attention.self.value.bias', 'generator.encoder.layer.4.attention.output.LayerNorm.weight', 'generator.encoder.layer.1.attention.self.key.bias', 'generator.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.encoder.layer.6.attention.self.value.weight', 'generator.encoder.layer.2.output.LayerNorm.weight', 'generator.embeddings.LayerNorm.weight', 'generator.encoder.layer.2.attention.self.key.bias', 'generator.encoder.layer.2.output.dense.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.bias', 'generator.encoder.layer.6.attention.self.key.bias', 'generator.encoder.layer.1.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.bias', 'generator.encoder.layer.8.attention.self.query.weight', 'generator.encoder.layer.7.attention.self.key.bias', 'generator.encoder.layer.10.attention.output.dense.weight', 'generator.encoder.layer.8.attention.output.dense.weight', 'generator.encoder.layer.9.attention.self.query.weight', 'generator.encoder.layer.10.attention.output.LayerNorm.weight', 'generator.encoder.layer.1.attention.self.key.weight', 'generator.encoder.layer.1.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.dense.weight', 'generator.encoder.layer.8.attention.self.value.bias', 'generator.encoder.layer.1.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.value.bias', 'generator.encoder.layer.10.output.dense.weight', 'generator.encoder.layer.3.intermediate.dense.weight', 'generator.encoder.layer.5.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.attention.output.dense.bias', 'generator.encoder.layer.2.attention.self.value.bias', 'generator.encoder.layer.7.attention.self.value.weight', 'generator.encoder.layer.10.attention.self.key.bias', 'generator.encoder.layer.8.attention.output.dense.bias', 'generator.encoder.layer.8.intermediate.dense.bias', 'generator.encoder.layer.1.attention.output.dense.weight', 'discriminator_predictions.classifier.weight', 'generator.encoder.layer.3.output.dense.weight', 'generator.encoder.layer.11.attention.output.LayerNorm.bias', 'generator.encoder.layer.8.intermediate.dense.weight', 'generator.encoder.layer.2.attention.output.dense.weight', 'generator.encoder.layer.11.attention.output.dense.weight', 'generator.encoder.layer.1.attention.self.query.weight', 'generator.encoder.layer.7.intermediate.dense.weight', 'generator.encoder.layer.2.intermediate.dense.bias', 'discriminator_predictions.classifier.bias', 'generator.encoder.layer.5.output.dense.bias', 'generator.encoder.layer.11.attention.self.value.weight', 'generator.encoder.layer.6.attention.self.query.weight', 'generator.encoder.layer.5.output.dense.weight', 'generator.encoder.layer.5.attention.output.dense.bias', 'generator_predictions.decoder.bias', 'generator.encoder.layer.3.attention.self.query.bias', 'generator.encoder.layer.2.attention.self.query.bias', 'generator.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.encoder.layer.2.output.dense.bias', 'generator.encoder.layer.6.attention.output.LayerNorm.weight', 'generator.embeddings_project.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.weight', 'generator.encoder.layer.4.attention.self.query.bias', 'generator.encoder.layer.9.attention.self.value.weight', 'generator.encoder.layer.3.attention.self.value.bias', 'generator.encoder.layer.11.attention.self.query.weight', 'generator.encoder.layer.11.attention.output.dense.bias', 'generator.embeddings_project.weight', 'generator.encoder.layer.10.attention.self.query.bias', 'generator.encoder.layer.8.attention.self.key.bias', 'generator.encoder.layer.9.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.output.dense.bias', 'generator.encoder.layer.8.attention.self.query.bias', 'generator.encoder.layer.10.attention.self.query.weight', 'generator.encoder.layer.9.attention.self.query.bias', 'generator.encoder.layer.0.intermediate.dense.weight', 'generator_predictions.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.key.weight', 'generator.encoder.layer.1.output.dense.bias', 'generator.encoder.layer.7.attention.output.LayerNorm.weight', 'discriminator_predictions.LayerNorm.bias', 'generator.encoder.layer.3.attention.self.value.weight', 'generator.encoder.layer.10.output.LayerNorm.bias', 'generator.encoder.layer.6.attention.output.LayerNorm.bias', 'generator.encoder.layer.9.attention.output.dense.bias', 'generator.encoder.layer.0.attention.output.dense.weight', 'generator.encoder.layer.4.output.dense.weight', 'generator.encoder.layer.5.output.LayerNorm.weight', 'generator.encoder.layer.7.attention.output.dense.weight', 'generator.encoder.layer.7.output.LayerNorm.weight', 'generator.encoder.layer.4.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.weight', 'generator.encoder.layer.6.attention.self.key.weight', 'generator.encoder.layer.11.attention.output.LayerNorm.weight', 'generator.encoder.layer.3.attention.self.query.weight', 'generator.encoder.layer.10.attention.self.value.weight', 'discriminator_predictions.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.key.weight', 'generator.encoder.layer.7.output.dense.bias', 'generator.encoder.layer.4.intermediate.dense.bias', 'generator.embeddings.word_embeddings.weight', 'generator.encoder.layer.6.intermediate.dense.bias', 'generator.encoder.layer.3.attention.self.key.bias', 'generator.encoder.layer.11.output.dense.bias', 'generator.encoder.layer.7.output.dense.weight', 'generator.encoder.layer.3.output.LayerNorm.weight', 'generator.encoder.layer.2.output.LayerNorm.bias', 'generator.encoder.layer.5.attention.self.value.bias', 'generator.encoder.layer.0.output.dense.weight', 'generator.encoder.layer.9.intermediate.dense.bias', 'generator.encoder.layer.1.attention.self.value.bias', 'generator.encoder.layer.8.attention.self.key.weight', 'generator.encoder.layer.10.attention.self.value.bias', 'generator.encoder.layer.5.attention.output.dense.weight', 'generator_predictions.LayerNorm.weight', 'generator.encoder.layer.7.intermediate.dense.bias', 'generator.encoder.layer.8.output.dense.bias', 'generator.encoder.layer.4.attention.self.query.weight', 'generator.encoder.layer.2.intermediate.dense.weight', 'generator.encoder.layer.9.attention.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.self.value.bias', 'generator.encoder.layer.9.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.dense.bias', 'generator.encoder.layer.9.attention.self.key.bias', 'generator.encoder.layer.5.attention.self.key.weight', 'generator.encoder.layer.3.attention.self.key.weight', 'generator.encoder.layer.0.output.dense.bias', 'generator.encoder.layer.2.attention.self.value.weight', 'generator.encoder.layer.3.attention.output.LayerNorm.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at Maltehb/-l-ctra-danish-electra-small-cased and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# creating the model for finetuning\n",
    "model = AutoModelForSequenceClassification.from_pretrained('Maltehb/-l-ctra-danish-electra-small-cased', num_labels=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5f4e7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(50))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(50))\n",
    "full_train_dataset = tokenized_datasets['train']\n",
    "full_eval_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cc1742e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d584b7acf7d416a90591c6b67c586e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1848.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"glue\", 'cola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "58573130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
       "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
       "Args:\n",
       "    predictions: list of predictions to score.\n",
       "        Each translation should be tokenized into a list of tokens.\n",
       "    references: list of lists of references for each translation.\n",
       "        Each reference should be tokenized into a list of tokens.\n",
       "Returns: depending on the GLUE subset, one or several of:\n",
       "    \"accuracy\": Accuracy\n",
       "    \"f1\": F1 score\n",
       "    \"pearson\": Pearson Correlation\n",
       "    \"spearmanr\": Spearman Correlation\n",
       "    \"matthews_correlation\": Matthew Correlation\n",
       "Examples:\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0, 'f1': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
       "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
       "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'matthews_correlation': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "daadafe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='test_results',\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy='epoch',      # computes metrics every epoch!\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10,\n",
    "    metric_for_best_model='accuracy'\n",
    ")\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb12f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/20 02:31 < 00:30, 0.10 it/s, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.387724</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.387674</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.389100</td>\n",
       "      <td>1.387562</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/4 00:07 < 00:14, 0.14 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "51697062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.387768268585205,\n",
       " 'eval_accuracy': 0.14,\n",
       " 'eval_runtime': 6.8318,\n",
       " 'eval_samples_per_second': 7.319,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee25359",
   "metadata": {},
   "source": [
    "## Trying to use &tals algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a683910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model_checkpoint = 'Maltehb/-l-ctra-danish-electra-small-cased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "    model = ElectraClassifier(model_checkpoint,2)\n",
    "    model_path = 'attack/pytorch_model.bin'\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return(model, tokenizer)\n",
    "\n",
    "def make_prediction(text):\n",
    "    tokenized_text = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids = tokenized_text['input_ids']\n",
    "    attention_masks = tokenized_text['attention_mask']\n",
    "    logits = model(input_ids, attention_masks)\n",
    "    \n",
    "    logit,preds = torch.max(logits, dim=1)\n",
    "    return(int(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca4791d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Maltehb/-l-ctra-danish-electra-small-cased were not used when initializing ElectraModel: ['generator.encoder.layer.1.intermediate.dense.weight', 'generator.encoder.layer.5.attention.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.key.weight', 'generator.encoder.layer.10.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.intermediate.dense.weight', 'generator.encoder.layer.2.attention.self.key.weight', 'generator.encoder.layer.3.output.dense.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.bias', 'discriminator_predictions.dense.weight', 'generator.encoder.layer.7.output.dense.bias', 'generator.encoder.layer.4.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.value.weight', 'generator.encoder.layer.8.output.dense.weight', 'generator.encoder.layer.5.attention.self.value.bias', 'generator.encoder.layer.6.attention.self.value.weight', 'generator_predictions.decoder.bias', 'generator.encoder.layer.1.output.dense.bias', 'generator.encoder.layer.3.attention.output.dense.weight', 'generator.encoder.layer.2.intermediate.dense.bias', 'generator.embeddings.LayerNorm.bias', 'generator.encoder.layer.6.output.LayerNorm.weight', 'generator.encoder.layer.1.attention.self.query.bias', 'generator.encoder.layer.8.output.LayerNorm.bias', 'generator.encoder.layer.7.attention.self.query.bias', 'generator.encoder.layer.2.attention.output.dense.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.weight', 'generator.encoder.layer.3.intermediate.dense.bias', 'generator.encoder.layer.10.output.LayerNorm.bias', 'generator.encoder.layer.6.intermediate.dense.bias', 'generator.encoder.layer.3.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.query.bias', 'generator.encoder.layer.2.attention.self.key.bias', 'generator.encoder.layer.11.output.LayerNorm.weight', 'generator.encoder.layer.8.attention.self.key.weight', 'generator.encoder.layer.4.output.LayerNorm.weight', 'generator.encoder.layer.4.attention.self.query.bias', 'generator.encoder.layer.5.attention.self.query.bias', 'generator.encoder.layer.11.attention.output.dense.weight', 'generator.encoder.layer.10.attention.self.query.weight', 'generator.encoder.layer.8.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.dense.weight', 'generator.encoder.layer.4.output.dense.weight', 'generator.encoder.layer.9.attention.output.dense.weight', 'generator.encoder.layer.9.attention.self.key.bias', 'generator.encoder.layer.6.intermediate.dense.weight', 'generator.encoder.layer.9.output.LayerNorm.bias', 'generator.encoder.layer.5.output.dense.weight', 'generator.encoder.layer.1.attention.output.dense.bias', 'generator.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.query.bias', 'generator.encoder.layer.10.attention.self.value.weight', 'generator.encoder.layer.4.attention.self.value.weight', 'generator.encoder.layer.2.attention.self.value.bias', 'generator.encoder.layer.9.attention.self.value.bias', 'generator_predictions.LayerNorm.bias', 'generator.encoder.layer.6.output.dense.bias', 'generator.embeddings_project.weight', 'generator.encoder.layer.5.attention.self.key.bias', 'generator.encoder.layer.0.attention.self.key.bias', 'generator.encoder.layer.1.attention.self.query.weight', 'generator.encoder.layer.8.attention.self.key.bias', 'generator.encoder.layer.11.intermediate.dense.bias', 'generator.encoder.layer.9.output.LayerNorm.weight', 'generator.encoder.layer.7.attention.output.dense.bias', 'generator.encoder.layer.1.output.dense.weight', 'discriminator_predictions.dense.bias', 'generator.encoder.layer.3.attention.self.key.weight', 'generator.encoder.layer.9.attention.self.value.weight', 'generator.encoder.layer.0.attention.output.dense.bias', 'generator.encoder.layer.5.output.LayerNorm.bias', 'generator.encoder.layer.11.intermediate.dense.weight', 'generator.encoder.layer.3.output.dense.weight', 'generator.encoder.layer.9.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.weight', 'generator.encoder.layer.8.attention.self.value.bias', 'generator.encoder.layer.3.output.LayerNorm.bias', 'generator.encoder.layer.9.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.value.bias', 'discriminator_predictions.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.key.bias', 'generator.encoder.layer.2.output.dense.bias', 'generator.encoder.layer.9.output.dense.bias', 'generator.encoder.layer.5.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.intermediate.dense.bias', 'generator.encoder.layer.8.attention.self.value.weight', 'generator.encoder.layer.10.attention.self.key.weight', 'generator.encoder.layer.5.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.value.weight', 'generator.encoder.layer.10.attention.output.dense.bias', 'generator.encoder.layer.8.attention.self.query.bias', 'generator.encoder.layer.0.output.dense.bias', 'generator.encoder.layer.10.output.dense.bias', 'generator.encoder.layer.1.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.value.weight', 'generator.encoder.layer.2.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.self.value.weight', 'generator.encoder.layer.11.attention.self.value.bias', 'generator.encoder.layer.9.intermediate.dense.bias', 'generator.encoder.layer.7.attention.self.query.weight', 'generator.encoder.layer.10.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.weight', 'generator.encoder.layer.5.attention.output.dense.bias', 'generator.encoder.layer.9.attention.self.query.weight', 'generator.encoder.layer.9.attention.output.dense.bias', 'generator.encoder.layer.9.output.dense.weight', 'generator.encoder.layer.1.output.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator.encoder.layer.2.attention.output.dense.bias', 'generator.encoder.layer.10.attention.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.key.weight', 'discriminator_predictions.classifier.weight', 'generator.encoder.layer.7.attention.output.LayerNorm.bias', 'generator.encoder.layer.10.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.key.weight', 'generator.encoder.layer.6.output.dense.weight', 'generator.encoder.layer.4.attention.self.key.weight', 'generator.encoder.layer.2.intermediate.dense.weight', 'generator.embeddings.word_embeddings.weight', 'discriminator_predictions.classifier.bias', 'generator.encoder.layer.1.attention.self.value.bias', 'generator.encoder.layer.3.attention.self.key.bias', 'generator.encoder.layer.0.output.dense.weight', 'generator.encoder.layer.8.attention.self.query.weight', 'generator.encoder.layer.7.attention.self.key.weight', 'generator_predictions.bias', 'generator.encoder.layer.3.intermediate.dense.weight', 'generator.encoder.layer.10.output.dense.weight', 'generator.encoder.layer.8.output.LayerNorm.weight', 'generator.encoder.layer.10.intermediate.dense.bias', 'generator.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.self.query.bias', 'generator.encoder.layer.7.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.output.dense.weight', 'generator.encoder.layer.3.attention.self.query.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.query.bias', 'generator.encoder.layer.4.attention.self.value.bias', 'generator.encoder.layer.4.intermediate.dense.weight', 'generator.encoder.layer.7.attention.self.value.weight', 'generator.encoder.layer.1.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.bias', 'generator.encoder.layer.4.attention.self.query.weight', 'discriminator_predictions.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.query.weight', 'generator.encoder.layer.10.attention.self.value.bias', 'generator.encoder.layer.4.intermediate.dense.bias', 'generator.encoder.layer.8.attention.output.dense.bias', 'generator.encoder.layer.1.intermediate.dense.bias', 'generator.encoder.layer.8.intermediate.dense.weight', 'generator.encoder.layer.9.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.dense.bias', 'generator.encoder.layer.11.attention.output.dense.bias', 'generator.encoder.layer.8.attention.output.dense.weight', 'generator.encoder.layer.8.output.dense.bias', 'generator.encoder.layer.7.attention.self.key.bias', 'generator.encoder.layer.5.attention.self.key.weight', 'generator.encoder.layer.7.attention.self.value.bias', 'generator.encoder.layer.0.attention.self.value.bias', 'generator.encoder.layer.5.output.dense.bias', 'generator.embeddings_project.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.weight', 'generator.embeddings.position_embeddings.weight', 'generator.embeddings.LayerNorm.weight', 'generator.encoder.layer.9.attention.self.key.weight', 'generator.embeddings.token_type_embeddings.weight', 'generator.encoder.layer.4.attention.output.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator.encoder.layer.2.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.bias', 'generator_predictions.dense.weight', 'generator_predictions.decoder.weight', 'generator.encoder.layer.7.attention.output.dense.weight', 'generator.encoder.layer.4.attention.self.key.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.bias', 'generator.encoder.layer.0.intermediate.dense.weight', 'generator.encoder.layer.3.attention.self.query.weight', 'generator.encoder.layer.3.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.self.value.bias', 'generator.encoder.layer.2.output.dense.weight', 'generator.encoder.layer.3.attention.self.value.weight', 'generator.encoder.layer.7.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.query.weight', 'generator.encoder.layer.4.output.dense.bias', 'generator.encoder.layer.5.attention.self.query.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.value.weight', 'generator.encoder.layer.11.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.output.dense.weight', 'generator.encoder.layer.6.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.query.weight', 'generator.encoder.layer.6.attention.self.key.bias', 'generator.encoder.layer.9.attention.self.query.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.dense.weight', 'generator.encoder.layer.6.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.weight', 'generator.encoder.layer.0.attention.self.key.weight', 'generator.encoder.layer.1.attention.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.bias', 'generator.encoder.layer.1.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "370b7f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction('Helt sikkert din torsk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc75ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = \"models/Ælæctra_uncased_32k/pytorch_model.bin\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/-l-ctra-danish-electra-small-uncased-ner-dane\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"models/Ælæctra_uncased_32k/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab49dd4",
   "metadata": {},
   "source": [
    "## Trying to finetune Ælæctra in the same way that &TAL did it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78acfdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_model_name, num_labels=4):\n",
    "        super(ElectraClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.electra = ElectraModel.from_pretrained(pretrained_model_name)\n",
    "        self.dense = nn.Linear(self.electra.config.hidden_size, self.electra.config.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.electra.config.hidden_dropout_prob)\n",
    "        self.out_proj = nn.Linear(self.electra.config.hidden_size, self.num_labels)\n",
    "\n",
    "    def classifier(self, sequence_output):\n",
    "        x = sequence_output[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.dense(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.dense(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.dense(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.out_proj(x)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        discriminator_hidden_states = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = discriminator_hidden_states[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7af06527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, text, targets, tokenizer, max_len):\n",
    "        self.text = text\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.text[item]\n",
    "        target = self.targets[item]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "          'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'targets': torch.tensor(target, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def get_data_loader(path, tokenizer, max_len, batch_size):\n",
    "    # data is stored with its context, in case we want to train a model using the context as well\n",
    "    dataset = pd.read_csv(path, index_col = 0)\n",
    "    dataset = remove_invalid_inputs(dataset, 'tweet_full_text')\n",
    "\n",
    "    data = custom_dataset(\n",
    "                    text= dataset.tweet_full_text.to_numpy(),  # used to be text\n",
    "                    targets= dataset.label.to_numpy(),  # used to be target\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_len=max_len\n",
    "                    )\n",
    "\n",
    "    sampler = RandomSampler(data)\n",
    "    dataloader = DataLoader(data,batch_size=batch_size,sampler=sampler,pin_memory=True)\n",
    "    return dataloader, data\n",
    "\n",
    "def remove_invalid_inputs(dataset,text_column):\n",
    "    'Simpel metode til at fjerne alle rækker fra en dataframe, baseret på om værdierne i en kolonne er af typen str'\n",
    "    dataset['valid'] = dataset[text_column].apply(lambda x: isinstance(x, str))\n",
    "    return dataset.loc[dataset.valid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e56fabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Maltehb/-l-ctra-danish-electra-small-cased were not used when initializing ElectraModel: ['generator.encoder.layer.1.intermediate.dense.weight', 'generator.encoder.layer.5.attention.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.key.weight', 'generator.encoder.layer.10.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.intermediate.dense.weight', 'generator.encoder.layer.2.attention.self.key.weight', 'generator.encoder.layer.3.output.dense.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.bias', 'discriminator_predictions.dense.weight', 'generator.encoder.layer.7.output.dense.bias', 'generator.encoder.layer.4.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.value.weight', 'generator.encoder.layer.8.output.dense.weight', 'generator.encoder.layer.5.attention.self.value.bias', 'generator.encoder.layer.6.attention.self.value.weight', 'generator_predictions.decoder.bias', 'generator.encoder.layer.1.output.dense.bias', 'generator.encoder.layer.3.attention.output.dense.weight', 'generator.encoder.layer.2.intermediate.dense.bias', 'generator.embeddings.LayerNorm.bias', 'generator.encoder.layer.6.output.LayerNorm.weight', 'generator.encoder.layer.1.attention.self.query.bias', 'generator.encoder.layer.8.output.LayerNorm.bias', 'generator.encoder.layer.7.attention.self.query.bias', 'generator.encoder.layer.2.attention.output.dense.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.weight', 'generator.encoder.layer.3.intermediate.dense.bias', 'generator.encoder.layer.10.output.LayerNorm.bias', 'generator.encoder.layer.6.intermediate.dense.bias', 'generator.encoder.layer.3.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.query.bias', 'generator.encoder.layer.2.attention.self.key.bias', 'generator.encoder.layer.11.output.LayerNorm.weight', 'generator.encoder.layer.8.attention.self.key.weight', 'generator.encoder.layer.4.output.LayerNorm.weight', 'generator.encoder.layer.4.attention.self.query.bias', 'generator.encoder.layer.5.attention.self.query.bias', 'generator.encoder.layer.11.attention.output.dense.weight', 'generator.encoder.layer.10.attention.self.query.weight', 'generator.encoder.layer.8.intermediate.dense.bias', 'generator.encoder.layer.6.attention.output.dense.weight', 'generator.encoder.layer.4.output.dense.weight', 'generator.encoder.layer.9.attention.output.dense.weight', 'generator.encoder.layer.9.attention.self.key.bias', 'generator.encoder.layer.6.intermediate.dense.weight', 'generator.encoder.layer.9.output.LayerNorm.bias', 'generator.encoder.layer.5.output.dense.weight', 'generator.encoder.layer.1.attention.output.dense.bias', 'generator.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.query.bias', 'generator.encoder.layer.10.attention.self.value.weight', 'generator.encoder.layer.4.attention.self.value.weight', 'generator.encoder.layer.2.attention.self.value.bias', 'generator.encoder.layer.9.attention.self.value.bias', 'generator_predictions.LayerNorm.bias', 'generator.encoder.layer.6.output.dense.bias', 'generator.embeddings_project.weight', 'generator.encoder.layer.5.attention.self.key.bias', 'generator.encoder.layer.0.attention.self.key.bias', 'generator.encoder.layer.1.attention.self.query.weight', 'generator.encoder.layer.8.attention.self.key.bias', 'generator.encoder.layer.11.intermediate.dense.bias', 'generator.encoder.layer.9.output.LayerNorm.weight', 'generator.encoder.layer.7.attention.output.dense.bias', 'generator.encoder.layer.1.output.dense.weight', 'discriminator_predictions.dense.bias', 'generator.encoder.layer.3.attention.self.key.weight', 'generator.encoder.layer.9.attention.self.value.weight', 'generator.encoder.layer.0.attention.output.dense.bias', 'generator.encoder.layer.5.output.LayerNorm.bias', 'generator.encoder.layer.11.intermediate.dense.weight', 'generator.encoder.layer.3.output.dense.weight', 'generator.encoder.layer.9.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.weight', 'generator.encoder.layer.8.attention.self.value.bias', 'generator.encoder.layer.3.output.LayerNorm.bias', 'generator.encoder.layer.9.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.value.bias', 'discriminator_predictions.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.key.bias', 'generator.encoder.layer.2.output.dense.bias', 'generator.encoder.layer.9.output.dense.bias', 'generator.encoder.layer.5.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.intermediate.dense.bias', 'generator.encoder.layer.8.attention.self.value.weight', 'generator.encoder.layer.10.attention.self.key.weight', 'generator.encoder.layer.5.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.value.weight', 'generator.encoder.layer.10.attention.output.dense.bias', 'generator.encoder.layer.8.attention.self.query.bias', 'generator.encoder.layer.0.output.dense.bias', 'generator.encoder.layer.10.output.dense.bias', 'generator.encoder.layer.1.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.value.weight', 'generator.encoder.layer.2.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.self.value.weight', 'generator.encoder.layer.11.attention.self.value.bias', 'generator.encoder.layer.9.intermediate.dense.bias', 'generator.encoder.layer.7.attention.self.query.weight', 'generator.encoder.layer.10.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.weight', 'generator.encoder.layer.5.attention.output.dense.bias', 'generator.encoder.layer.9.attention.self.query.weight', 'generator.encoder.layer.9.attention.output.dense.bias', 'generator.encoder.layer.9.output.dense.weight', 'generator.encoder.layer.1.output.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator.encoder.layer.2.attention.output.dense.bias', 'generator.encoder.layer.10.attention.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.key.weight', 'discriminator_predictions.classifier.weight', 'generator.encoder.layer.7.attention.output.LayerNorm.bias', 'generator.encoder.layer.10.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.key.weight', 'generator.encoder.layer.6.output.dense.weight', 'generator.encoder.layer.4.attention.self.key.weight', 'generator.encoder.layer.2.intermediate.dense.weight', 'generator.embeddings.word_embeddings.weight', 'discriminator_predictions.classifier.bias', 'generator.encoder.layer.1.attention.self.value.bias', 'generator.encoder.layer.3.attention.self.key.bias', 'generator.encoder.layer.0.output.dense.weight', 'generator.encoder.layer.8.attention.self.query.weight', 'generator.encoder.layer.7.attention.self.key.weight', 'generator_predictions.bias', 'generator.encoder.layer.3.intermediate.dense.weight', 'generator.encoder.layer.10.output.dense.weight', 'generator.encoder.layer.8.output.LayerNorm.weight', 'generator.encoder.layer.10.intermediate.dense.bias', 'generator.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.self.query.bias', 'generator.encoder.layer.7.attention.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.output.dense.weight', 'generator.encoder.layer.3.attention.self.query.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.query.bias', 'generator.encoder.layer.4.attention.self.value.bias', 'generator.encoder.layer.4.intermediate.dense.weight', 'generator.encoder.layer.7.attention.self.value.weight', 'generator.encoder.layer.1.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.bias', 'generator.encoder.layer.4.attention.self.query.weight', 'discriminator_predictions.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.query.weight', 'generator.encoder.layer.10.attention.self.value.bias', 'generator.encoder.layer.4.intermediate.dense.bias', 'generator.encoder.layer.8.attention.output.dense.bias', 'generator.encoder.layer.1.intermediate.dense.bias', 'generator.encoder.layer.8.intermediate.dense.weight', 'generator.encoder.layer.9.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.dense.bias', 'generator.encoder.layer.11.attention.output.dense.bias', 'generator.encoder.layer.8.attention.output.dense.weight', 'generator.encoder.layer.8.output.dense.bias', 'generator.encoder.layer.7.attention.self.key.bias', 'generator.encoder.layer.5.attention.self.key.weight', 'generator.encoder.layer.7.attention.self.value.bias', 'generator.encoder.layer.0.attention.self.value.bias', 'generator.encoder.layer.5.output.dense.bias', 'generator.embeddings_project.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.weight', 'generator.embeddings.position_embeddings.weight', 'generator.embeddings.LayerNorm.weight', 'generator.encoder.layer.9.attention.self.key.weight', 'generator.embeddings.token_type_embeddings.weight', 'generator.encoder.layer.4.attention.output.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator.encoder.layer.2.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.bias', 'generator_predictions.dense.weight', 'generator_predictions.decoder.weight', 'generator.encoder.layer.7.attention.output.dense.weight', 'generator.encoder.layer.4.attention.self.key.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.bias', 'generator.encoder.layer.0.intermediate.dense.weight', 'generator.encoder.layer.3.attention.self.query.weight', 'generator.encoder.layer.3.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.self.value.bias', 'generator.encoder.layer.2.output.dense.weight', 'generator.encoder.layer.3.attention.self.value.weight', 'generator.encoder.layer.7.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.query.weight', 'generator.encoder.layer.4.output.dense.bias', 'generator.encoder.layer.5.attention.self.query.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.bias', 'generator.encoder.layer.1.attention.self.value.weight', 'generator.encoder.layer.11.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.output.dense.weight', 'generator.encoder.layer.6.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.query.weight', 'generator.encoder.layer.6.attention.self.key.bias', 'generator.encoder.layer.9.attention.self.query.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.output.dense.weight', 'generator.encoder.layer.6.output.LayerNorm.bias', 'generator.encoder.layer.0.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.weight', 'generator.encoder.layer.0.attention.self.key.weight', 'generator.encoder.layer.1.attention.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.weight', 'generator.encoder.layer.11.output.dense.bias', 'generator.encoder.layer.1.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ElectraClassifier('Maltehb/-l-ctra-danish-electra-small-cased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7609bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'Maltehb/-l-ctra-danish-electra-small-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72e187b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, data = get_data_loader('label6.csv', tokenizer=tokenizer, max_len=280, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e05c21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d6609d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1856ba6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Fremskrivning fra @Energistyr viser, at øget produktion af biogas gør, at vi er tættere på at nå klimamålet end forventet. Klimavenlig biogas leverer konkrete CO2-reduktioner nu og her og rummer et stort eksportpotentiale for DK. Det har jeg talt med @tv2fyn om #dkpol #dkgreen https://t.co/jqWOX3WaLM',\n",
       "  'Små skridt i den rigtige retning med ny bilaftale, men slet ikke ambitiøst nok. Godt at flere får mulighed for elbil, og at de største og mest forurenende biler bliver dyrere. Men vi er ikke færdige med at finde CO2-reduktioner på transport #dkpol #dkgreen',\n",
       "  '@tselsmark @okologidk Det danske klima er for koldt til soja, i stedet kan vi fodre dyr med protein fra græs, ærter, lupiner, hestebønner mm.',\n",
       "  '»Man skal lede med luppen«: Dan Jørgensen har fremlagt sine grønne bedrifter et år efter klimavalget 🔐 \\nhttps://t.co/ARxQno3ajg',\n",
       "  'Havde håbet at 40% af landbrugsstøtten skulle være grøn. Nu bliver det kun 20 % - hvordan kan det være godt @MogensJensenS ? . @enhedslisten ser frem til at prioritere klima, natur og økologi i den danske landbrugspolitik. #dkpol #dkgreen @okologidk https://t.co/06y3w5fNY9'],\n",
       " 'input_ids': tensor([[    2, 22143,  9046,  ...,     0,     0,     0],\n",
       "         [    2, 24495,  4953,  ...,     0,     0,     0],\n",
       "         [    2,    36,  9849,  ...,     0,     0,     0],\n",
       "         [    2,    98,  2027,  ...,     0,     0,     0],\n",
       "         [    2,  2196, 12211,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'targets': tensor([2, 1, 0, 1, 1])}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "325788ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from python engineer\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0511f616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-9fa2a6cc26c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mstep_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-24e8d2d50762>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloaders' is not defined"
     ]
    }
   ],
   "source": [
    "# freezing layers\n",
    "model = model.to(device)\n",
    "\n",
    "# can be given weights might be useful since we have uneven distribution of classes\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 7, gamma=0.1)\n",
    "\n",
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38fae49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
