{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "317a88cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils and general stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from exam_utils import timeParser\n",
    "import re\n",
    "\n",
    "#Packages to create DFM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "#Models to train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Packages for cross-validation and parameter tuning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ælæctra\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9910fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lemma_all.csv', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3406c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'tweet_created_at'] = df.tweet_created_at.apply(timeParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b626c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting only after the electiong and making a copy to get rid of the setting with copy warning\n",
    "df_ae = df.loc[df.tweet_created_at > '2019-06-05'].copy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a5c4b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ae.loc[:, 'tweet_id'] = df_ae.loc[:, 'tweet_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7027d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all retweets\n",
    "df_ae = df_ae.loc[~df_ae.tweet_full_text.str.contains('^RT')]\n",
    "\n",
    "# dropping nans in tweet lemma\n",
    "df_ae = df_ae.dropna(subset=['tweet_text_lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a0e55b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(df, n=100, random_state=42):\n",
    "    '''Takes in a df and returns 100 random tweets to be labelled'''\n",
    "    temp = df.sample(n, random_state=random_state)\n",
    "    temp.loc[:, 'label'] = np.nan\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdd6c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the first dataset to label!\n",
    "label = sample_dataset(df_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8d2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label.to_excel('label_this.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6886540",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_excel('label_this2.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26191160",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9769b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f013342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.loc[full_df.index.isin(temp.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d6574",
   "metadata": {},
   "source": [
    "# Active learning loop\n",
    "\n",
    "## importing data and splitting into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e86c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(path, test_data=False):\n",
    "    '''takes in the path to the latest labelled data set and returns X_train, y_train, and a df\n",
    "    could have used train_test_split'''\n",
    "    new_df = pd.read_excel(path, index_col=0)\n",
    "    X = new_df.tweet_text_lemma\n",
    "    y = new_df.label\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "    if test_data:\n",
    "        return X_train, X_test, y_train, y_test, new_df\n",
    "    else:\n",
    "        return X, y, new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f338be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelled_unlabelled(new_df, old_df):\n",
    "    '''takes in the new df and removes the ones in the new one from the old one'''\n",
    "    unlabelled_df = old_df.loc[~old_df.index.isin(new_df.index)]\n",
    "    return unlabelled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5fdce3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, labelled_df = split_data('label5.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e1718fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_df = labelled_unlabelled(labelled_df, df_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bca68945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline to train on\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "pipeline = Pipeline([ \n",
    "    ('cv', CountVectorizer(tokenizer=tokenizer.tokenize, ngram_range = (1, 2), max_df=0.999, min_df=0.01)),\n",
    "    ('tfidf', TfidfTransformer(use_idf = False)),\n",
    "    ('logreg', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88e61966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cv',\n",
       "                 CountVectorizer(max_df=0.999, min_df=0.01, ngram_range=(1, 2),\n",
       "                                 tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fefe80bf0d0>>)),\n",
       "                ('tfidf', TfidfTransformer(use_idf=False)),\n",
       "                ('logreg', LogisticRegression())])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d6ab0c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_unlabelled(pipeline, unlabelled_df):\n",
    "    '''takes in a pipeline, the unlabelled df and adds the maximum probability column\n",
    "    Then it sorts the dataframe by max proba and returns it'''\n",
    "    # predicts for the three classes for all entries in the dataset\n",
    "    predictions = pipeline.predict_proba(unlabelled_df.tweet_text_lemma)\n",
    "    # creates a column with the max probability\n",
    "    temp = unlabelled_df.copy()\n",
    "    temp.loc[:, 'max_proba'] = [max(pred) for pred in predictions]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "217e4b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_df = predict_unlabelled(pipeline, unlabelled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f10a7aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7636363636363637"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a31e3c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_new_set(unlabelled_df, labelled_df, new_name):\n",
    "    '''takes in the df produced above, sorts it and saves a new df to be labelled'''\n",
    "    unlabelled_df.sort_values(by='max_proba', inplace=True)\n",
    "    new_df = unlabelled_df[:100].copy()\n",
    "    new_df.loc[:, 'label'] = np.nan\n",
    "    new_df = pd.concat([new_df, labelled_df])\n",
    "    new_df.to_excel(f'{new_name}.xlsx')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "febfea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_new_set(unlabelled_df, labelled_df, 'label6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04431cc6",
   "metadata": {},
   "source": [
    "## Checking the current score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5629a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in the parameter values in the grid \n",
    "parameter_grid = {\n",
    "    'tfidf__use_idf': [False, True],\n",
    "    'logreg__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'logreg__C': [0.1, 0.5, 1],\n",
    "}\n",
    "\n",
    "#Initializing a kfold with 5 folds\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "#Initializing the GridSearchCV\n",
    "search = GridSearchCV(pipeline, parameter_grid, cv=cv, n_jobs = -1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d16211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a59c127b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.778"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a99de2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21572189, 0.20801173, 0.47135795, 0.10490843])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict_proba(X_train)[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65cb9d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7577e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18fb54bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[209   3   0   3]\n",
      " [ 51  75   1   5]\n",
      " [ 35   6  36   1]\n",
      " [  5   0   1  69]]\n"
     ]
    }
   ],
   "source": [
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989c714e",
   "metadata": {},
   "source": [
    "# Ælæctra classification\n",
    "\n",
    "The idea here is to use the ælectra transformer to predict the data that we have labelled during the active learning step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = \"models/Ælæctra_uncased_32k/pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a8dd2a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcb5e50b7fb45eb9826c2b2614171e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/992 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c012fd2630cf4b71965687200fdfd068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/248k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a47f68dad474f579d55cf2776d472dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ae4b07b1054c9bafd39592c7727c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/47.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322bdc3f0ccd41369cdb81f5f820f984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/54.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for 'Maltehb/-l-ctra-danish-electra-small-uncased-ner-dane' at '/Users/jeppefoldberg/.cache/huggingface/transformers/305fb4de2e990f3b6ef57bd01c5ebce5e3a4817977bebe1312d47d98d3b30908.81270b477b48320bee60c0edd1eebfc6187e8461bb1a80abd1079a4526b6217d'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/graph-tool/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graph-tool/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graph-tool/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: version_ <= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED at /Users/distiller/project/conda/conda-bld/pytorch_1579022061893/work/caffe2/serialize/inline_container.cc:132, please report a bug to PyTorch. Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2. Your PyTorch installation may be too old. (init at /Users/distiller/project/conda/conda-bld/pytorch_1579022061893/work/caffe2/serialize/inline_container.cc:132)\nframe #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 135 (0x1406c2267 in libc10.dylib)\nframe #1: caffe2::serialize::PyTorchStreamReader::init() + 2350 (0x13b59250e in libtorch.dylib)\nframe #2: caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 143 (0x13b591b5f in libtorch.dylib)\nframe #3: void pybind11::cpp_function::initialize<void pybind11::detail::initimpl::constructor<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >::execute<pybind11::class_<caffe2::serialize::PyTorchStreamReader>, 0>(pybind11::class_<caffe2::serialize::PyTorchStreamReader>&)::'lambda'(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >), void, pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::detail::is_new_style_constructor>(pybind11::class_<caffe2::serialize::PyTorchStreamReader>&&,  (*)(0...), void pybind11::detail::initimpl::constructor<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >::execute<pybind11::class_<caffe2::serialize::PyTorchStreamReader>, 0>(pybind11::class_<caffe2::serialize::PyTorchStreamReader>&)::'lambda'(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) const&...)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const + 147 (0x139570413 in libtorch_python.dylib)\nframe #4: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 3372 (0x138f6a1fc in libtorch_python.dylib)\n<omitting python frames>\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-90edef5a0f95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Maltehb/-l-ctra-danish-electra-small-uncased-ner-dane\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForTokenClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Maltehb/-l-ctra-danish-electra-small-uncased-ner-dane\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/graph-tool/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graph-tool/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for 'Maltehb/-l-ctra-danish-electra-small-uncased-ner-dane' at '/Users/jeppefoldberg/.cache/huggingface/transformers/305fb4de2e990f3b6ef57bd01c5ebce5e3a4817977bebe1312d47d98d3b30908.81270b477b48320bee60c0edd1eebfc6187e8461bb1a80abd1079a4526b6217d'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/-l-ctra-danish-electra-small-uncased-ner-dane\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"models/Ælæctra_uncased_32k/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782bf793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
