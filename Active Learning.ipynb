{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac7a9ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils and general stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from exam_utils import timeParser\n",
    "import re\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "#Packages to create DFM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "#Models to train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Packages for cross-validation and parameter tuning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c52f68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lemma_all.csv', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ead89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'tweet_created_at'] = df.tweet_created_at.apply(timeParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e5912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting only after the electiong and making a copy to get rid of the setting with copy warning\n",
    "df_ae = df.loc[df.tweet_created_at > '2019-06-05'].copy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c69a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ae.loc[:, 'tweet_id'] = df_ae.loc[:, 'tweet_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb0c4b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all retweets\n",
    "df_ae = df_ae.loc[~df_ae.tweet_full_text.str.contains('^RT')]\n",
    "\n",
    "# dropping nans in tweet lemma\n",
    "df_ae = df_ae.dropna(subset=['tweet_text_lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6f03e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(df, n=100, random_state=42):\n",
    "    '''Takes in a df and returns 100 random tweets to be labelled'''\n",
    "    temp = df.sample(n, random_state=random_state)\n",
    "    temp.loc[:, 'label'] = np.nan\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d213db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the first dataset to label!\n",
    "label = sample_dataset(df_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fdaa800",
   "metadata": {},
   "outputs": [],
   "source": [
    "label.to_excel('label_this.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938acb10",
   "metadata": {},
   "source": [
    "# Active learning loop\n",
    "\n",
    "## importing data and splitting into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ad34673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(path, test_data=False):\n",
    "    '''takes in the path to the latest labelled data set and returns X_train, y_train, and a df\n",
    "    could have used train_test_split'''\n",
    "    new_df = pd.read_excel(path, index_col=0)\n",
    "    X = new_df.tweet_text_lemma\n",
    "    y = new_df.label\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "    if test_data:\n",
    "        return X_train, X_test, y_train, y_test, new_df\n",
    "    else:\n",
    "        return X, y, new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41276e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unlabelled(new_df, old_df):\n",
    "    '''takes in the new df and removes the ones in the new one from the old one'''\n",
    "    unlabelled_df = old_df.loc[~old_df.index.isin(new_df.index)]\n",
    "    return unlabelled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "448fcd08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, labelled_df = split_data('label.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "397f5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_df = get_unlabelled(labelled_df, df_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4036c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline to train on\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "pipeline = Pipeline([ \n",
    "    ('cv', CountVectorizer(tokenizer=tokenizer.tokenize, ngram_range = (1, 2), max_df=0.999, min_df=0.01)),\n",
    "    ('tfidf', TfidfTransformer(use_idf = False)),\n",
    "    ('logreg', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87cfef87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cv',\n",
       "                 CountVectorizer(max_df=0.999, min_df=0.01, ngram_range=(1, 2),\n",
       "                                 tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7faac89cc310>>)),\n",
       "                ('tfidf', TfidfTransformer(use_idf=False)),\n",
       "                ('logreg', LogisticRegression())])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17b7eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_unlabelled(pipeline, unlabelled_df):\n",
    "    '''takes in a pipeline, the unlabelled df and adds the maximum probability column\n",
    "    Then it sorts the dataframe by max proba and returns it'''\n",
    "    # predicts for the three classes for all entries in the dataset\n",
    "    predictions = pipeline.predict_proba(unlabelled_df.tweet_text_lemma)\n",
    "    # creates a column with the max probability\n",
    "    temp = unlabelled_df.copy()\n",
    "    temp.loc[:, 'max_proba'] = [max(pred) for pred in predictions]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef110110",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_df = predict_unlabelled(pipeline, unlabelled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ee06ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_new_set(unlabelled_df, labelled_df, new_name):\n",
    "    '''takes in the df produced above, sorts it and saves a new df to be labelled'''\n",
    "    unlabelled_df.sort_values(by='max_proba', inplace=True)\n",
    "    new_df = unlabelled_df[:100].copy()\n",
    "    new_df.loc[:, 'label'] = np.nan\n",
    "    new_df = pd.concat([new_df, labelled_df])\n",
    "    new_df.to_excel(f'{new_name}.xlsx')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d99e450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_new_set(unlabelled_df, labelled_df, 'label7')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41da8c33",
   "metadata": {},
   "source": [
    "## Checking the current score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8677ae82",
   "metadata": {},
   "source": [
    "# NEW PLANS\n",
    "\n",
    "Implement function that calculates [Cohens kappa](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html) between the unlabelled set between iterations; Wiedemann proposes to stop when it reaches 0.99 after three revisions; we probably need to take a bit lower. If we do this we have actually implemented one full method from the course. This also gives us a way out of labelling data :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87f205fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohens_kappa(path_to_labelled, full_df):\n",
    "    '''takes in a labelled dict and an unlabelled dict. Fits a model for each hundred labelled entries\n",
    "    predicts on the unlabelled set and then calculates cohens kappa for each models prediction\n",
    "    and the former iterations and returns a list of cohens kappa scores for each iteration\n",
    "    Could also be augmented to print the score each iteration'''\n",
    "    df = pd.read_excel(path_to_labelled, index_col=0)\n",
    "    unlabelled_df = get_unlabelled(df, full_df)\n",
    "    X_test = unlabelled_df.tweet_text_lemma\n",
    "    predictions = []\n",
    "    \n",
    "    n = df.shape[0]\n",
    "    for i in range(100, n + 100, 100):\n",
    "        # creates a temp df with only the n lowest labelled examples\n",
    "        temp_df = df.tail(i).copy()\n",
    "        X = temp_df.tweet_text_lemma\n",
    "        y = temp_df.label\n",
    "        pipeline.fit(X, y)\n",
    "        predictions.append(pipeline.predict(X_test))\n",
    "    kappas = []\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        if (i + 1) == len(predictions):\n",
    "            break\n",
    "        else:\n",
    "            kappas.append(cohen_kappa_score(prediction, predictions[i + 1]))\n",
    "    return kappas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0f2cea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "df_anton.shape[0]\n",
    "for i in range(100, 900, 100):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56b5a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappas = cohens_kappa('label8.xlsx', df_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e31ac8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.6867892222348115,\n",
       " 0.6036853561752391,\n",
       " 0.4810620065186614,\n",
       " 0.6829383093527561,\n",
       " 0.7337599102137073,\n",
       " 0.7924861553236712,\n",
       " 0.8159139171172582]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kappas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f06073",
   "metadata": {},
   "source": [
    "## Leftovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bfc37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in the parameter values in the grid \n",
    "parameter_grid = {\n",
    "    'tfidf__use_idf': [False, True],\n",
    "    'logreg__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'logreg__C': [0.1, 0.5, 1],\n",
    "}\n",
    "\n",
    "#Initializing a kfold with 5 folds\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "#Initializing the GridSearchCV\n",
    "search = GridSearchCV(pipeline, parameter_grid, cv=cv, n_jobs = -1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fa9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "148e6aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.778"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59fc8169",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4af93b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e031483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[209   3   0   3]\n",
      " [ 51  75   1   5]\n",
      " [ 35   6  36   1]\n",
      " [  5   0   1  69]]\n"
     ]
    }
   ],
   "source": [
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c91d6e",
   "metadata": {},
   "source": [
    "## Cohens K: AE & EG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c7100c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active Learning.ipynb             label3.xlsx\r\n",
      "First scrape attempts.ipynb       label4.xlsx\r\n",
      "Hsbm keyword generator.ipynb      label5.xlsx\r\n",
      "Initial network.xlsx              label6.csv\r\n",
      "Preprocessing.ipynb               label6.xlsx\r\n",
      "Supervised learning keras.ipynb   label6_emilie.xlsx\r\n",
      "Supervised learning.ipynb         label7.xlsx\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m                       label7_ae.xlsx\r\n",
      "app_cred.py                       label7_eg.xlsx\r\n",
      "\u001b[34mattack\u001b[m\u001b[m                            label_this.xlsx\r\n",
      "exam_utils.py                     lemma_all.csv\r\n",
      "final_final_keywords.csv          \u001b[34mmodels\u001b[m\u001b[m\r\n",
      "final_keywords.csv                readme.md\r\n",
      "graph_full_dataset_grouped.xml.gz \u001b[34mresults\u001b[m\u001b[m\r\n",
      "green_words.txt                   \u001b[34mtest_trainer\u001b[m\u001b[m\r\n",
      "\u001b[34mhSBM_Topicmodel\u001b[m\u001b[m                   topics_100_000_nostopwords.csv\r\n",
      "hsbm_df.zip                       topics_grouped_dataset.csv\r\n",
      "keywords_qual.xlsx                tweets_final.zip\r\n",
      "label2.xlsx                       \u001b[34mw2v\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467aee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anton = pd.read_excel('label7_ae.xlsx', index_col=0)\n",
    "df_emilie = pd.read_excel('label7_eg.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc1798f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anton.to_csv('label7_totest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91fa8c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_a = df_anton.head(200).label\n",
    "labels_e = df_emilie.head(200).label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f14d8a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6644647163678306"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(labels_a, labels_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f22eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anton.loc[:, 'emilies_labels'] = df_emilie.loc[:, 'label'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73954e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anton.loc[df_anton.label != df_anton.emilies_labels].to_csv('uenige_eg_an.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d9088b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
