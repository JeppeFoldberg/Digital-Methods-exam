{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"only_climate_tweets.csv\")\n",
    "print(len(df))\n",
    "df=df.drop_duplicates(subset='tweet_id')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[df[\"user_screen_name\"],df[\"tweet_id\"],df[\"tweet_created_at\"],df[\"tweet_full_text\"]]\n",
    "df_preproc=pd.concat(data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preproc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy\n",
    "# !python -m spacy download da_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"da_core_news_lg\")\n",
    "all_stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence):\n",
    "    \"\"\"\n",
    "    Removes Danish stopwords imported from spacy and returns filtered string\n",
    "    \"\"\"  \n",
    "    tokens = sentence.split(\" \")\n",
    "    tokens_filtered= [word for word in tokens if not word in all_stopwords]\n",
    "    return (\" \").join(tokens_filtered)\n",
    "\n",
    "def preproccessor(string, verb_noun_only=False):\n",
    "    \"\"\"\n",
    "    Helper function for lemmatizer().\n",
    "    Preprocesses the string by:\n",
    "    1) lowercasing string\n",
    "    2) removing urls\n",
    "    3) remove mentions, hashtags, and RT\n",
    "    4) remove non-alphanumerical values\n",
    "    5) remove multiple whitespaces\n",
    "    6) remove trailing whitespaces\n",
    "    \"\"\"  \n",
    "    # Lowercase\n",
    "    string=string.lower()\n",
    "    \n",
    "    # Remove url\n",
    "    string=re.sub(\n",
    "        r\"(https|http?):\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b\",\n",
    "        \"\", \n",
    "        string)\n",
    "    \n",
    "    # Remove weird remaining http\n",
    "    string = re.sub(r'https?', '', string)\n",
    "    \n",
    "    # Remove mentions, hashtags, and RT\n",
    "    string=re.sub(\"@\\w+|#\\w+|^rt\",\"\", string)\n",
    "    \n",
    "    # Remove non-alphanumerical values\n",
    "    string=re.sub(r\"\\W\",\" \", string)\n",
    " \n",
    "    # Remove more than one whitespace\n",
    "    string=re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    # Remove trailing whitespaces\n",
    "    string=string.strip()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    string=remove_stopwords(string)\n",
    "    \n",
    "    # Create and return doc object\n",
    "    return nlp(string)  \n",
    "   \n",
    "def lemmatizer(string):\n",
    "    \"\"\"\n",
    "    Lemmatize the preprocessed string using spacy's lemmatizer\n",
    "    \"\"\"\n",
    "    doc=preproccessor(string)\n",
    "    \n",
    "    lemma=\" \".join(\n",
    "        [token.lemma_ for token in doc if len(str(token.lemma_))>3]\n",
    "    )\n",
    "    return lemma\n",
    "\n",
    "def lemmatizer_reduced(string):\n",
    "    \"\"\"\n",
    "    Reduce the allready lemmatized string by only including proper nouns, nouns, and verbs\n",
    "    \"\"\"\n",
    "    doc=nlp(string)\n",
    "    verb_and_noun=\" \".join(\n",
    "    [token.lemma_ for token in doc if token.pos_ in [\"PROPN\",\"NOUN\",\"VERB\"]]\n",
    "    )\n",
    "    return verb_and_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterates throuhh entire dataframe twice. Takes a long time.\n",
    "df_preproc[\"tweet_text_lemma\"]=[lemmatizer(string) for string in df_preproc.tweet_full_text.to_numpy()]\n",
    "df_preproc[\"tweet_text_lemma_reduced\"]=[lemmatizer_reduced(string) for string in df_preproc.tweet_text_lemma.values]\n",
    "# df_preproc.dropna(axis=0,subset=[\"tweet_text_lemma_reduced\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to csv\n",
    "compression=dict(method=\"zip\",archive_name=\"lemma.csv\")\n",
    "df_preproc.to_csv(\"lemma.zip\",compression=compression,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
